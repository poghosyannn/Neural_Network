{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X_diab, y_diab = load_diabetes(return_X_y=True) # returns diabetes data shapes: (442, 10) and (442,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=60, n_features=10, noise=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(\n",
    "            self, \n",
    "            units, \n",
    "            *, \n",
    "            input_layer: bool = False,\n",
    "            activation: str = \"linear\",\n",
    "            use_bias: bool = True):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer.\n",
    "\n",
    "        Args:\n",
    "            units (int): Count of neurons in the layer.\n",
    "            input_layer (bool, optional): Whether the layer is an input layer. Defaults to False.\n",
    "            activation (str, optional): Activation function for the layer. Can be \"linear\", \"relu\", or \"sigmoid\". Defaults to \"linear\".\n",
    "            use_bias (bool, optional): Whether to use bias in the layer. Defaults to True.\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        self.units = units\n",
    "        self.input_layer = input_layer\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "\n",
    "        self.w = None # Weights matrix\n",
    "        self._weight_gradient = None # Weight derivative matrix\n",
    "        self._bias_gradient = None # Bias derivative vector\n",
    "        self.m = 0\n",
    "        self.v = 0\n",
    "        self.b1 = 0.9\n",
    "        self.b2 = 0.99\n",
    "        \n",
    "    def activationFunction(self, z):\n",
    "        \"\"\"\n",
    "        Apply the activation function to the given input.\n",
    "\n",
    "        Args:\n",
    "            z (numpy.ndarray): Input to the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output after applying the activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return z\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(z, np.zeros(z.shape))\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _weightInit(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the weights matrix based on the input size.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of the input.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return # input_layer doesn't need weights\n",
    "\n",
    "        self.w = np.random.normal(loc = 0, scale = 1 / input_size, size=(input_size, self.units)) # loc -> mean, scale -> variance\n",
    "        self.bias = np.zeros((1, self.units))\n",
    "\n",
    "\n",
    "    def _activationDerivative(self):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the activation function.\n",
    "\n",
    "        Notes:\n",
    "            Only supports the \"linear\", \"relu\", and \"sigmoid\" activation functions.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return 1\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return (self._output > 0) * 1\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self._output * (1 - self._output)\n",
    "\n",
    "    def _setGrad(self, grad):\n",
    "        \"\"\"\n",
    "        Calculate the gradients of weights and bias for backpropagation.\n",
    "\n",
    "        Args:\n",
    "            grad (numpy.ndarray): Gradient from the previous layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient to be passed to the previous layer.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "        \n",
    "        grad = grad * self._activationDerivative()\n",
    "        self._weight_gradient = self._input.T @ grad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self._bias_gradient = grad.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return grad @ self.w.T\n",
    "    \n",
    "    def _updateGrad(self, learning_rate, iteration, eps = 0.00001):\n",
    "        \"\"\"\n",
    "        Update the weights and bias based on the computed gradients.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "        if self.optimizer == 'adam':\n",
    "            iteration += 1\n",
    "            self.m = self.b1 * self.m + (1- self.b1) * self._weight_gradient\n",
    "            self.v = self.b2 * self.v + (1- self.b2) * np.square(self._weight_gradient)\n",
    "            self.m_ = self.m / (1 - np.pow(b1, iteration))\n",
    "            self.v_ = self.v / (1 - np.pow(b2, iteration))\n",
    "            self.w -= learning_rate * self.m_ / (np.sqrt(self.v_) + eps)\n",
    "          \n",
    "        else:\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient \n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input to the layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output of the layer after applying the activation function.\n",
    "        \"\"\"\n",
    "        if self.input_layer:\n",
    "            return X\n",
    "        \n",
    "        self._input = X\n",
    "        self._output = self.activationFunction(X @ self.w + self.bias)\n",
    "\n",
    "        return self._output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeauralNetwork:\n",
    "    def __init__(\n",
    "            self, \n",
    "            layers: list, \n",
    "            loss_function: str = \"mse\", \n",
    "            learning_rate = 0.01, \n",
    "            max_iter=1000,\n",
    "            verbose: bool = False):\n",
    "        \"\"\"\n",
    "        Initialize a neural network.\n",
    "\n",
    "        Args:\n",
    "            layers (list): List of Layer objects defining the network architecture. \n",
    "            loss_function (str, optional): Loss function to use. Defaults to \"mse\".\n",
    "            learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n",
    "            max_iter (int, optional): Maximum number of iterations for training. Defaults to 1000.\n",
    "            verbose (bool, optional): Whether to display training progress. Defaults to False.\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "\n",
    "        # Weights initializing:\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i]._weightInit(self.layers[i - 1].units)\n",
    "\n",
    "    def lossFunction(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the loss between the true values and predicted values.\n",
    "\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True values.\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 0.5 * np.mean(np.linalg.norm(y_pred - y_true, axis=1)**2)\n",
    "\n",
    "        # Can be add\n",
    "\n",
    "    def _lossFunctionDerivative(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 1 / len(y_pred) * (y_pred - y_true)\n",
    "\n",
    "        # Can be add\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the neural network on the given input-output pairs.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "            y (numpy.ndarray): Output data.\n",
    "\n",
    "        Notes:\n",
    "            Reshapes X and y to match the expected input shapes of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            pred = self.forward(X)\n",
    "\n",
    "            if self.verbose:\n",
    "                process_percent = int(_ / self.max_iter * 20)\n",
    "                print(f\"\\r {_}/{self.max_iter}: [{process_percent * '=' + '>' + (20 - process_percent) * '-'}] - loss: {self.lossFunction(y, pred)}\", end=\"\")\n",
    "            self.backward(pred, y)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\r {self.max_iter}/{self.max_iter}: [{21 * '='}] - loss: {self.lossFunction(y, pred)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform predictions using the trained neural network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted output data.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        X_ = np.copy(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            X_ = layer.call(X_)\n",
    "        return X_\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to update the weights of the network.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = self._lossFunctionDerivative(y_pred, y_true)\n",
    "\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            gradient = self.layers[i]._setGrad(gradient)\n",
    "            self.layers[i]._updateGrad(self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100000/100000: [=====================] - loss: 4.281329570642632e-26\n"
     ]
    }
   ],
   "source": [
    "nn = NeauralNetwork(layers=[\n",
    "        Layer(units=10, input_layer=True),\n",
    "        Layer(units=40, activation=\"sigmoid\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=1),\n",
    "    ],\n",
    "    loss_function = \"mse\",\n",
    "    learning_rate=0.001, \n",
    "    max_iter=100000,\n",
    "    verbose=True,\n",
    ")\n",
    "y_reg = y_reg.reshape(-1, 1)\n",
    "nn.fit(X_reg, y_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -81.64367582],\n",
       "       [ 437.86437198],\n",
       "       [ 104.15502486],\n",
       "       [ 168.70187103],\n",
       "       [ 293.72877761],\n",
       "       [  34.34481735],\n",
       "       [  82.08138321],\n",
       "       [ 154.43813612],\n",
       "       [  91.2402544 ],\n",
       "       [ 167.28073431],\n",
       "       [ -23.0387585 ],\n",
       "       [ -76.52328597],\n",
       "       [ -99.68779939],\n",
       "       [-184.07195072],\n",
       "       [-187.26427736],\n",
       "       [  80.30344265],\n",
       "       [  41.03782639],\n",
       "       [  53.67150682],\n",
       "       [  89.53823638],\n",
       "       [ 210.3238773 ],\n",
       "       [ -40.06460916],\n",
       "       [ 197.24561364],\n",
       "       [ -56.88269642],\n",
       "       [-268.83324876],\n",
       "       [  16.78730897],\n",
       "       [  -1.80806096],\n",
       "       [ 196.39720247],\n",
       "       [-153.89219543],\n",
       "       [  28.97055044],\n",
       "       [-414.81335343],\n",
       "       [ 207.68697853],\n",
       "       [ 165.21614808],\n",
       "       [ -27.8684952 ],\n",
       "       [-278.11827871],\n",
       "       [  33.62812399],\n",
       "       [ 107.13976681],\n",
       "       [ -68.69481842],\n",
       "       [-208.85049207],\n",
       "       [ 178.23657958],\n",
       "       [ -97.17164961],\n",
       "       [ 109.5142967 ],\n",
       "       [-118.13038389],\n",
       "       [-214.58644604],\n",
       "       [-331.26344009],\n",
       "       [-110.38551895],\n",
       "       [ 277.55557628],\n",
       "       [-154.02237213],\n",
       "       [-256.73054448],\n",
       "       [  92.79977595],\n",
       "       [-107.05743503],\n",
       "       [ 222.89781554],\n",
       "       [ -26.72679165],\n",
       "       [ -96.34696814],\n",
       "       [ 304.79801877],\n",
       "       [  64.0981859 ],\n",
       "       [  17.76996184],\n",
       "       [-247.17933238],\n",
       "       [ -79.58877256],\n",
       "       [-299.79373424],\n",
       "       [-458.4077167 ]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(X_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -81.64367582],\n",
       "       [ 437.86437198],\n",
       "       [ 104.15502486],\n",
       "       [ 168.70187103],\n",
       "       [ 293.72877761],\n",
       "       [  34.34481735],\n",
       "       [  82.08138321],\n",
       "       [ 154.43813612],\n",
       "       [  91.2402544 ],\n",
       "       [ 167.28073431],\n",
       "       [ -23.0387585 ],\n",
       "       [ -76.52328597],\n",
       "       [ -99.68779939],\n",
       "       [-184.07195072],\n",
       "       [-187.26427736],\n",
       "       [  80.30344265],\n",
       "       [  41.03782639],\n",
       "       [  53.67150682],\n",
       "       [  89.53823638],\n",
       "       [ 210.3238773 ],\n",
       "       [ -40.06460916],\n",
       "       [ 197.24561364],\n",
       "       [ -56.88269642],\n",
       "       [-268.83324876],\n",
       "       [  16.78730897],\n",
       "       [  -1.80806096],\n",
       "       [ 196.39720247],\n",
       "       [-153.89219543],\n",
       "       [  28.97055044],\n",
       "       [-414.81335343],\n",
       "       [ 207.68697853],\n",
       "       [ 165.21614808],\n",
       "       [ -27.8684952 ],\n",
       "       [-278.11827871],\n",
       "       [  33.62812399],\n",
       "       [ 107.13976681],\n",
       "       [ -68.69481842],\n",
       "       [-208.85049207],\n",
       "       [ 178.23657958],\n",
       "       [ -97.17164961],\n",
       "       [ 109.5142967 ],\n",
       "       [-118.13038389],\n",
       "       [-214.58644604],\n",
       "       [-331.26344009],\n",
       "       [-110.38551895],\n",
       "       [ 277.55557628],\n",
       "       [-154.02237213],\n",
       "       [-256.73054448],\n",
       "       [  92.79977595],\n",
       "       [-107.05743503],\n",
       "       [ 222.89781554],\n",
       "       [ -26.72679165],\n",
       "       [ -96.34696814],\n",
       "       [ 304.79801877],\n",
       "       [  64.0981859 ],\n",
       "       [  17.76996184],\n",
       "       [-247.17933238],\n",
       "       [ -79.58877256],\n",
       "       [-299.79373424],\n",
       "       [-458.4077167 ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# alg = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(10),\n",
    "#     tf.keras.layers.Dense(20, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1, activation='relu'),\n",
    "# ])\n",
    "\n",
    "# alg.compile(optimizer='adam', loss='mse', )\n",
    "\n",
    "# alg.fit(X_diab, y_diab, epochs=1000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alg.predict(X_diab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
