{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X_diab, y_diab = load_diabetes(return_X_y=True) # returns diabetes data shapes: (442, 10) and (442,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"\n",
    "    Represents a neural network layer.\n",
    "    ==========\n",
    "    Attributes:\n",
    "    ----------\n",
    "        units (int): Number of neurons in the layer.\n",
    "        input_layer (bool): Whether the layer is an input layer.\n",
    "        activation (str): Activation function for the layer.\n",
    "        use_bias (bool): Whether to use bias in the layer.\n",
    "        optimizer (str): Optimization algorithm used for the layer.\n",
    "        w (numpy.ndarray): Weights matrix for the layer.\n",
    "\n",
    "    Methods:\n",
    "    ---------\n",
    "        activationFunction(z):\n",
    "            Apply the activation function to the given input.\n",
    "\n",
    "        call(X):\n",
    "            Perform a forward pass through the layer.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            units, \n",
    "            *, \n",
    "            input_layer: bool = False,\n",
    "            activation: str = \"linear\",\n",
    "            use_bias: bool = True,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer.\n",
    "\n",
    "        Args:\n",
    "            units (int): Count of neurons in the layer.\n",
    "            input_layer (bool, optional): Whether the layer is an input layer. Defaults to False.\n",
    "            activation (str, optional): Activation function for the layer. Can be \"linear\", \"relu\", or \"sigmoid\". Defaults to \"linear\".\n",
    "            use_bias (bool, optional): Whether to use bias in the layer. Defaults to True.\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        self.units = units\n",
    "        self.input_layer = input_layer\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.optimizer = None # Optimizer for layer\n",
    "\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "\n",
    "        self.w = None # Weights matrix\n",
    "        self._weight_gradient = None # Weights derivative matrix\n",
    "        self._bias_gradient = None # Biases derivative vector\n",
    "\n",
    "    def activationFunction(self, z):\n",
    "        \"\"\"\n",
    "        Apply the activation function to the given input.\n",
    "\n",
    "        Args:\n",
    "            z (numpy.ndarray): Input to the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output after applying the activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return z\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(z, np.zeros(z.shape))\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _weightInit(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the weights matrix based on the input size.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of the input.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return # input_layer doesn't need weights\n",
    "\n",
    "        self.w = np.random.normal(loc = 0, scale = 1 / input_size, size=(input_size, self.units))\n",
    "        # Initialize weights matrix using a normal distribution with mean 0 and variance 1 / input_size\n",
    "\n",
    "        self.bias = np.zeros((1, self.units))\n",
    "        # Initialize biases as zeros\n",
    "\n",
    "\n",
    "    def _setOptimizer(self, optimizer, beta_1, beta_2):\n",
    "        \"\"\"\n",
    "        Set the optimizer and initialize optimizer-specific variables.\n",
    "\n",
    "        Args:\n",
    "            optimizer (str): Optimization algorithm to use.\n",
    "            beta_1 (float): Value for the optimizer parameter beta_1.\n",
    "            beta_2 (float): Value for the optimizer parameter beta_2.\n",
    "\n",
    "        Notes:\n",
    "            - Only executed for layers other than the input layer.\n",
    "            - Sets the optimizer and initializes optimizer-specific variables based on the chosen optimizer.\n",
    "            - For each optimizer, the corresponding variables are initialized.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self._b1 = beta_1\n",
    "        self._b2 = beta_2\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "            # Initialize weight-specific variables for AdaGrad\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "                # Initialize bias-specific variables for AdaGrad\n",
    "\n",
    "        if self.optimizer == 'adam':\n",
    "            self._iter = 0  # Calculate iterations\n",
    "\n",
    "            self._weight_m = np.zeros(self.w.shape)\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "            # Initialize weight-specific variables for Adam\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = np.zeros(self.bias.shape)\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "                # Initialize bias-specific variables for Adam\n",
    "\n",
    "        if self.optimizer == 'rms_prop':\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "            # Initialize weight and bias-specific variables for RMSprop\n",
    "\n",
    "        if self.optimizer == 'gdm':\n",
    "            self._weight_m = np.zeros(self.w.shape)\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = np.zeros(self.bias.shape)\n",
    "            # Initialize weight and bias-specific variables for Gradient Descent with Momentum   \n",
    "\n",
    "    def _activationDerivative(self):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the activation function.\n",
    "\n",
    "        Notes:\n",
    "            Only supports the \"linear\", \"relu\", and \"sigmoid\" activation functions.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return 1\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return (self._output > 0) * 1\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self._output * (1 - self._output)\n",
    "\n",
    "    def _setGrad(self, grad):\n",
    "        \"\"\"\n",
    "        Calculate the gradients of weights and bias for backpropagation.\n",
    "\n",
    "        Args:\n",
    "            grad (numpy.ndarray): Gradient from the previous layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient to be passed to the previous layer.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "        \n",
    "        grad = grad * self._activationDerivative()\n",
    "        self._weight_gradient = self._input.T @ grad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self._bias_gradient = grad.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return grad @ self.w.T\n",
    "    \n",
    "    def _updateGrad(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and bias based on the computed gradients.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "\n",
    "        Notes:\n",
    "            - Only executed for layers other than the input layer.\n",
    "            - Updates the weights and biases based on the computed gradients and the chosen optimizer.\n",
    "            - For each optimizer, the corresponding update rule is applied.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        eps = 10e-8 # Optimizer's epsilon\n",
    "\n",
    "        if self.optimizer == \"gd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._weight_v += np.square(self._weight_gradient)\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._weight_v) + eps)\n",
    "\n",
    "            self.w -= learning_rate_weight * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v += np.square(self._bias_gradient)\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._bias_v) + eps)\n",
    "\n",
    "                self.bias -= learning_rate_bias * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == 'adam':\n",
    "            self._iter += 1\n",
    "\n",
    "            self._weight_m = self._b1 * self._weight_m + (1- self._b1) * self._weight_gradient\n",
    "            self._weight_v = self._b2 * self._weight_v + (1- self._b2) * np.square(self._weight_gradient)\n",
    "\n",
    "            weight_m = self._weight_m / (1 - np.power(self._b1, self._iter))\n",
    "            weight_v = self._weight_v / (1 - np.power(self._b2, self._iter))\n",
    "\n",
    "            self.w -= learning_rate * weight_m / (np.sqrt(weight_v) + eps) # Updating\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = self._b1 * self._bias_m + (1- self._b1) * self._bias_gradient\n",
    "                self._bias_v = self._b2 * self._bias_v + (1- self._b2) * np.square(self._bias_gradient)\n",
    "\n",
    "                bias_m = self._bias_m / (1 - np.power(self._b1, self._iter)) \n",
    "                bias_v = self._bias_v / (1 - np.power(self._b2, self._iter))\n",
    "\n",
    "\n",
    "                self.bias -= learning_rate * bias_m / (np.sqrt(bias_v) + eps) # Updating\n",
    "\n",
    "        \n",
    "        if self.optimizer == 'rms_prop':\n",
    "            self._weight_v = self._b2 * self._weight_v + (1- self._b2) * np.square(self._weight_gradient)\n",
    "\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._weight_v) + eps)\n",
    "\n",
    "            self.w -= learning_rate_weight * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = self._b2 * self._bias_v + (1- self._b2) * np.square(self._bias_gradient)\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._bias_v) + eps)\n",
    "\n",
    "                self.bias -= learning_rate_bias * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == 'gdm':\n",
    "            self._weight_m = self._b2 * self._weight_m + (1 - self._b2) * self._weight_gradient\n",
    "\n",
    "            self.w -= learning_rate * self._weight_m\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = self._b2 * self._bias_m + (1 - self._b2) * self._bias_gradient\n",
    "\n",
    "                self.bias -= learning_rate * self._bias_m\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input to the layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output of the layer after applying the activation function.\n",
    "        \"\"\"\n",
    "        if self.input_layer:\n",
    "            return X\n",
    "        \n",
    "        self._input = X\n",
    "        self._output = self.activationFunction(X @ self.w + self.bias)\n",
    "\n",
    "        return self._output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeauralNetwork:\n",
    "    \"\"\"\n",
    "    Neural Network\n",
    "    ==============\n",
    "\n",
    "    A neural network model for deep learning.\n",
    "\n",
    "    The `NeuralNetwork` class allows you to create and train a neural network model with customizable architecture and\n",
    "    training parameters.\n",
    "\n",
    "    Args:\n",
    "    -------\n",
    "        layers (list): List of Layer objects defining the network architecture.\n",
    "        loss_function (str, optional): Loss function to use. Defaults to \"mse\".\n",
    "        learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n",
    "        verbose (bool, optional): Whether to display training progress. Defaults to False.\n",
    "        optimizer (str, optional): Optimization algorithm to use for updating weights during training. Defaults to \"gd\".\n",
    "        epochs (int, optional): Number of epochs for training. Defaults to 1.\n",
    "        batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "        beta_1 (float, optional): Parameter for the optimizer. Defaults to 0.9.\n",
    "        beta_2 (float, optional): Parameter for the optimizer. Defaults to 0.999.\n",
    "\n",
    "    Methods:\n",
    "    --------\n",
    "        __init__(self, layers, loss_function=\"mse\", learning_rate=0.01, verbose=False, optimizer=\"gd\", epochs=1,\n",
    "                 batch_size=32, beta_1=0.9, beta_2=0.999)\n",
    "            Initializes a neural network object.\n",
    "        lossFunction(self, y_true, y_pred)\n",
    "            Compute the loss between the true values and predicted values.\n",
    "        fit(self, X, y)\n",
    "            Train the neural network on the given input-output pairs.\n",
    "        predict(self, X)\n",
    "            Perform predictions using the trained neural network.\n",
    "        forward(self, X)\n",
    "            Perform a forward pass through the network.\n",
    "        backward(self, y_pred, y_true)\n",
    "            Perform backpropagation to update the weights of the network.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self, \n",
    "            layers: list, \n",
    "            loss_function: str = \"mse\", \n",
    "            learning_rate = 0.01,\n",
    "            verbose: bool = False,\n",
    "            optimizer: str = \"gd\",\n",
    "            epochs: int = 1, \n",
    "            batch_size: int = 32,\n",
    "            beta_1: float = 0.9,\n",
    "            beta_2: float = 0.999\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network.\n",
    "        --------\n",
    "        Args:\n",
    "        --------\n",
    "            layers (list): List of Layer objects defining the network architecture. \n",
    "            loss_function (str, optional): Loss function to use. Defaults to \"mse\".\n",
    "            optimizer (str, optional): Optimization algorithm to use for updating weights during training.\n",
    "                Options include:\n",
    "                - \"gd\" (Gradient Descent): Standard gradient descent.\n",
    "                - \"sgd\" (Stochastic Gradient Descent): Update weights using a single sample at a time.\n",
    "                - \"adagrad\" (Adaptive Gradient): Adjust the learning rate based on the frequency of feature occurrences.\n",
    "                - \"adam\" (Adam): Adaptive Moment Estimation algorithm.\n",
    "                - \"rms_prop\" (Root Mean Square Propagation): Adapt the learning rate based on the moving average of squared gradients.\n",
    "                - \"gdm\" (Gradient Descent with Momentum): Add momentum to the gradient descent algorithm.\n",
    "                Defaults to \"gd\".\n",
    "\n",
    "            learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n",
    "            epochs (int, optional): Number of epochs for training. Defaults to 1.\n",
    "            batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "            verbose (bool, optional): Whether to display training progress. Defaults to False.\n",
    "\n",
    "            beta_1 (float, optional): Parameter for the optimizer. Defaults to 0.9.\n",
    "            beta_2 (float, optional): Parameter for the optimizer. Defaults to 0.999.\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.optimizer = optimizer  # Optimizer for all layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.beta_1 = beta_1  # Optimizer parameters\n",
    "        self.beta_2 = beta_2  # Optimizer parameters\n",
    "\n",
    "        # Weights initializing:\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i]._weightInit(self.layers[i - 1].units)\n",
    "            self.layers[i]._setOptimizer(self.optimizer, self.beta_1, self.beta_2)\n",
    "            # Initialize weights for each layer and set the optimizer\n",
    "\n",
    "        # Only for SGD\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.batch_size = 1  # SGD is the same as mini-batch gradient descent when batch_size = 1\n",
    "\n",
    "\n",
    "\n",
    "    def lossFunction(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the loss between the true values and predicted values.\n",
    "        \n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True values.\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 0.5 * np.mean(np.linalg.norm(y_pred - y_true, axis=1)**2)\n",
    "\n",
    "        # Can be added\n",
    "\n",
    "    def _lossFunctionDerivative(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 1 / len(y_pred) * (y_pred - y_true)\n",
    "\n",
    "        # Can be added\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the neural network on the given input-output pairs.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "            y (numpy.ndarray): Output data.\n",
    "\n",
    "        Notes:\n",
    "            - Reshape y to a column vector (shape: (n_samples, output_size)).\n",
    "        \"\"\"\n",
    "        batch_separation = [(i, i + self.batch_size) for i in range(0, len(X), self.batch_size)] # Get batch indices\n",
    "        epoch_len = len(batch_separation)\n",
    "\n",
    "        indeces = np.arange(len(X))\n",
    "\n",
    "        for _ in range(self.epochs):    \n",
    "            np.random.shuffle(indeces) # Shuffle the training data\n",
    "\n",
    "            for iter, (i, j) in enumerate(batch_separation):\n",
    "                X_ = X[indeces[i:j]] # Get current batch\n",
    "                y_ = y[indeces[i:j]] # Get current batch\n",
    "\n",
    "                pred = self.forward(X_)\n",
    "\n",
    "                if self.verbose:\n",
    "                    process_percent = int(iter / epoch_len * 10)\n",
    "                    print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter}/{epoch_len}: [{process_percent * '=' + '>' + (10 - process_percent) * '-'}] - loss: {self.lossFunction(y_, pred)}\",end='')\n",
    "                \n",
    "                self.backward(pred, y_)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter + 1}/{epoch_len}: [{11 * '='}] - loss: {self.lossFunction(y_, pred)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform predictions using the trained neural network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted output data.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        X_ = np.copy(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            X_ = layer.call(X_)\n",
    "        return X_\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to update the weights of the network.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = self._lossFunctionDerivative(y_pred, y_true)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer._setGrad(gradient)\n",
    "            layer._updateGrad(self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/1000; Batch 22/22: [===========] - loss: 16379.891288245719\n",
      " Epoch 2/1000; Batch 22/22: [===========] - loss: 3527.3532433358232\n",
      " Epoch 3/1000; Batch 22/22: [===========] - loss: 13282.517994555777\n",
      " Epoch 4/1000; Batch 22/22: [===========] - loss: 8578.0841451669386\n",
      " Epoch 5/1000; Batch 22/22: [===========] - loss: 42916.768979155368\n",
      " Epoch 6/1000; Batch 22/22: [===========] - loss: 14785.942303084545\n",
      " Epoch 7/1000; Batch 22/22: [===========] - loss: 15656.176891126355\n",
      " Epoch 8/1000; Batch 22/22: [===========] - loss: 1401.1593482460944\n",
      " Epoch 9/1000; Batch 22/22: [===========] - loss: 29261.166503427763\n",
      " Epoch 10/1000; Batch 22/22: [===========] - loss: 15117.985437265115\n",
      " Epoch 11/1000; Batch 22/22: [===========] - loss: 27108.054739028692\n",
      " Epoch 12/1000; Batch 22/22: [===========] - loss: 11372.672322461993\n",
      " Epoch 13/1000; Batch 22/22: [===========] - loss: 43738.581358959842\n",
      " Epoch 14/1000; Batch 22/22: [===========] - loss: 23271.009919578213\n",
      " Epoch 15/1000; Batch 22/22: [===========] - loss: 38002.515736726666\n",
      " Epoch 16/1000; Batch 22/22: [===========] - loss: 7268.0255463211952\n",
      " Epoch 17/1000; Batch 22/22: [===========] - loss: 10300.163298376143\n",
      " Epoch 18/1000; Batch 22/22: [===========] - loss: 15041.138349791592\n",
      " Epoch 19/1000; Batch 22/22: [===========] - loss: 11762.329764273378\n",
      " Epoch 20/1000; Batch 22/22: [===========] - loss: 5341.1475454517853\n",
      " Epoch 21/1000; Batch 22/22: [===========] - loss: 30530.200327809463\n",
      " Epoch 22/1000; Batch 22/22: [===========] - loss: 29014.070889427874\n",
      " Epoch 23/1000; Batch 22/22: [===========] - loss: 29999.120779505916\n",
      " Epoch 24/1000; Batch 22/22: [===========] - loss: 3358.2047447709867\n",
      " Epoch 25/1000; Batch 22/22: [===========] - loss: 4009.3499291616795\n",
      " Epoch 26/1000; Batch 22/22: [===========] - loss: 50978.619594554424\n",
      " Epoch 27/1000; Batch 22/22: [===========] - loss: 21241.088668253575\n",
      " Epoch 28/1000; Batch 22/22: [===========] - loss: 2619.9134580465525\n",
      " Epoch 29/1000; Batch 22/22: [===========] - loss: 1866.4638775811588\n",
      " Epoch 30/1000; Batch 22/22: [===========] - loss: 3921.9499096843483\n",
      " Epoch 31/1000; Batch 22/22: [===========] - loss: 8012.8729267916161\n",
      " Epoch 32/1000; Batch 22/22: [===========] - loss: 4687.9064099836997\n",
      " Epoch 33/1000; Batch 22/22: [===========] - loss: 5392.1004745898035\n",
      " Epoch 34/1000; Batch 22/22: [===========] - loss: 3441.5549243611292\n",
      " Epoch 35/1000; Batch 22/22: [===========] - loss: 16540.660093822644\n",
      " Epoch 36/1000; Batch 22/22: [===========] - loss: 8042.2423087415614\n",
      " Epoch 37/1000; Batch 22/22: [===========] - loss: 1002.5147426844278\n",
      " Epoch 38/1000; Batch 22/22: [===========] - loss: 24186.752263375018\n",
      " Epoch 39/1000; Batch 22/22: [===========] - loss: 3437.7040391095551\n",
      " Epoch 40/1000; Batch 22/22: [===========] - loss: 4216.7033336570792\n",
      " Epoch 41/1000; Batch 22/22: [===========] - loss: 198.16327960850668\n",
      " Epoch 42/1000; Batch 22/22: [===========] - loss: 12259.021353169126\n",
      " Epoch 43/1000; Batch 22/22: [===========] - loss: 1947.1527545096255\n",
      " Epoch 44/1000; Batch 22/22: [===========] - loss: 6403.6016902075159\n",
      " Epoch 45/1000; Batch 22/22: [===========] - loss: 5269.7575802833574\n",
      " Epoch 46/1000; Batch 22/22: [===========] - loss: 4939.1333710183443\n",
      " Epoch 47/1000; Batch 22/22: [===========] - loss: 33312.458118653574\n",
      " Epoch 48/1000; Batch 22/22: [===========] - loss: 29698.227153349334\n",
      " Epoch 49/1000; Batch 22/22: [===========] - loss: 15115.147252590994\n",
      " Epoch 50/1000; Batch 22/22: [===========] - loss: 9677.2842580049982\n",
      " Epoch 51/1000; Batch 22/22: [===========] - loss: 20842.800191867085\n",
      " Epoch 52/1000; Batch 22/22: [===========] - loss: 1947.2939221546953\n",
      " Epoch 53/1000; Batch 22/22: [===========] - loss: 3029.0064185787737\n",
      " Epoch 54/1000; Batch 22/22: [===========] - loss: 7773.5692229136592\n",
      " Epoch 55/1000; Batch 22/22: [===========] - loss: 12623.925050637747\n",
      " Epoch 56/1000; Batch 22/22: [===========] - loss: 1990.8014436230762\n",
      " Epoch 57/1000; Batch 22/22: [===========] - loss: 12679.115228303186\n",
      " Epoch 58/1000; Batch 22/22: [===========] - loss: 26989.088782090337\n",
      " Epoch 59/1000; Batch 22/22: [===========] - loss: 3661.2014768213603\n",
      " Epoch 60/1000; Batch 22/22: [===========] - loss: 29523.480921254932\n",
      " Epoch 61/1000; Batch 22/22: [===========] - loss: 589.99504928385168\n",
      " Epoch 62/1000; Batch 22/22: [===========] - loss: 6393.3270375409713\n",
      " Epoch 63/1000; Batch 22/22: [===========] - loss: 1468.8036693712531\n",
      " Epoch 64/1000; Batch 22/22: [===========] - loss: 5146.3137733151518\n",
      " Epoch 65/1000; Batch 22/22: [===========] - loss: 29806.501033632997\n",
      " Epoch 66/1000; Batch 22/22: [===========] - loss: 18426.495751677616\n",
      " Epoch 67/1000; Batch 22/22: [===========] - loss: 1594.2459892438723\n",
      " Epoch 68/1000; Batch 22/22: [===========] - loss: 502.40570364504023\n",
      " Epoch 69/1000; Batch 22/22: [===========] - loss: 3618.9466410585155\n",
      " Epoch 70/1000; Batch 22/22: [===========] - loss: 24274.200026919407\n",
      " Epoch 71/1000; Batch 22/22: [===========] - loss: 3257.3643136084684\n",
      " Epoch 72/1000; Batch 22/22: [===========] - loss: 1306.3275581960713\n",
      " Epoch 73/1000; Batch 22/22: [===========] - loss: 12600.711576521857\n",
      " Epoch 74/1000; Batch 22/22: [===========] - loss: 9082.0922590440514\n",
      " Epoch 75/1000; Batch 22/22: [===========] - loss: 265.17539966323704\n",
      " Epoch 76/1000; Batch 22/22: [===========] - loss: 22971.648677715708\n",
      " Epoch 77/1000; Batch 22/22: [===========] - loss: 108.65298342918675\n",
      " Epoch 78/1000; Batch 22/22: [===========] - loss: 922.89602469852352\n",
      " Epoch 79/1000; Batch 22/22: [===========] - loss: 1282.3301036723465\n",
      " Epoch 80/1000; Batch 22/22: [===========] - loss: 3274.8146653176464\n",
      " Epoch 81/1000; Batch 22/22: [===========] - loss: 2174.3363348956427\n",
      " Epoch 82/1000; Batch 22/22: [===========] - loss: 521.10353693116786\n",
      " Epoch 83/1000; Batch 22/22: [===========] - loss: 16384.938138879555\n",
      " Epoch 84/1000; Batch 22/22: [===========] - loss: 9714.1634028979465\n",
      " Epoch 85/1000; Batch 22/22: [===========] - loss: 116.32884034674667\n",
      " Epoch 86/1000; Batch 22/22: [===========] - loss: 35980.871614416964\n",
      " Epoch 87/1000; Batch 22/22: [===========] - loss: 2883.9294016237836\n",
      " Epoch 88/1000; Batch 22/22: [===========] - loss: 1485.7763917282157\n",
      " Epoch 89/1000; Batch 22/22: [===========] - loss: 2137.2534150539172\n",
      " Epoch 90/1000; Batch 22/22: [===========] - loss: 1863.9864901069772\n",
      " Epoch 91/1000; Batch 22/22: [===========] - loss: 1261.7802313735627\n",
      " Epoch 92/1000; Batch 22/22: [===========] - loss: 12315.091334316554\n",
      " Epoch 93/1000; Batch 22/22: [===========] - loss: 16232.600672327924\n",
      " Epoch 94/1000; Batch 22/22: [===========] - loss: 9935.7536448745455\n",
      " Epoch 95/1000; Batch 22/22: [===========] - loss: 2287.3667377752214\n",
      " Epoch 96/1000; Batch 22/22: [===========] - loss: 26893.924210691992\n",
      " Epoch 97/1000; Batch 22/22: [===========] - loss: 97.971002138699831\n",
      " Epoch 98/1000; Batch 22/22: [===========] - loss: 6005.1010151281041\n",
      " Epoch 99/1000; Batch 22/22: [===========] - loss: 1017.7438588194217\n",
      " Epoch 100/1000; Batch 22/22: [===========] - loss: 21135.870127546004\n",
      " Epoch 101/1000; Batch 22/22: [===========] - loss: 8995.5331679397445\n",
      " Epoch 102/1000; Batch 22/22: [===========] - loss: 0.5032831754916454\n",
      " Epoch 103/1000; Batch 22/22: [===========] - loss: 1.0500048789038792\n",
      " Epoch 104/1000; Batch 22/22: [===========] - loss: 345.82409253765345\n",
      " Epoch 105/1000; Batch 22/22: [===========] - loss: 9908.5134130387255\n",
      " Epoch 106/1000; Batch 22/22: [===========] - loss: 31180.290482670578\n",
      " Epoch 107/1000; Batch 22/22: [===========] - loss: 3560.989085067009\n",
      " Epoch 108/1000; Batch 22/22: [===========] - loss: 6953.7764762837536\n",
      " Epoch 109/1000; Batch 22/22: [===========] - loss: 7255.2020814668064\n",
      " Epoch 110/1000; Batch 22/22: [===========] - loss: 20026.983807587058\n",
      " Epoch 111/1000; Batch 22/22: [===========] - loss: 12570.436358510417\n",
      " Epoch 112/1000; Batch 22/22: [===========] - loss: 13273.785666787811\n",
      " Epoch 113/1000; Batch 22/22: [===========] - loss: 11273.681496171455\n",
      " Epoch 114/1000; Batch 22/22: [===========] - loss: 3525.6957874433033\n",
      " Epoch 115/1000; Batch 22/22: [===========] - loss: 450.5267462964649\n",
      " Epoch 116/1000; Batch 22/22: [===========] - loss: 495.80979522789806\n",
      " Epoch 117/1000; Batch 22/22: [===========] - loss: 715.26356664026244\n",
      " Epoch 118/1000; Batch 22/22: [===========] - loss: 3632.8637600159058\n",
      " Epoch 119/1000; Batch 22/22: [===========] - loss: 5275.5494827928715\n",
      " Epoch 120/1000; Batch 22/22: [===========] - loss: 4567.271505386504\n",
      " Epoch 121/1000; Batch 22/22: [===========] - loss: 2650.5246920424276\n",
      " Epoch 122/1000; Batch 22/22: [===========] - loss: 17959.866315766663\n",
      " Epoch 123/1000; Batch 22/22: [===========] - loss: 377.68266951397454\n",
      " Epoch 124/1000; Batch 22/22: [===========] - loss: 14.801092029614623\n",
      " Epoch 125/1000; Batch 22/22: [===========] - loss: 5931.8464530514855\n",
      " Epoch 126/1000; Batch 22/22: [===========] - loss: 42.939976402296235\n",
      " Epoch 127/1000; Batch 22/22: [===========] - loss: 13.742930370920012\n",
      " Epoch 128/1000; Batch 22/22: [===========] - loss: 10985.989014903036\n",
      " Epoch 129/1000; Batch 22/22: [===========] - loss: 52.912959105473526\n",
      " Epoch 130/1000; Batch 22/22: [===========] - loss: 624.14743497324593\n",
      " Epoch 131/1000; Batch 22/22: [===========] - loss: 6807.5135045028035\n",
      " Epoch 132/1000; Batch 22/22: [===========] - loss: 1.2892451931061633\n",
      " Epoch 133/1000; Batch 22/22: [===========] - loss: 15.058597172139937\n",
      " Epoch 134/1000; Batch 22/22: [===========] - loss: 11740.851644409458\n",
      " Epoch 135/1000; Batch 22/22: [===========] - loss: 9.7096695832427662\n",
      " Epoch 136/1000; Batch 22/22: [===========] - loss: 226.74659602080547\n",
      " Epoch 137/1000; Batch 22/22: [===========] - loss: 11227.743184798563\n",
      " Epoch 138/1000; Batch 22/22: [===========] - loss: 355.17125282983395\n",
      " Epoch 139/1000; Batch 22/22: [===========] - loss: 176.06970010867663\n",
      " Epoch 140/1000; Batch 22/22: [===========] - loss: 0.049169465010341404\n",
      " Epoch 141/1000; Batch 22/22: [===========] - loss: 11243.701289041555\n",
      " Epoch 142/1000; Batch 22/22: [===========] - loss: 1191.2555575760625\n",
      " Epoch 143/1000; Batch 22/22: [===========] - loss: 10316.578412451776\n",
      " Epoch 144/1000; Batch 22/22: [===========] - loss: 3375.7168179415397\n",
      " Epoch 145/1000; Batch 22/22: [===========] - loss: 1272.9594415979955\n",
      " Epoch 146/1000; Batch 22/22: [===========] - loss: 120.80977485688823\n",
      " Epoch 147/1000; Batch 22/22: [===========] - loss: 1879.8152259973538\n",
      " Epoch 148/1000; Batch 22/22: [===========] - loss: 7527.5417849020713\n",
      " Epoch 149/1000; Batch 22/22: [===========] - loss: 808.40604664343361\n",
      " Epoch 150/1000; Batch 22/22: [===========] - loss: 309.02503427427126\n",
      " Epoch 151/1000; Batch 22/22: [===========] - loss: 1789.3954288991479\n",
      " Epoch 152/1000; Batch 22/22: [===========] - loss: 52.913646791853893\n",
      " Epoch 153/1000; Batch 22/22: [===========] - loss: 5707.2749278161037\n",
      " Epoch 154/1000; Batch 22/22: [===========] - loss: 530.30157805924424\n",
      " Epoch 155/1000; Batch 22/22: [===========] - loss: 6.4735647497413738\n",
      " Epoch 156/1000; Batch 22/22: [===========] - loss: 346.17724166303953\n",
      " Epoch 157/1000; Batch 22/22: [===========] - loss: 352.92326774746772\n",
      " Epoch 158/1000; Batch 22/22: [===========] - loss: 14.625448585726826\n",
      " Epoch 159/1000; Batch 22/22: [===========] - loss: 486.12855812808635\n",
      " Epoch 160/1000; Batch 22/22: [===========] - loss: 2267.3255509890323\n",
      " Epoch 161/1000; Batch 22/22: [===========] - loss: 8047.4569794374744\n",
      " Epoch 162/1000; Batch 22/22: [===========] - loss: 125.21980832003827\n",
      " Epoch 163/1000; Batch 22/22: [===========] - loss: 43.805968516423725\n",
      " Epoch 164/1000; Batch 22/22: [===========] - loss: 1308.2254830743725\n",
      " Epoch 165/1000; Batch 22/22: [===========] - loss: 5600.6478964318013\n",
      " Epoch 166/1000; Batch 22/22: [===========] - loss: 2441.9987509526774\n",
      " Epoch 167/1000; Batch 22/22: [===========] - loss: 3524.4267670537333\n",
      " Epoch 168/1000; Batch 22/22: [===========] - loss: 8517.8738743413363\n",
      " Epoch 169/1000; Batch 22/22: [===========] - loss: 4109.1288820870426\n",
      " Epoch 170/1000; Batch 22/22: [===========] - loss: 111.00451963921597\n",
      " Epoch 171/1000; Batch 22/22: [===========] - loss: 36.957415977545264\n",
      " Epoch 172/1000; Batch 22/22: [===========] - loss: 416.39758485393752\n",
      " Epoch 173/1000; Batch 22/22: [===========] - loss: 786.73059072038213\n",
      " Epoch 174/1000; Batch 22/22: [===========] - loss: 697.73088806504012\n",
      " Epoch 175/1000; Batch 22/22: [===========] - loss: 2657.4532408923717\n",
      " Epoch 176/1000; Batch 22/22: [===========] - loss: 2561.6931840616326\n",
      " Epoch 177/1000; Batch 22/22: [===========] - loss: 39.600032362224687\n",
      " Epoch 178/1000; Batch 22/22: [===========] - loss: 17.461936759526854\n",
      " Epoch 179/1000; Batch 22/22: [===========] - loss: 7111.3525575690655\n",
      " Epoch 180/1000; Batch 22/22: [===========] - loss: 2262.2553334919903\n",
      " Epoch 181/1000; Batch 22/22: [===========] - loss: 1423.8204833428595\n",
      " Epoch 182/1000; Batch 22/22: [===========] - loss: 67.518671169476753\n",
      " Epoch 183/1000; Batch 22/22: [===========] - loss: 1281.6340173458557\n",
      " Epoch 184/1000; Batch 22/22: [===========] - loss: 2704.4952878350492\n",
      " Epoch 185/1000; Batch 22/22: [===========] - loss: 1296.1883843682798\n",
      " Epoch 186/1000; Batch 22/22: [===========] - loss: 2379.7746225462993\n",
      " Epoch 187/1000; Batch 22/22: [===========] - loss: 2211.5400162559114\n",
      " Epoch 188/1000; Batch 22/22: [===========] - loss: 225.69103473918082\n",
      " Epoch 189/1000; Batch 22/22: [===========] - loss: 2145.7697381988064\n",
      " Epoch 190/1000; Batch 22/22: [===========] - loss: 73.889953153931024\n",
      " Epoch 191/1000; Batch 22/22: [===========] - loss: 1483.6919256433032\n",
      " Epoch 192/1000; Batch 22/22: [===========] - loss: 964.66660557542923\n",
      " Epoch 193/1000; Batch 22/22: [===========] - loss: 1390.3264437029293\n",
      " Epoch 194/1000; Batch 22/22: [===========] - loss: 416.75870482770545\n",
      " Epoch 195/1000; Batch 22/22: [===========] - loss: 820.13050514555574\n",
      " Epoch 196/1000; Batch 22/22: [===========] - loss: 6931.7394219634195\n",
      " Epoch 197/1000; Batch 22/22: [===========] - loss: 2082.9299966926505\n",
      " Epoch 198/1000; Batch 22/22: [===========] - loss: 1097.9658751937194\n",
      " Epoch 199/1000; Batch 22/22: [===========] - loss: 2272.6360736043024\n",
      " Epoch 200/1000; Batch 22/22: [===========] - loss: 1270.6956365825554\n",
      " Epoch 201/1000; Batch 22/22: [===========] - loss: 4722.0354960151935\n",
      " Epoch 202/1000; Batch 22/22: [===========] - loss: 38.954501237793036\n",
      " Epoch 203/1000; Batch 22/22: [===========] - loss: 1288.0585305226043\n",
      " Epoch 204/1000; Batch 22/22: [===========] - loss: 391.76713246027407\n",
      " Epoch 205/1000; Batch 22/22: [===========] - loss: 3608.3801582041065\n",
      " Epoch 206/1000; Batch 22/22: [===========] - loss: 1023.7361034340947\n",
      " Epoch 207/1000; Batch 22/22: [===========] - loss: 1421.4809339622554\n",
      " Epoch 208/1000; Batch 22/22: [===========] - loss: 2390.5328302960194\n",
      " Epoch 209/1000; Batch 22/22: [===========] - loss: 644.87752553226745\n",
      " Epoch 210/1000; Batch 22/22: [===========] - loss: 1053.2497865621515\n",
      " Epoch 211/1000; Batch 22/22: [===========] - loss: 3561.0527380599237\n",
      " Epoch 212/1000; Batch 22/22: [===========] - loss: 657.56013470299728\n",
      " Epoch 213/1000; Batch 22/22: [===========] - loss: 142.80720788118816\n",
      " Epoch 214/1000; Batch 22/22: [===========] - loss: 2122.8267744386946\n",
      " Epoch 215/1000; Batch 22/22: [===========] - loss: 1049.5451759205744\n",
      " Epoch 216/1000; Batch 22/22: [===========] - loss: 77.655137508917294\n",
      " Epoch 217/1000; Batch 22/22: [===========] - loss: 7871.4996083931655\n",
      " Epoch 218/1000; Batch 22/22: [===========] - loss: 243.94089250292672\n",
      " Epoch 219/1000; Batch 22/22: [===========] - loss: 4371.9900513720504\n",
      " Epoch 220/1000; Batch 22/22: [===========] - loss: 2719.9665564156926\n",
      " Epoch 221/1000; Batch 22/22: [===========] - loss: 4499.2532349077374\n",
      " Epoch 222/1000; Batch 22/22: [===========] - loss: 1433.0970001710173\n",
      " Epoch 223/1000; Batch 22/22: [===========] - loss: 867.93532846614355\n",
      " Epoch 224/1000; Batch 22/22: [===========] - loss: 9909.1033009514055\n",
      " Epoch 225/1000; Batch 22/22: [===========] - loss: 4.1222532840092872\n",
      " Epoch 226/1000; Batch 22/22: [===========] - loss: 1287.0604359990676\n",
      " Epoch 227/1000; Batch 22/22: [===========] - loss: 2653.9365263731593\n",
      " Epoch 228/1000; Batch 22/22: [===========] - loss: 3920.5799666214463\n",
      " Epoch 229/1000; Batch 22/22: [===========] - loss: 1494.8181738593662\n",
      " Epoch 230/1000; Batch 22/22: [===========] - loss: 1213.7260924034894\n",
      " Epoch 231/1000; Batch 22/22: [===========] - loss: 216.29146245086733\n",
      " Epoch 232/1000; Batch 22/22: [===========] - loss: 4558.2280134529197\n",
      " Epoch 233/1000; Batch 22/22: [===========] - loss: 4321.4329827568797\n",
      " Epoch 234/1000; Batch 22/22: [===========] - loss: 2889.5850400168194\n",
      " Epoch 235/1000; Batch 22/22: [===========] - loss: 9200.6876685254676\n",
      " Epoch 236/1000; Batch 22/22: [===========] - loss: 1070.2252228062375\n",
      " Epoch 237/1000; Batch 22/22: [===========] - loss: 705.28333196124967\n",
      " Epoch 238/1000; Batch 22/22: [===========] - loss: 508.65910902195276\n",
      " Epoch 239/1000; Batch 22/22: [===========] - loss: 141.65033826837902\n",
      " Epoch 240/1000; Batch 22/22: [===========] - loss: 1394.5795059961488\n",
      " Epoch 241/1000; Batch 22/22: [===========] - loss: 1959.4792472086529\n",
      " Epoch 242/1000; Batch 22/22: [===========] - loss: 1999.8679798175725\n",
      " Epoch 243/1000; Batch 22/22: [===========] - loss: 2064.3618025162688\n",
      " Epoch 244/1000; Batch 22/22: [===========] - loss: 176.25518449396015\n",
      " Epoch 245/1000; Batch 22/22: [===========] - loss: 982.48907626096516\n",
      " Epoch 246/1000; Batch 22/22: [===========] - loss: 1183.3873634982167\n",
      " Epoch 247/1000; Batch 22/22: [===========] - loss: 61.364107872416334\n",
      " Epoch 248/1000; Batch 22/22: [===========] - loss: 2124.9404374576616\n",
      " Epoch 249/1000; Batch 22/22: [===========] - loss: 15.216445872660664\n",
      " Epoch 250/1000; Batch 22/22: [===========] - loss: 5387.3091841559447\n",
      " Epoch 251/1000; Batch 22/22: [===========] - loss: 1506.7771650603917\n",
      " Epoch 252/1000; Batch 22/22: [===========] - loss: 5966.7329957056566\n",
      " Epoch 253/1000; Batch 22/22: [===========] - loss: 30.613737141544327\n",
      " Epoch 254/1000; Batch 22/22: [===========] - loss: 0.06760927340845946\n",
      " Epoch 255/1000; Batch 22/22: [===========] - loss: 1215.4171308387727\n",
      " Epoch 256/1000; Batch 22/22: [===========] - loss: 2697.8973510617519\n",
      " Epoch 257/1000; Batch 22/22: [===========] - loss: 1961.6735870634543\n",
      " Epoch 258/1000; Batch 22/22: [===========] - loss: 2030.4978455182952\n",
      " Epoch 259/1000; Batch 22/22: [===========] - loss: 2302.5247101947393\n",
      " Epoch 260/1000; Batch 22/22: [===========] - loss: 332.77563712641517\n",
      " Epoch 261/1000; Batch 22/22: [===========] - loss: 46.434897830371664\n",
      " Epoch 262/1000; Batch 22/22: [===========] - loss: 5270.5367589790238\n",
      " Epoch 263/1000; Batch 22/22: [===========] - loss: 4.6187368903702646\n",
      " Epoch 264/1000; Batch 22/22: [===========] - loss: 3089.7905424896635\n",
      " Epoch 265/1000; Batch 22/22: [===========] - loss: 10737.186001929986\n",
      " Epoch 266/1000; Batch 22/22: [===========] - loss: 480.15098065801494\n",
      " Epoch 267/1000; Batch 22/22: [===========] - loss: 2236.2506153611074\n",
      " Epoch 268/1000; Batch 22/22: [===========] - loss: 1853.2594122934924\n",
      " Epoch 269/1000; Batch 22/22: [===========] - loss: 627.10407891228653\n",
      " Epoch 270/1000; Batch 22/22: [===========] - loss: 9396.4005603733248\n",
      " Epoch 271/1000; Batch 22/22: [===========] - loss: 4140.0971678391626\n",
      " Epoch 272/1000; Batch 22/22: [===========] - loss: 939.52141672146015\n",
      " Epoch 273/1000; Batch 22/22: [===========] - loss: 145.82378100995396\n",
      " Epoch 274/1000; Batch 22/22: [===========] - loss: 1953.7933464180771\n",
      " Epoch 275/1000; Batch 22/22: [===========] - loss: 0.08712244104439436\n",
      " Epoch 276/1000; Batch 22/22: [===========] - loss: 257.21182386766793\n",
      " Epoch 277/1000; Batch 22/22: [===========] - loss: 1456.2403243405747\n",
      " Epoch 278/1000; Batch 22/22: [===========] - loss: 905.40878887749566\n",
      " Epoch 279/1000; Batch 22/22: [===========] - loss: 1244.9046520793832\n",
      " Epoch 280/1000; Batch 22/22: [===========] - loss: 662.67510319055774\n",
      " Epoch 281/1000; Batch 22/22: [===========] - loss: 8416.3571450417319\n",
      " Epoch 282/1000; Batch 22/22: [===========] - loss: 2322.9795601860253\n",
      " Epoch 283/1000; Batch 22/22: [===========] - loss: 779.13517637806186\n",
      " Epoch 284/1000; Batch 22/22: [===========] - loss: 1505.2409521759654\n",
      " Epoch 285/1000; Batch 22/22: [===========] - loss: 4153.0984143878627\n",
      " Epoch 286/1000; Batch 22/22: [===========] - loss: 2956.8159759450833\n",
      " Epoch 287/1000; Batch 22/22: [===========] - loss: 2384.3472640469897\n",
      " Epoch 288/1000; Batch 22/22: [===========] - loss: 707.54224981144449\n",
      " Epoch 289/1000; Batch 22/22: [===========] - loss: 1610.2616525255507\n",
      " Epoch 290/1000; Batch 22/22: [===========] - loss: 449.95605334136974\n",
      " Epoch 291/1000; Batch 22/22: [===========] - loss: 1481.6452105428436\n",
      " Epoch 292/1000; Batch 22/22: [===========] - loss: 38.722046112844682\n",
      " Epoch 293/1000; Batch 22/22: [===========] - loss: 288.11024376341362\n",
      " Epoch 294/1000; Batch 22/22: [===========] - loss: 1920.3329777183217\n",
      " Epoch 295/1000; Batch 22/22: [===========] - loss: 2388.3123957830862\n",
      " Epoch 296/1000; Batch 22/22: [===========] - loss: 514.57390748137938\n",
      " Epoch 297/1000; Batch 22/22: [===========] - loss: 11.492635898557928\n",
      " Epoch 298/1000; Batch 22/22: [===========] - loss: 1658.3408660319925\n",
      " Epoch 299/1000; Batch 22/22: [===========] - loss: 1902.6111164448046\n",
      " Epoch 300/1000; Batch 22/22: [===========] - loss: 781.99283075413765\n",
      " Epoch 301/1000; Batch 22/22: [===========] - loss: 0.9898147274365218\n",
      " Epoch 302/1000; Batch 22/22: [===========] - loss: 3710.3611208667513\n",
      " Epoch 303/1000; Batch 22/22: [===========] - loss: 4161.6910284193197\n",
      " Epoch 304/1000; Batch 22/22: [===========] - loss: 2936.5660694701322\n",
      " Epoch 305/1000; Batch 22/22: [===========] - loss: 44.748583659935328\n",
      " Epoch 306/1000; Batch 22/22: [===========] - loss: 2070.8476569116015\n",
      " Epoch 307/1000; Batch 22/22: [===========] - loss: 832.03886289547684\n",
      " Epoch 308/1000; Batch 22/22: [===========] - loss: 264.05435503225326\n",
      " Epoch 309/1000; Batch 22/22: [===========] - loss: 4078.1714561178114\n",
      " Epoch 310/1000; Batch 22/22: [===========] - loss: 6542.4212446603264\n",
      " Epoch 311/1000; Batch 22/22: [===========] - loss: 1203.7771089910188\n",
      " Epoch 312/1000; Batch 22/22: [===========] - loss: 3036.2337399430294\n",
      " Epoch 313/1000; Batch 22/22: [===========] - loss: 4032.8568667101657\n",
      " Epoch 314/1000; Batch 22/22: [===========] - loss: 804.68427231333254\n",
      " Epoch 315/1000; Batch 22/22: [===========] - loss: 701.37929620557717\n",
      " Epoch 316/1000; Batch 22/22: [===========] - loss: 2.1067007641234703\n",
      " Epoch 317/1000; Batch 22/22: [===========] - loss: 503.47581485156426\n",
      " Epoch 318/1000; Batch 22/22: [===========] - loss: 127.52801085919668\n",
      " Epoch 319/1000; Batch 22/22: [===========] - loss: 1200.9911748175057\n",
      " Epoch 320/1000; Batch 22/22: [===========] - loss: 38.794486875574292\n",
      " Epoch 321/1000; Batch 22/22: [===========] - loss: 2041.6690251299208\n",
      " Epoch 322/1000; Batch 22/22: [===========] - loss: 319.52813982624764\n",
      " Epoch 323/1000; Batch 22/22: [===========] - loss: 1914.9840789375276\n",
      " Epoch 324/1000; Batch 22/22: [===========] - loss: 391.79129861603504\n",
      " Epoch 325/1000; Batch 22/22: [===========] - loss: 28.261623445316715\n",
      " Epoch 326/1000; Batch 22/22: [===========] - loss: 1607.7804642056824\n",
      " Epoch 327/1000; Batch 22/22: [===========] - loss: 144.75777847206142\n",
      " Epoch 328/1000; Batch 22/22: [===========] - loss: 727.01310177710715\n",
      " Epoch 329/1000; Batch 22/22: [===========] - loss: 263.17189793509704\n",
      " Epoch 330/1000; Batch 22/22: [===========] - loss: 14.925154882257841\n",
      " Epoch 331/1000; Batch 22/22: [===========] - loss: 1420.8796427319112\n",
      " Epoch 332/1000; Batch 22/22: [===========] - loss: 6.0872480307700253\n",
      " Epoch 333/1000; Batch 22/22: [===========] - loss: 145.73713062143204\n",
      " Epoch 334/1000; Batch 22/22: [===========] - loss: 3157.2121542613268\n",
      " Epoch 335/1000; Batch 22/22: [===========] - loss: 784.83939144832356\n",
      " Epoch 336/1000; Batch 22/22: [===========] - loss: 2793.3602026001578\n",
      " Epoch 337/1000; Batch 22/22: [===========] - loss: 1479.3323645355606\n",
      " Epoch 338/1000; Batch 22/22: [===========] - loss: 1.4885444334613471\n",
      " Epoch 339/1000; Batch 22/22: [===========] - loss: 1790.6201125374612\n",
      " Epoch 340/1000; Batch 22/22: [===========] - loss: 4750.9449749784037\n",
      " Epoch 341/1000; Batch 22/22: [===========] - loss: 10190.361269823055\n",
      " Epoch 342/1000; Batch 22/22: [===========] - loss: 5.0825163479216828\n",
      " Epoch 343/1000; Batch 22/22: [===========] - loss: 139.41099915980473\n",
      " Epoch 344/1000; Batch 22/22: [===========] - loss: 410.12589224319372\n",
      " Epoch 345/1000; Batch 22/22: [===========] - loss: 4424.6178017780685\n",
      " Epoch 346/1000; Batch 22/22: [===========] - loss: 415.09760832266236\n",
      " Epoch 347/1000; Batch 22/22: [===========] - loss: 1601.2860126131006\n",
      " Epoch 348/1000; Batch 22/22: [===========] - loss: 113.03176358707744\n",
      " Epoch 349/1000; Batch 22/22: [===========] - loss: 6304.3997220966136\n",
      " Epoch 350/1000; Batch 22/22: [===========] - loss: 687.30996731462313\n",
      " Epoch 351/1000; Batch 22/22: [===========] - loss: 879.43848314895188\n",
      " Epoch 352/1000; Batch 22/22: [===========] - loss: 1498.4105548306043\n",
      " Epoch 353/1000; Batch 22/22: [===========] - loss: 0.03611593159803112\n",
      " Epoch 354/1000; Batch 22/22: [===========] - loss: 4850.1905957618264\n",
      " Epoch 355/1000; Batch 22/22: [===========] - loss: 935.91030775134662\n",
      " Epoch 356/1000; Batch 22/22: [===========] - loss: 1819.8170932190903\n",
      " Epoch 357/1000; Batch 22/22: [===========] - loss: 2.5434974670909307\n",
      " Epoch 358/1000; Batch 22/22: [===========] - loss: 759.84626394968935\n",
      " Epoch 359/1000; Batch 22/22: [===========] - loss: 729.37276168874277\n",
      " Epoch 360/1000; Batch 22/22: [===========] - loss: 596.95324011220767\n",
      " Epoch 361/1000; Batch 22/22: [===========] - loss: 725.85264854985019\n",
      " Epoch 362/1000; Batch 22/22: [===========] - loss: 7623.4149320627573\n",
      " Epoch 363/1000; Batch 22/22: [===========] - loss: 682.77607609225038\n",
      " Epoch 364/1000; Batch 22/22: [===========] - loss: 4584.9378353161686\n",
      " Epoch 365/1000; Batch 22/22: [===========] - loss: 920.99479536686337\n",
      " Epoch 366/1000; Batch 22/22: [===========] - loss: 1342.8807938728037\n",
      " Epoch 367/1000; Batch 22/22: [===========] - loss: 717.88258268012785\n",
      " Epoch 368/1000; Batch 22/22: [===========] - loss: 908.25567737944118\n",
      " Epoch 369/1000; Batch 22/22: [===========] - loss: 1128.4277318794896\n",
      " Epoch 370/1000; Batch 22/22: [===========] - loss: 1422.6607564152134\n",
      " Epoch 371/1000; Batch 22/22: [===========] - loss: 703.00300130766374\n",
      " Epoch 372/1000; Batch 22/22: [===========] - loss: 77.902599589455597\n",
      " Epoch 373/1000; Batch 22/22: [===========] - loss: 2291.0944457376613\n",
      " Epoch 374/1000; Batch 22/22: [===========] - loss: 183.49765358169964\n",
      " Epoch 375/1000; Batch 22/22: [===========] - loss: 166.20106255416462\n",
      " Epoch 376/1000; Batch 22/22: [===========] - loss: 2321.2626083983123\n",
      " Epoch 377/1000; Batch 22/22: [===========] - loss: 700.34137795452327\n",
      " Epoch 378/1000; Batch 22/22: [===========] - loss: 4570.9990535205496\n",
      " Epoch 379/1000; Batch 22/22: [===========] - loss: 2305.0275944335515\n",
      " Epoch 380/1000; Batch 22/22: [===========] - loss: 33.437177692380835\n",
      " Epoch 381/1000; Batch 22/22: [===========] - loss: 226.63222596668646\n",
      " Epoch 382/1000; Batch 22/22: [===========] - loss: 9.9800371379469355\n",
      " Epoch 383/1000; Batch 22/22: [===========] - loss: 2624.6927750772043\n",
      " Epoch 384/1000; Batch 22/22: [===========] - loss: 13.586729319999417\n",
      " Epoch 385/1000; Batch 22/22: [===========] - loss: 269.18209211575476\n",
      " Epoch 386/1000; Batch 22/22: [===========] - loss: 1118.2235789959152\n",
      " Epoch 387/1000; Batch 22/22: [===========] - loss: 768.79791933668148\n",
      " Epoch 388/1000; Batch 22/22: [===========] - loss: 258.99206744345626\n",
      " Epoch 389/1000; Batch 22/22: [===========] - loss: 162.24683184680464\n",
      " Epoch 390/1000; Batch 22/22: [===========] - loss: 1276.6069787930342\n",
      " Epoch 391/1000; Batch 22/22: [===========] - loss: 184.10464563921278\n",
      " Epoch 392/1000; Batch 22/22: [===========] - loss: 1274.4671001229453\n",
      " Epoch 393/1000; Batch 22/22: [===========] - loss: 215.78601060965647\n",
      " Epoch 394/1000; Batch 22/22: [===========] - loss: 538.75096525963457\n",
      " Epoch 395/1000; Batch 22/22: [===========] - loss: 2245.4930974066297\n",
      " Epoch 396/1000; Batch 22/22: [===========] - loss: 688.57112434542195\n",
      " Epoch 397/1000; Batch 22/22: [===========] - loss: 3619.6852226441766\n",
      " Epoch 398/1000; Batch 22/22: [===========] - loss: 571.73104948852347\n",
      " Epoch 399/1000; Batch 22/22: [===========] - loss: 16.324625940536226\n",
      " Epoch 400/1000; Batch 22/22: [===========] - loss: 1318.3955937529595\n",
      " Epoch 401/1000; Batch 22/22: [===========] - loss: 74.186238086061415\n",
      " Epoch 402/1000; Batch 22/22: [===========] - loss: 274.07975711659555\n",
      " Epoch 403/1000; Batch 22/22: [===========] - loss: 1893.5004678893615\n",
      " Epoch 404/1000; Batch 22/22: [===========] - loss: 267.76707169036288\n",
      " Epoch 405/1000; Batch 22/22: [===========] - loss: 3138.9041446321317\n",
      " Epoch 406/1000; Batch 22/22: [===========] - loss: 885.08957990152867\n",
      " Epoch 407/1000; Batch 22/22: [===========] - loss: 13.430113992211894\n",
      " Epoch 408/1000; Batch 22/22: [===========] - loss: 220.65927403414167\n",
      " Epoch 409/1000; Batch 22/22: [===========] - loss: 457.86467242390154\n",
      " Epoch 410/1000; Batch 22/22: [===========] - loss: 1046.7324592686994\n",
      " Epoch 411/1000; Batch 22/22: [===========] - loss: 1323.9487067072082\n",
      " Epoch 412/1000; Batch 22/22: [===========] - loss: 4047.9694828538186\n",
      " Epoch 413/1000; Batch 22/22: [===========] - loss: 2513.8940070083017\n",
      " Epoch 414/1000; Batch 22/22: [===========] - loss: 3401.5647241420587\n",
      " Epoch 415/1000; Batch 22/22: [===========] - loss: 35.816324505295775\n",
      " Epoch 416/1000; Batch 22/22: [===========] - loss: 1013.6375788357432\n",
      " Epoch 417/1000; Batch 22/22: [===========] - loss: 905.10186431752895\n",
      " Epoch 418/1000; Batch 22/22: [===========] - loss: 7691.4088927764841\n",
      " Epoch 419/1000; Batch 22/22: [===========] - loss: 30.809437226056524\n",
      " Epoch 420/1000; Batch 22/22: [===========] - loss: 3082.1762215769554\n",
      " Epoch 421/1000; Batch 22/22: [===========] - loss: 489.45622241084317\n",
      " Epoch 422/1000; Batch 22/22: [===========] - loss: 22.875656027256757\n",
      " Epoch 423/1000; Batch 22/22: [===========] - loss: 978.04655564217964\n",
      " Epoch 424/1000; Batch 22/22: [===========] - loss: 3437.2687845583832\n",
      " Epoch 425/1000; Batch 22/22: [===========] - loss: 187.15620238271816\n",
      " Epoch 426/1000; Batch 22/22: [===========] - loss: 1251.2216518720143\n",
      " Epoch 427/1000; Batch 22/22: [===========] - loss: 1748.3637969349888\n",
      " Epoch 428/1000; Batch 22/22: [===========] - loss: 752.55728776994259\n",
      " Epoch 429/1000; Batch 22/22: [===========] - loss: 3347.3175428593604\n",
      " Epoch 430/1000; Batch 22/22: [===========] - loss: 617.76108082687394\n",
      " Epoch 431/1000; Batch 22/22: [===========] - loss: 5260.9561416806851\n",
      " Epoch 432/1000; Batch 22/22: [===========] - loss: 408.27599131493336\n",
      " Epoch 433/1000; Batch 22/22: [===========] - loss: 1692.6598692688717\n",
      " Epoch 434/1000; Batch 22/22: [===========] - loss: 3818.6171586893931\n",
      " Epoch 435/1000; Batch 22/22: [===========] - loss: 0.20756859646250825\n",
      " Epoch 436/1000; Batch 22/22: [===========] - loss: 147.16715165064417\n",
      " Epoch 437/1000; Batch 22/22: [===========] - loss: 1199.8734081214648\n",
      " Epoch 438/1000; Batch 22/22: [===========] - loss: 597.67911944671827\n",
      " Epoch 439/1000; Batch 22/22: [===========] - loss: 839.02672991483737\n",
      " Epoch 440/1000; Batch 22/22: [===========] - loss: 3548.1900571524698\n",
      " Epoch 441/1000; Batch 22/22: [===========] - loss: 776.62872835277857\n",
      " Epoch 442/1000; Batch 22/22: [===========] - loss: 1777.6149082843378\n",
      " Epoch 443/1000; Batch 22/22: [===========] - loss: 3346.8599703797771\n",
      " Epoch 444/1000; Batch 22/22: [===========] - loss: 125.41999051572665\n",
      " Epoch 445/1000; Batch 22/22: [===========] - loss: 1600.1364572305524\n",
      " Epoch 446/1000; Batch 22/22: [===========] - loss: 209.86957978262083\n",
      " Epoch 447/1000; Batch 22/22: [===========] - loss: 3140.0119778531157\n",
      " Epoch 448/1000; Batch 22/22: [===========] - loss: 107.16937512253979\n",
      " Epoch 449/1000; Batch 22/22: [===========] - loss: 5771.5160514978319\n",
      " Epoch 450/1000; Batch 22/22: [===========] - loss: 82.935915011772328\n",
      " Epoch 451/1000; Batch 22/22: [===========] - loss: 3754.7325969209574\n",
      " Epoch 452/1000; Batch 22/22: [===========] - loss: 972.29554898432646\n",
      " Epoch 453/1000; Batch 22/22: [===========] - loss: 668.65711266196764\n",
      " Epoch 454/1000; Batch 22/22: [===========] - loss: 3248.8675019104543\n",
      " Epoch 455/1000; Batch 22/22: [===========] - loss: 9294.8057130928727\n",
      " Epoch 456/1000; Batch 22/22: [===========] - loss: 4290.3558533795285\n",
      " Epoch 457/1000; Batch 22/22: [===========] - loss: 3304.7380697582193\n",
      " Epoch 458/1000; Batch 22/22: [===========] - loss: 2994.0210062867227\n",
      " Epoch 459/1000; Batch 22/22: [===========] - loss: 1936.3963722663011\n",
      " Epoch 460/1000; Batch 22/22: [===========] - loss: 1418.6171692484634\n",
      " Epoch 461/1000; Batch 22/22: [===========] - loss: 6446.9329324915934\n",
      " Epoch 462/1000; Batch 22/22: [===========] - loss: 149.31693741867672\n",
      " Epoch 463/1000; Batch 22/22: [===========] - loss: 1300.9793865330003\n",
      " Epoch 464/1000; Batch 22/22: [===========] - loss: 1708.7796311089378\n",
      " Epoch 465/1000; Batch 22/22: [===========] - loss: 74.817483976490686\n",
      " Epoch 466/1000; Batch 22/22: [===========] - loss: 86.796638859393252\n",
      " Epoch 467/1000; Batch 22/22: [===========] - loss: 3637.7057357156073\n",
      " Epoch 468/1000; Batch 22/22: [===========] - loss: 812.90998646708267\n",
      " Epoch 469/1000; Batch 22/22: [===========] - loss: 93.999399563150238\n",
      " Epoch 470/1000; Batch 22/22: [===========] - loss: 1200.8406635967126\n",
      " Epoch 471/1000; Batch 22/22: [===========] - loss: 3790.2908922998654\n",
      " Epoch 472/1000; Batch 22/22: [===========] - loss: 647.03792899813233\n",
      " Epoch 473/1000; Batch 22/22: [===========] - loss: 1224.0455435585486\n",
      " Epoch 474/1000; Batch 22/22: [===========] - loss: 699.70709844483147\n",
      " Epoch 475/1000; Batch 22/22: [===========] - loss: 3376.3062850298625\n",
      " Epoch 476/1000; Batch 22/22: [===========] - loss: 839.04126641912597\n",
      " Epoch 477/1000; Batch 22/22: [===========] - loss: 95.079437765243796\n",
      " Epoch 478/1000; Batch 22/22: [===========] - loss: 5442.7674978524552\n",
      " Epoch 479/1000; Batch 22/22: [===========] - loss: 1662.0583744971377\n",
      " Epoch 480/1000; Batch 22/22: [===========] - loss: 204.16233730037567\n",
      " Epoch 481/1000; Batch 22/22: [===========] - loss: 2430.1956430961386\n",
      " Epoch 482/1000; Batch 22/22: [===========] - loss: 82.007854122705853\n",
      " Epoch 483/1000; Batch 22/22: [===========] - loss: 665.41106634142486\n",
      " Epoch 484/1000; Batch 22/22: [===========] - loss: 357.03759364228273\n",
      " Epoch 485/1000; Batch 22/22: [===========] - loss: 146.13234474712766\n",
      " Epoch 486/1000; Batch 22/22: [===========] - loss: 3586.6905307651365\n",
      " Epoch 487/1000; Batch 22/22: [===========] - loss: 310.49571408906147\n",
      " Epoch 488/1000; Batch 22/22: [===========] - loss: 454.68932069459296\n",
      " Epoch 489/1000; Batch 22/22: [===========] - loss: 2653.9763005428446\n",
      " Epoch 490/1000; Batch 22/22: [===========] - loss: 74.495396860940288\n",
      " Epoch 491/1000; Batch 22/22: [===========] - loss: 2992.0037422403534\n",
      " Epoch 492/1000; Batch 22/22: [===========] - loss: 362.39794489004126\n",
      " Epoch 493/1000; Batch 22/22: [===========] - loss: 1835.6425685531722\n",
      " Epoch 494/1000; Batch 22/22: [===========] - loss: 3840.2287886215995\n",
      " Epoch 495/1000; Batch 22/22: [===========] - loss: 29.277722429598345\n",
      " Epoch 496/1000; Batch 22/22: [===========] - loss: 9.5691090809229745\n",
      " Epoch 497/1000; Batch 22/22: [===========] - loss: 159.61781952700553\n",
      " Epoch 498/1000; Batch 22/22: [===========] - loss: 666.77584527852512\n",
      " Epoch 499/1000; Batch 22/22: [===========] - loss: 181.01309924975664\n",
      " Epoch 500/1000; Batch 22/22: [===========] - loss: 889.59068328079094\n",
      " Epoch 501/1000; Batch 22/22: [===========] - loss: 1576.8757499404085\n",
      " Epoch 502/1000; Batch 22/22: [===========] - loss: 145.48004063310424\n",
      " Epoch 503/1000; Batch 22/22: [===========] - loss: 961.20244601237157\n",
      " Epoch 504/1000; Batch 22/22: [===========] - loss: 442.64057498923588\n",
      " Epoch 505/1000; Batch 22/22: [===========] - loss: 85.317540959826028\n",
      " Epoch 506/1000; Batch 22/22: [===========] - loss: 383.33368084860588\n",
      " Epoch 507/1000; Batch 22/22: [===========] - loss: 3079.5985791074854\n",
      " Epoch 508/1000; Batch 22/22: [===========] - loss: 1959.5352079248519\n",
      " Epoch 509/1000; Batch 22/22: [===========] - loss: 7479.6962782783145\n",
      " Epoch 510/1000; Batch 22/22: [===========] - loss: 322.04040007882775\n",
      " Epoch 511/1000; Batch 22/22: [===========] - loss: 532.99996031547036\n",
      " Epoch 512/1000; Batch 22/22: [===========] - loss: 92.408904786924647\n",
      " Epoch 513/1000; Batch 22/22: [===========] - loss: 1129.9407478475887\n",
      " Epoch 514/1000; Batch 22/22: [===========] - loss: 2019.9169572930068\n",
      " Epoch 515/1000; Batch 22/22: [===========] - loss: 1513.0447446106164\n",
      " Epoch 516/1000; Batch 22/22: [===========] - loss: 488.32766469908364\n",
      " Epoch 517/1000; Batch 22/22: [===========] - loss: 0.006444888440244982\n",
      " Epoch 518/1000; Batch 22/22: [===========] - loss: 1661.6044572226583\n",
      " Epoch 519/1000; Batch 22/22: [===========] - loss: 2306.8162645027737\n",
      " Epoch 520/1000; Batch 22/22: [===========] - loss: 3063.1308835571353\n",
      " Epoch 521/1000; Batch 22/22: [===========] - loss: 0.5411371087762087\n",
      " Epoch 522/1000; Batch 22/22: [===========] - loss: 2252.9417939473365\n",
      " Epoch 523/1000; Batch 22/22: [===========] - loss: 1703.1691813451578\n",
      " Epoch 524/1000; Batch 22/22: [===========] - loss: 5066.5171478322518\n",
      " Epoch 525/1000; Batch 22/22: [===========] - loss: 660.76373762466029\n",
      " Epoch 526/1000; Batch 22/22: [===========] - loss: 228.82215137317266\n",
      " Epoch 527/1000; Batch 22/22: [===========] - loss: 652.18165154505836\n",
      " Epoch 528/1000; Batch 22/22: [===========] - loss: 2928.4027644738744\n",
      " Epoch 529/1000; Batch 22/22: [===========] - loss: 3552.4848763788236\n",
      " Epoch 530/1000; Batch 22/22: [===========] - loss: 2007.4767931981883\n",
      " Epoch 531/1000; Batch 22/22: [===========] - loss: 513.17026876656017\n",
      " Epoch 532/1000; Batch 22/22: [===========] - loss: 2.3678598961479825\n",
      " Epoch 533/1000; Batch 22/22: [===========] - loss: 26.486142442484226\n",
      " Epoch 534/1000; Batch 22/22: [===========] - loss: 2406.2574819994875\n",
      " Epoch 535/1000; Batch 22/22: [===========] - loss: 213.56104963957727\n",
      " Epoch 536/1000; Batch 22/22: [===========] - loss: 2846.2514487603194\n",
      " Epoch 537/1000; Batch 22/22: [===========] - loss: 3445.0022098681498\n",
      " Epoch 538/1000; Batch 22/22: [===========] - loss: 320.54369865358858\n",
      " Epoch 539/1000; Batch 22/22: [===========] - loss: 64.835295654783516\n",
      " Epoch 540/1000; Batch 22/22: [===========] - loss: 1760.4331049238833\n",
      " Epoch 541/1000; Batch 22/22: [===========] - loss: 3852.0607454029396\n",
      " Epoch 542/1000; Batch 22/22: [===========] - loss: 1901.0767018729696\n",
      " Epoch 543/1000; Batch 22/22: [===========] - loss: 1437.3552560382143\n",
      " Epoch 544/1000; Batch 22/22: [===========] - loss: 971.34149612971775\n",
      " Epoch 545/1000; Batch 22/22: [===========] - loss: 2077.4648590703923\n",
      " Epoch 546/1000; Batch 22/22: [===========] - loss: 1446.5180816179645\n",
      " Epoch 547/1000; Batch 22/22: [===========] - loss: 532.53026860605646\n",
      " Epoch 548/1000; Batch 22/22: [===========] - loss: 5257.5105829144473\n",
      " Epoch 549/1000; Batch 22/22: [===========] - loss: 1688.1664924800446\n",
      " Epoch 550/1000; Batch 22/22: [===========] - loss: 3.3925998518480167\n",
      " Epoch 551/1000; Batch 22/22: [===========] - loss: 330.73182282838343\n",
      " Epoch 552/1000; Batch 22/22: [===========] - loss: 1177.3945973088216\n",
      " Epoch 553/1000; Batch 22/22: [===========] - loss: 2310.3499285413993\n",
      " Epoch 554/1000; Batch 22/22: [===========] - loss: 329.30417533672955\n",
      " Epoch 555/1000; Batch 22/22: [===========] - loss: 202.22366573458967\n",
      " Epoch 556/1000; Batch 22/22: [===========] - loss: 4070.8636354061778\n",
      " Epoch 557/1000; Batch 22/22: [===========] - loss: 908.30709025863293\n",
      " Epoch 558/1000; Batch 22/22: [===========] - loss: 1936.7816799078196\n",
      " Epoch 559/1000; Batch 22/22: [===========] - loss: 442.83757000038946\n",
      " Epoch 560/1000; Batch 22/22: [===========] - loss: 779.66614376068166\n",
      " Epoch 561/1000; Batch 22/22: [===========] - loss: 28.097569941136726\n",
      " Epoch 562/1000; Batch 22/22: [===========] - loss: 519.50560586979053\n",
      " Epoch 563/1000; Batch 22/22: [===========] - loss: 1865.8239343503562\n",
      " Epoch 564/1000; Batch 22/22: [===========] - loss: 3487.2768060544886\n",
      " Epoch 565/1000; Batch 22/22: [===========] - loss: 2.9831072888134136\n",
      " Epoch 566/1000; Batch 22/22: [===========] - loss: 410.35343700118807\n",
      " Epoch 567/1000; Batch 22/22: [===========] - loss: 1779.2961101892517\n",
      " Epoch 568/1000; Batch 22/22: [===========] - loss: 678.41879759157377\n",
      " Epoch 569/1000; Batch 22/22: [===========] - loss: 1566.2175690007898\n",
      " Epoch 570/1000; Batch 22/22: [===========] - loss: 2352.6161671653317\n",
      " Epoch 571/1000; Batch 22/22: [===========] - loss: 208.75750248172426\n",
      " Epoch 572/1000; Batch 22/22: [===========] - loss: 136.19838338125487\n",
      " Epoch 573/1000; Batch 22/22: [===========] - loss: 3413.7374510486877\n",
      " Epoch 574/1000; Batch 22/22: [===========] - loss: 1717.6493368273773\n",
      " Epoch 575/1000; Batch 22/22: [===========] - loss: 1965.5437755141857\n",
      " Epoch 576/1000; Batch 22/22: [===========] - loss: 4781.3608600078673\n",
      " Epoch 577/1000; Batch 22/22: [===========] - loss: 8362.8285663377723\n",
      " Epoch 578/1000; Batch 22/22: [===========] - loss: 323.91430437477567\n",
      " Epoch 579/1000; Batch 22/22: [===========] - loss: 2532.5016074849838\n",
      " Epoch 580/1000; Batch 22/22: [===========] - loss: 11.156378450653815\n",
      " Epoch 581/1000; Batch 22/22: [===========] - loss: 484.86494837387912\n",
      " Epoch 582/1000; Batch 22/22: [===========] - loss: 255.05585667606005\n",
      " Epoch 583/1000; Batch 22/22: [===========] - loss: 762.97398253296637\n",
      " Epoch 584/1000; Batch 22/22: [===========] - loss: 2028.8481602678746\n",
      " Epoch 585/1000; Batch 22/22: [===========] - loss: 509.82565975441661\n",
      " Epoch 586/1000; Batch 22/22: [===========] - loss: 3367.2152212193005\n",
      " Epoch 587/1000; Batch 22/22: [===========] - loss: 1822.4788395670691\n",
      " Epoch 588/1000; Batch 22/22: [===========] - loss: 865.89154807219646\n",
      " Epoch 589/1000; Batch 22/22: [===========] - loss: 209.16456827197934\n",
      " Epoch 590/1000; Batch 22/22: [===========] - loss: 433.64115915455454\n",
      " Epoch 591/1000; Batch 22/22: [===========] - loss: 3821.0583099738656\n",
      " Epoch 592/1000; Batch 22/22: [===========] - loss: 1225.7627248121419\n",
      " Epoch 593/1000; Batch 22/22: [===========] - loss: 1005.5982175504289\n",
      " Epoch 594/1000; Batch 22/22: [===========] - loss: 523.51237630790565\n",
      " Epoch 595/1000; Batch 22/22: [===========] - loss: 1389.6700215527399\n",
      " Epoch 596/1000; Batch 22/22: [===========] - loss: 1014.9353611882966\n",
      " Epoch 597/1000; Batch 22/22: [===========] - loss: 2.9628323058126963\n",
      " Epoch 598/1000; Batch 22/22: [===========] - loss: 527.42735093168228\n",
      " Epoch 599/1000; Batch 22/22: [===========] - loss: 10.971315547946539\n",
      " Epoch 600/1000; Batch 22/22: [===========] - loss: 1306.6943649477393\n",
      " Epoch 601/1000; Batch 22/22: [===========] - loss: 1804.9531084463886\n",
      " Epoch 602/1000; Batch 22/22: [===========] - loss: 1943.1004812172923\n",
      " Epoch 603/1000; Batch 22/22: [===========] - loss: 1.1298637818813968\n",
      " Epoch 604/1000; Batch 22/22: [===========] - loss: 1413.3169756178045\n",
      " Epoch 605/1000; Batch 22/22: [===========] - loss: 6718.3271970902162\n",
      " Epoch 606/1000; Batch 22/22: [===========] - loss: 865.59568160736868\n",
      " Epoch 607/1000; Batch 22/22: [===========] - loss: 3140.9620558709335\n",
      " Epoch 608/1000; Batch 22/22: [===========] - loss: 6605.7137958675568\n",
      " Epoch 609/1000; Batch 22/22: [===========] - loss: 875.19117172020844\n",
      " Epoch 610/1000; Batch 22/22: [===========] - loss: 7747.7079449847764\n",
      " Epoch 611/1000; Batch 22/22: [===========] - loss: 282.64828437168679\n",
      " Epoch 612/1000; Batch 22/22: [===========] - loss: 1004.9616578072148\n",
      " Epoch 613/1000; Batch 22/22: [===========] - loss: 1779.4776573667757\n",
      " Epoch 614/1000; Batch 22/22: [===========] - loss: 40.801248415813294\n",
      " Epoch 615/1000; Batch 22/22: [===========] - loss: 2129.9473365208187\n",
      " Epoch 616/1000; Batch 22/22: [===========] - loss: 8309.9065193041306\n",
      " Epoch 617/1000; Batch 22/22: [===========] - loss: 3599.3259350432345\n",
      " Epoch 618/1000; Batch 22/22: [===========] - loss: 4098.5087820358117\n",
      " Epoch 619/1000; Batch 22/22: [===========] - loss: 2332.1779020413364\n",
      " Epoch 620/1000; Batch 22/22: [===========] - loss: 415.28074648665955\n",
      " Epoch 621/1000; Batch 22/22: [===========] - loss: 2090.1451935020523\n",
      " Epoch 622/1000; Batch 22/22: [===========] - loss: 406.34697404782304\n",
      " Epoch 623/1000; Batch 22/22: [===========] - loss: 1743.4381935915853\n",
      " Epoch 624/1000; Batch 22/22: [===========] - loss: 2251.5280841891126\n",
      " Epoch 625/1000; Batch 22/22: [===========] - loss: 3260.1780269024407\n",
      " Epoch 626/1000; Batch 22/22: [===========] - loss: 585.39665973183892\n",
      " Epoch 627/1000; Batch 22/22: [===========] - loss: 3747.0773909659125\n",
      " Epoch 628/1000; Batch 22/22: [===========] - loss: 1689.1889125978314\n",
      " Epoch 629/1000; Batch 22/22: [===========] - loss: 8.8074629312385793\n",
      " Epoch 630/1000; Batch 22/22: [===========] - loss: 1151.4837417007595\n",
      " Epoch 631/1000; Batch 22/22: [===========] - loss: 0.4338720767710663\n",
      " Epoch 632/1000; Batch 22/22: [===========] - loss: 113.63346899320202\n",
      " Epoch 633/1000; Batch 22/22: [===========] - loss: 2463.3255177022648\n",
      " Epoch 634/1000; Batch 22/22: [===========] - loss: 1281.6227795411605\n",
      " Epoch 635/1000; Batch 22/22: [===========] - loss: 87.675468495728471\n",
      " Epoch 636/1000; Batch 22/22: [===========] - loss: 6147.6854747354748\n",
      " Epoch 637/1000; Batch 22/22: [===========] - loss: 1177.9501997964637\n",
      " Epoch 638/1000; Batch 22/22: [===========] - loss: 6114.1383783107091\n",
      " Epoch 639/1000; Batch 22/22: [===========] - loss: 92.476672497693778\n",
      " Epoch 640/1000; Batch 22/22: [===========] - loss: 316.86001441493571\n",
      " Epoch 641/1000; Batch 22/22: [===========] - loss: 443.89053771047974\n",
      " Epoch 642/1000; Batch 22/22: [===========] - loss: 5602.5101678639856\n",
      " Epoch 643/1000; Batch 22/22: [===========] - loss: 1069.5639479259316\n",
      " Epoch 644/1000; Batch 22/22: [===========] - loss: 1754.5100397858748\n",
      " Epoch 645/1000; Batch 22/22: [===========] - loss: 1682.5171172379003\n",
      " Epoch 646/1000; Batch 22/22: [===========] - loss: 3830.4099726649481\n",
      " Epoch 647/1000; Batch 22/22: [===========] - loss: 579.09912301309021\n",
      " Epoch 648/1000; Batch 22/22: [===========] - loss: 5796.7084062460848\n",
      " Epoch 649/1000; Batch 22/22: [===========] - loss: 2879.6622819331214\n",
      " Epoch 650/1000; Batch 22/22: [===========] - loss: 2220.9958112882614\n",
      " Epoch 651/1000; Batch 22/22: [===========] - loss: 790.93324286614393\n",
      " Epoch 652/1000; Batch 22/22: [===========] - loss: 77.541944083450337\n",
      " Epoch 653/1000; Batch 22/22: [===========] - loss: 2804.0524614930355\n",
      " Epoch 654/1000; Batch 22/22: [===========] - loss: 2208.6229085248133\n",
      " Epoch 655/1000; Batch 22/22: [===========] - loss: 661.58418999260623\n",
      " Epoch 656/1000; Batch 22/22: [===========] - loss: 0.4906422655862002\n",
      " Epoch 657/1000; Batch 22/22: [===========] - loss: 180.57235108368207\n",
      " Epoch 658/1000; Batch 22/22: [===========] - loss: 3256.8517450541985\n",
      " Epoch 659/1000; Batch 22/22: [===========] - loss: 993.73173914270958\n",
      " Epoch 660/1000; Batch 22/22: [===========] - loss: 72.164402202164358\n",
      " Epoch 661/1000; Batch 22/22: [===========] - loss: 0.7265279382485985\n",
      " Epoch 662/1000; Batch 22/22: [===========] - loss: 3349.6065218601184\n",
      " Epoch 663/1000; Batch 22/22: [===========] - loss: 3238.8593744103932\n",
      " Epoch 664/1000; Batch 22/22: [===========] - loss: 98.507687835290754\n",
      " Epoch 665/1000; Batch 22/22: [===========] - loss: 4638.5225374271015\n",
      " Epoch 666/1000; Batch 22/22: [===========] - loss: 1732.9592757982982\n",
      " Epoch 667/1000; Batch 22/22: [===========] - loss: 0.7658215189348354\n",
      " Epoch 668/1000; Batch 22/22: [===========] - loss: 108.92481245030113\n",
      " Epoch 669/1000; Batch 22/22: [===========] - loss: 37.139129204211476\n",
      " Epoch 670/1000; Batch 22/22: [===========] - loss: 30.174759217688123\n",
      " Epoch 671/1000; Batch 22/22: [===========] - loss: 690.62826227797545\n",
      " Epoch 672/1000; Batch 22/22: [===========] - loss: 1294.5131997559401\n",
      " Epoch 673/1000; Batch 22/22: [===========] - loss: 95.328324701885932\n",
      " Epoch 674/1000; Batch 22/22: [===========] - loss: 7396.3239368528787\n",
      " Epoch 675/1000; Batch 22/22: [===========] - loss: 1120.0600841486073\n",
      " Epoch 676/1000; Batch 22/22: [===========] - loss: 31.606660840979864\n",
      " Epoch 677/1000; Batch 22/22: [===========] - loss: 934.29489928849471\n",
      " Epoch 678/1000; Batch 22/22: [===========] - loss: 481.25520271760685\n",
      " Epoch 679/1000; Batch 22/22: [===========] - loss: 1075.9378539922836\n",
      " Epoch 680/1000; Batch 22/22: [===========] - loss: 430.44485168858055\n",
      " Epoch 681/1000; Batch 22/22: [===========] - loss: 1320.8668932500015\n",
      " Epoch 682/1000; Batch 22/22: [===========] - loss: 212.47859791695536\n",
      " Epoch 683/1000; Batch 22/22: [===========] - loss: 429.02426704286874\n",
      " Epoch 684/1000; Batch 22/22: [===========] - loss: 1061.1358310376631\n",
      " Epoch 685/1000; Batch 22/22: [===========] - loss: 0.9934752423842063\n",
      " Epoch 686/1000; Batch 22/22: [===========] - loss: 2720.4855474201838\n",
      " Epoch 687/1000; Batch 22/22: [===========] - loss: 2395.4980361666184\n",
      " Epoch 688/1000; Batch 22/22: [===========] - loss: 1043.3660050795793\n",
      " Epoch 689/1000; Batch 22/22: [===========] - loss: 196.22429087580358\n",
      " Epoch 690/1000; Batch 22/22: [===========] - loss: 80.616749485899065\n",
      " Epoch 691/1000; Batch 22/22: [===========] - loss: 76.827127605735957\n",
      " Epoch 692/1000; Batch 22/22: [===========] - loss: 191.55488340082766\n",
      " Epoch 693/1000; Batch 22/22: [===========] - loss: 381.48261773715359\n",
      " Epoch 694/1000; Batch 22/22: [===========] - loss: 3133.9539816286565\n",
      " Epoch 695/1000; Batch 22/22: [===========] - loss: 3246.1666612793365\n",
      " Epoch 696/1000; Batch 22/22: [===========] - loss: 1413.8144409280852\n",
      " Epoch 697/1000; Batch 22/22: [===========] - loss: 1707.1583917765573\n",
      " Epoch 698/1000; Batch 22/22: [===========] - loss: 926.47883758019296\n",
      " Epoch 699/1000; Batch 22/22: [===========] - loss: 1914.1791636314865\n",
      " Epoch 700/1000; Batch 22/22: [===========] - loss: 15.998338845830022\n",
      " Epoch 701/1000; Batch 22/22: [===========] - loss: 216.22057734715167\n",
      " Epoch 702/1000; Batch 22/22: [===========] - loss: 505.96142156326932\n",
      " Epoch 703/1000; Batch 22/22: [===========] - loss: 3977.8272478942844\n",
      " Epoch 704/1000; Batch 22/22: [===========] - loss: 40.905535947574466\n",
      " Epoch 705/1000; Batch 22/22: [===========] - loss: 2130.4066854410644\n",
      " Epoch 706/1000; Batch 22/22: [===========] - loss: 3864.9740236034913\n",
      " Epoch 707/1000; Batch 22/22: [===========] - loss: 1899.9129293727786\n",
      " Epoch 708/1000; Batch 22/22: [===========] - loss: 3448.1763710334667\n",
      " Epoch 709/1000; Batch 22/22: [===========] - loss: 59.840298166873764\n",
      " Epoch 710/1000; Batch 22/22: [===========] - loss: 6154.1966449634183\n",
      " Epoch 711/1000; Batch 22/22: [===========] - loss: 1837.3850847253182\n",
      " Epoch 712/1000; Batch 22/22: [===========] - loss: 2161.3376100548303\n",
      " Epoch 713/1000; Batch 22/22: [===========] - loss: 405.38616240025766\n",
      " Epoch 714/1000; Batch 22/22: [===========] - loss: 402.41336608293421\n",
      " Epoch 715/1000; Batch 22/22: [===========] - loss: 61.982903478217072\n",
      " Epoch 716/1000; Batch 22/22: [===========] - loss: 3880.8322247339474\n",
      " Epoch 717/1000; Batch 22/22: [===========] - loss: 0.1341467610846766\n",
      " Epoch 718/1000; Batch 22/22: [===========] - loss: 5604.9557085068824\n",
      " Epoch 719/1000; Batch 22/22: [===========] - loss: 1306.5197938438023\n",
      " Epoch 720/1000; Batch 22/22: [===========] - loss: 705.21764286849753\n",
      " Epoch 721/1000; Batch 22/22: [===========] - loss: 278.74456359876297\n",
      " Epoch 722/1000; Batch 22/22: [===========] - loss: 1368.9750274389822\n",
      " Epoch 723/1000; Batch 22/22: [===========] - loss: 713.22601264051497\n",
      " Epoch 724/1000; Batch 22/22: [===========] - loss: 111.47121695860146\n",
      " Epoch 725/1000; Batch 22/22: [===========] - loss: 104.10311718804225\n",
      " Epoch 726/1000; Batch 22/22: [===========] - loss: 9.4888092560026524\n",
      " Epoch 727/1000; Batch 22/22: [===========] - loss: 2607.1026537059878\n",
      " Epoch 728/1000; Batch 22/22: [===========] - loss: 2.4270512276063974\n",
      " Epoch 729/1000; Batch 22/22: [===========] - loss: 377.30623815621592\n",
      " Epoch 730/1000; Batch 22/22: [===========] - loss: 1561.0345295637906\n",
      " Epoch 731/1000; Batch 22/22: [===========] - loss: 4332.4651365344682\n",
      " Epoch 732/1000; Batch 22/22: [===========] - loss: 1083.2858526032305\n",
      " Epoch 733/1000; Batch 22/22: [===========] - loss: 489.96744310870923\n",
      " Epoch 734/1000; Batch 22/22: [===========] - loss: 2370.4244411192944\n",
      " Epoch 735/1000; Batch 22/22: [===========] - loss: 995.63473928824012\n",
      " Epoch 736/1000; Batch 22/22: [===========] - loss: 379.66006530987914\n",
      " Epoch 737/1000; Batch 22/22: [===========] - loss: 644.60255918137723\n",
      " Epoch 738/1000; Batch 22/22: [===========] - loss: 272.14943435254037\n",
      " Epoch 739/1000; Batch 22/22: [===========] - loss: 585.33597916203643\n",
      " Epoch 740/1000; Batch 22/22: [===========] - loss: 1583.4370082026971\n",
      " Epoch 741/1000; Batch 22/22: [===========] - loss: 365.47915772501372\n",
      " Epoch 742/1000; Batch 22/22: [===========] - loss: 5.7395544502721945\n",
      " Epoch 743/1000; Batch 22/22: [===========] - loss: 3991.8643422519644\n",
      " Epoch 744/1000; Batch 22/22: [===========] - loss: 240.93069151281452\n",
      " Epoch 745/1000; Batch 22/22: [===========] - loss: 136.64056430992432\n",
      " Epoch 746/1000; Batch 22/22: [===========] - loss: 1861.9873581215676\n",
      " Epoch 747/1000; Batch 22/22: [===========] - loss: 371.91854690620277\n",
      " Epoch 748/1000; Batch 22/22: [===========] - loss: 91.432152217765166\n",
      " Epoch 749/1000; Batch 22/22: [===========] - loss: 790.24670862426915\n",
      " Epoch 750/1000; Batch 22/22: [===========] - loss: 6893.4843302194379\n",
      " Epoch 751/1000; Batch 22/22: [===========] - loss: 1209.3365161632537\n",
      " Epoch 752/1000; Batch 22/22: [===========] - loss: 2763.6605241880682\n",
      " Epoch 753/1000; Batch 22/22: [===========] - loss: 11.468811677886288\n",
      " Epoch 754/1000; Batch 22/22: [===========] - loss: 2701.4299959024605\n",
      " Epoch 755/1000; Batch 22/22: [===========] - loss: 18.119249910331344\n",
      " Epoch 756/1000; Batch 22/22: [===========] - loss: 3741.3431679391474\n",
      " Epoch 757/1000; Batch 22/22: [===========] - loss: 379.29278040060053\n",
      " Epoch 758/1000; Batch 22/22: [===========] - loss: 6669.1606565334942\n",
      " Epoch 759/1000; Batch 22/22: [===========] - loss: 14.804237267298074\n",
      " Epoch 760/1000; Batch 22/22: [===========] - loss: 4551.2980747680015\n",
      " Epoch 761/1000; Batch 22/22: [===========] - loss: 6187.8234918609974\n",
      " Epoch 762/1000; Batch 22/22: [===========] - loss: 2259.4910321269213\n",
      " Epoch 763/1000; Batch 22/22: [===========] - loss: 808.51764348692772\n",
      " Epoch 764/1000; Batch 22/22: [===========] - loss: 227.98182623475083\n",
      " Epoch 765/1000; Batch 22/22: [===========] - loss: 2956.3374763772526\n",
      " Epoch 766/1000; Batch 22/22: [===========] - loss: 1554.2533606541237\n",
      " Epoch 767/1000; Batch 22/22: [===========] - loss: 246.74826777977674\n",
      " Epoch 768/1000; Batch 22/22: [===========] - loss: 2989.3868293405364\n",
      " Epoch 769/1000; Batch 22/22: [===========] - loss: 326.52748966504034\n",
      " Epoch 770/1000; Batch 22/22: [===========] - loss: 410.03690701380816\n",
      " Epoch 771/1000; Batch 22/22: [===========] - loss: 3839.9188280934827\n",
      " Epoch 772/1000; Batch 22/22: [===========] - loss: 4866.8269797189319\n",
      " Epoch 773/1000; Batch 22/22: [===========] - loss: 27.741896329483133\n",
      " Epoch 774/1000; Batch 22/22: [===========] - loss: 1238.5422959425312\n",
      " Epoch 775/1000; Batch 22/22: [===========] - loss: 2927.1889194149229\n",
      " Epoch 776/1000; Batch 22/22: [===========] - loss: 620.64162137681946\n",
      " Epoch 777/1000; Batch 22/22: [===========] - loss: 9109.9684646825953\n",
      " Epoch 778/1000; Batch 22/22: [===========] - loss: 2313.9864510717316\n",
      " Epoch 779/1000; Batch 22/22: [===========] - loss: 431.52166902149772\n",
      " Epoch 780/1000; Batch 22/22: [===========] - loss: 8.4918541311262876\n",
      " Epoch 781/1000; Batch 22/22: [===========] - loss: 23.568032570428065\n",
      " Epoch 782/1000; Batch 22/22: [===========] - loss: 443.39546869064499\n",
      " Epoch 783/1000; Batch 22/22: [===========] - loss: 931.32024776887775\n",
      " Epoch 784/1000; Batch 22/22: [===========] - loss: 565.87073083552644\n",
      " Epoch 785/1000; Batch 22/22: [===========] - loss: 21.296397720433887\n",
      " Epoch 786/1000; Batch 22/22: [===========] - loss: 151.69910741660627\n",
      " Epoch 787/1000; Batch 22/22: [===========] - loss: 1431.4044206101955\n",
      " Epoch 788/1000; Batch 22/22: [===========] - loss: 839.47806070223266\n",
      " Epoch 789/1000; Batch 22/22: [===========] - loss: 737.71108610984726\n",
      " Epoch 790/1000; Batch 22/22: [===========] - loss: 35.562557683034174\n",
      " Epoch 791/1000; Batch 22/22: [===========] - loss: 1686.3107832546833\n",
      " Epoch 792/1000; Batch 22/22: [===========] - loss: 192.46613328695295\n",
      " Epoch 793/1000; Batch 22/22: [===========] - loss: 4238.5423127133996\n",
      " Epoch 794/1000; Batch 22/22: [===========] - loss: 750.59218808536544\n",
      " Epoch 795/1000; Batch 22/22: [===========] - loss: 2747.2564663535148\n",
      " Epoch 796/1000; Batch 22/22: [===========] - loss: 54.959292471267467\n",
      " Epoch 797/1000; Batch 22/22: [===========] - loss: 1472.8872892963739\n",
      " Epoch 798/1000; Batch 22/22: [===========] - loss: 1428.8032533710567\n",
      " Epoch 799/1000; Batch 22/22: [===========] - loss: 897.76584795161465\n",
      " Epoch 800/1000; Batch 22/22: [===========] - loss: 715.40412807209937\n",
      " Epoch 801/1000; Batch 22/22: [===========] - loss: 1925.4496706627792\n",
      " Epoch 802/1000; Batch 22/22: [===========] - loss: 4289.1505450747163\n",
      " Epoch 803/1000; Batch 22/22: [===========] - loss: 8604.1179995363168\n",
      " Epoch 804/1000; Batch 22/22: [===========] - loss: 360.22719587268038\n",
      " Epoch 805/1000; Batch 22/22: [===========] - loss: 20.905532102044276\n",
      " Epoch 806/1000; Batch 22/22: [===========] - loss: 175.92443867169632\n",
      " Epoch 807/1000; Batch 22/22: [===========] - loss: 5308.5166781518255\n",
      " Epoch 808/1000; Batch 22/22: [===========] - loss: 9620.5402264020183\n",
      " Epoch 809/1000; Batch 22/22: [===========] - loss: 204.82193203470837\n",
      " Epoch 810/1000; Batch 22/22: [===========] - loss: 3573.3996084113878\n",
      " Epoch 811/1000; Batch 22/22: [===========] - loss: 991.71428073891463\n",
      " Epoch 812/1000; Batch 22/22: [===========] - loss: 514.91570520471686\n",
      " Epoch 813/1000; Batch 22/22: [===========] - loss: 3644.8064464724694\n",
      " Epoch 814/1000; Batch 22/22: [===========] - loss: 961.95784506034372\n",
      " Epoch 815/1000; Batch 22/22: [===========] - loss: 2585.4373673124924\n",
      " Epoch 816/1000; Batch 22/22: [===========] - loss: 4000.9955878819096\n",
      " Epoch 817/1000; Batch 22/22: [===========] - loss: 36.554489662563345\n",
      " Epoch 818/1000; Batch 22/22: [===========] - loss: 3121.9048629006825\n",
      " Epoch 819/1000; Batch 22/22: [===========] - loss: 229.89450778868294\n",
      " Epoch 820/1000; Batch 22/22: [===========] - loss: 41.634198297430096\n",
      " Epoch 821/1000; Batch 22/22: [===========] - loss: 279.50734553251856\n",
      " Epoch 822/1000; Batch 22/22: [===========] - loss: 1372.0085243891447\n",
      " Epoch 823/1000; Batch 22/22: [===========] - loss: 997.59068436506328\n",
      " Epoch 824/1000; Batch 22/22: [===========] - loss: 550.23108496173485\n",
      " Epoch 825/1000; Batch 22/22: [===========] - loss: 9550.3070540391076\n",
      " Epoch 826/1000; Batch 22/22: [===========] - loss: 3004.9530960356993\n",
      " Epoch 827/1000; Batch 22/22: [===========] - loss: 12.587494830182782\n",
      " Epoch 828/1000; Batch 22/22: [===========] - loss: 4754.5115196210782\n",
      " Epoch 829/1000; Batch 22/22: [===========] - loss: 1340.6574821139202\n",
      " Epoch 830/1000; Batch 22/22: [===========] - loss: 50.474520860091557\n",
      " Epoch 831/1000; Batch 22/22: [===========] - loss: 844.59055096088287\n",
      " Epoch 832/1000; Batch 22/22: [===========] - loss: 18.002606869293196\n",
      " Epoch 833/1000; Batch 22/22: [===========] - loss: 0.1714573595410836\n",
      " Epoch 834/1000; Batch 22/22: [===========] - loss: 829.35546716648135\n",
      " Epoch 835/1000; Batch 22/22: [===========] - loss: 1158.1219977307173\n",
      " Epoch 836/1000; Batch 22/22: [===========] - loss: 28.422869453316692\n",
      " Epoch 837/1000; Batch 22/22: [===========] - loss: 13.992009503908986\n",
      " Epoch 838/1000; Batch 22/22: [===========] - loss: 178.76664541317685\n",
      " Epoch 839/1000; Batch 22/22: [===========] - loss: 6521.6425991458895\n",
      " Epoch 840/1000; Batch 22/22: [===========] - loss: 184.98930158239068\n",
      " Epoch 841/1000; Batch 22/22: [===========] - loss: 1700.3695507252844\n",
      " Epoch 842/1000; Batch 22/22: [===========] - loss: 169.72942372091404\n",
      " Epoch 843/1000; Batch 22/22: [===========] - loss: 194.12133291773569\n",
      " Epoch 844/1000; Batch 22/22: [===========] - loss: 871.94999542763687\n",
      " Epoch 845/1000; Batch 22/22: [===========] - loss: 2288.1683940053126\n",
      " Epoch 846/1000; Batch 22/22: [===========] - loss: 821.80612421773058\n",
      " Epoch 847/1000; Batch 22/22: [===========] - loss: 67.468027744956274\n",
      " Epoch 848/1000; Batch 22/22: [===========] - loss: 2802.3537927978628\n",
      " Epoch 849/1000; Batch 22/22: [===========] - loss: 59.251404905456556\n",
      " Epoch 850/1000; Batch 22/22: [===========] - loss: 113.74321549398407\n",
      " Epoch 851/1000; Batch 22/22: [===========] - loss: 1273.3571388483788\n",
      " Epoch 852/1000; Batch 22/22: [===========] - loss: 194.13762521119943\n",
      " Epoch 853/1000; Batch 22/22: [===========] - loss: 2467.2025663041413\n",
      " Epoch 854/1000; Batch 22/22: [===========] - loss: 115.17908387951384\n",
      " Epoch 855/1000; Batch 22/22: [===========] - loss: 54.643782772403695\n",
      " Epoch 856/1000; Batch 22/22: [===========] - loss: 5.6944151993333877\n",
      " Epoch 857/1000; Batch 22/22: [===========] - loss: 8.1049910997175095\n",
      " Epoch 858/1000; Batch 22/22: [===========] - loss: 269.27143143365697\n",
      " Epoch 859/1000; Batch 22/22: [===========] - loss: 2355.5434102067044\n",
      " Epoch 860/1000; Batch 22/22: [===========] - loss: 3555.7239266219544\n",
      " Epoch 861/1000; Batch 22/22: [===========] - loss: 18.930596983677693\n",
      " Epoch 862/1000; Batch 22/22: [===========] - loss: 338.61679749478157\n",
      " Epoch 863/1000; Batch 22/22: [===========] - loss: 3273.5070758782726\n",
      " Epoch 864/1000; Batch 22/22: [===========] - loss: 1641.2420944770745\n",
      " Epoch 865/1000; Batch 22/22: [===========] - loss: 4.0554930131988583\n",
      " Epoch 866/1000; Batch 22/22: [===========] - loss: 1433.0981120855838\n",
      " Epoch 867/1000; Batch 22/22: [===========] - loss: 619.52990560103337\n",
      " Epoch 868/1000; Batch 22/22: [===========] - loss: 31.083472347642903\n",
      " Epoch 869/1000; Batch 22/22: [===========] - loss: 71.599207994306016\n",
      " Epoch 870/1000; Batch 22/22: [===========] - loss: 3228.0598150534385\n",
      " Epoch 871/1000; Batch 22/22: [===========] - loss: 960.82858357582885\n",
      " Epoch 872/1000; Batch 22/22: [===========] - loss: 1229.6943639180304\n",
      " Epoch 873/1000; Batch 22/22: [===========] - loss: 800.97346874975048\n",
      " Epoch 874/1000; Batch 22/22: [===========] - loss: 577.52756056057648\n",
      " Epoch 875/1000; Batch 22/22: [===========] - loss: 6925.7319228046142\n",
      " Epoch 876/1000; Batch 22/22: [===========] - loss: 639.74941454164044\n",
      " Epoch 877/1000; Batch 22/22: [===========] - loss: 4842.6691704513736\n",
      " Epoch 878/1000; Batch 22/22: [===========] - loss: 561.88348252673694\n",
      " Epoch 879/1000; Batch 22/22: [===========] - loss: 31.605031559131955\n",
      " Epoch 880/1000; Batch 22/22: [===========] - loss: 1478.1629718287347\n",
      " Epoch 881/1000; Batch 22/22: [===========] - loss: 1622.7018491084498\n",
      " Epoch 882/1000; Batch 22/22: [===========] - loss: 0.9114803502418147\n",
      " Epoch 883/1000; Batch 22/22: [===========] - loss: 3308.0259823437254\n",
      " Epoch 884/1000; Batch 22/22: [===========] - loss: 0.2690997380466595\n",
      " Epoch 885/1000; Batch 22/22: [===========] - loss: 987.62457615799857\n",
      " Epoch 886/1000; Batch 22/22: [===========] - loss: 1052.5888083244126\n",
      " Epoch 887/1000; Batch 22/22: [===========] - loss: 1075.2617883814935\n",
      " Epoch 888/1000; Batch 22/22: [===========] - loss: 49.432903526893442\n",
      " Epoch 889/1000; Batch 22/22: [===========] - loss: 22.269873205770878\n",
      " Epoch 890/1000; Batch 22/22: [===========] - loss: 4160.9922068620965\n",
      " Epoch 891/1000; Batch 22/22: [===========] - loss: 594.84360641743395\n",
      " Epoch 892/1000; Batch 22/22: [===========] - loss: 10.701596085906344\n",
      " Epoch 893/1000; Batch 22/22: [===========] - loss: 2527.8360319286747\n",
      " Epoch 894/1000; Batch 22/22: [===========] - loss: 2544.5456137050978\n",
      " Epoch 895/1000; Batch 22/22: [===========] - loss: 594.53509654102986\n",
      " Epoch 896/1000; Batch 22/22: [===========] - loss: 29.990160462906635\n",
      " Epoch 897/1000; Batch 22/22: [===========] - loss: 6319.7331701849192\n",
      " Epoch 898/1000; Batch 22/22: [===========] - loss: 1131.6083499038318\n",
      " Epoch 899/1000; Batch 22/22: [===========] - loss: 163.61768126650873\n",
      " Epoch 900/1000; Batch 22/22: [===========] - loss: 14.936769305983926\n",
      " Epoch 901/1000; Batch 22/22: [===========] - loss: 114.25977956476385\n",
      " Epoch 902/1000; Batch 22/22: [===========] - loss: 255.16607396511924\n",
      " Epoch 903/1000; Batch 22/22: [===========] - loss: 2501.7290072650613\n",
      " Epoch 904/1000; Batch 22/22: [===========] - loss: 264.70405223825118\n",
      " Epoch 905/1000; Batch 22/22: [===========] - loss: 5328.8652144459495\n",
      " Epoch 906/1000; Batch 22/22: [===========] - loss: 6444.6550210557793\n",
      " Epoch 907/1000; Batch 22/22: [===========] - loss: 86.265812288608718\n",
      " Epoch 908/1000; Batch 22/22: [===========] - loss: 19.531001065144043\n",
      " Epoch 909/1000; Batch 22/22: [===========] - loss: 2234.4176360541833\n",
      " Epoch 910/1000; Batch 22/22: [===========] - loss: 5120.1170480507113\n",
      " Epoch 911/1000; Batch 22/22: [===========] - loss: 2506.9647807069177\n",
      " Epoch 912/1000; Batch 22/22: [===========] - loss: 5225.9871010810482\n",
      " Epoch 913/1000; Batch 22/22: [===========] - loss: 320.84790532553933\n",
      " Epoch 914/1000; Batch 22/22: [===========] - loss: 2211.4712409045035\n",
      " Epoch 915/1000; Batch 22/22: [===========] - loss: 162.94014362711195\n",
      " Epoch 916/1000; Batch 22/22: [===========] - loss: 1738.9302156365498\n",
      " Epoch 917/1000; Batch 22/22: [===========] - loss: 4572.6486563615517\n",
      " Epoch 918/1000; Batch 22/22: [===========] - loss: 55.114957330982697\n",
      " Epoch 919/1000; Batch 22/22: [===========] - loss: 780.70565592754099\n",
      " Epoch 920/1000; Batch 22/22: [===========] - loss: 3842.1901686358142\n",
      " Epoch 921/1000; Batch 22/22: [===========] - loss: 142.22773115191097\n",
      " Epoch 922/1000; Batch 22/22: [===========] - loss: 15.071525111986217\n",
      " Epoch 923/1000; Batch 22/22: [===========] - loss: 1996.1222412706188\n",
      " Epoch 924/1000; Batch 22/22: [===========] - loss: 7090.9218398773046\n",
      " Epoch 925/1000; Batch 22/22: [===========] - loss: 575.09106568037275\n",
      " Epoch 926/1000; Batch 22/22: [===========] - loss: 467.67164129769891\n",
      " Epoch 927/1000; Batch 22/22: [===========] - loss: 1308.8133551501096\n",
      " Epoch 928/1000; Batch 22/22: [===========] - loss: 1041.1534380369298\n",
      " Epoch 929/1000; Batch 22/22: [===========] - loss: 3295.7592862319098\n",
      " Epoch 930/1000; Batch 22/22: [===========] - loss: 819.85625516686564\n",
      " Epoch 931/1000; Batch 22/22: [===========] - loss: 941.05109123931422\n",
      " Epoch 932/1000; Batch 22/22: [===========] - loss: 90.669619824851344\n",
      " Epoch 933/1000; Batch 22/22: [===========] - loss: 225.59268225776165\n",
      " Epoch 934/1000; Batch 22/22: [===========] - loss: 6577.5952641536442\n",
      " Epoch 935/1000; Batch 22/22: [===========] - loss: 586.26289926672291\n",
      " Epoch 936/1000; Batch 22/22: [===========] - loss: 2013.6831512876486\n",
      " Epoch 937/1000; Batch 22/22: [===========] - loss: 14.659935613788306\n",
      " Epoch 938/1000; Batch 22/22: [===========] - loss: 899.74932801741065\n",
      " Epoch 939/1000; Batch 22/22: [===========] - loss: 5195.9456970170334\n",
      " Epoch 940/1000; Batch 22/22: [===========] - loss: 481.04452682369482\n",
      " Epoch 941/1000; Batch 22/22: [===========] - loss: 98.421053493517874\n",
      " Epoch 942/1000; Batch 22/22: [===========] - loss: 2821.0516514973872\n",
      " Epoch 943/1000; Batch 22/22: [===========] - loss: 1018.8713610068608\n",
      " Epoch 944/1000; Batch 22/22: [===========] - loss: 51.837924987305684\n",
      " Epoch 945/1000; Batch 22/22: [===========] - loss: 1173.4861959567943\n",
      " Epoch 946/1000; Batch 22/22: [===========] - loss: 69.252580147870658\n",
      " Epoch 947/1000; Batch 22/22: [===========] - loss: 1513.6245843308437\n",
      " Epoch 948/1000; Batch 22/22: [===========] - loss: 781.04935608033225\n",
      " Epoch 949/1000; Batch 22/22: [===========] - loss: 472.26302672614396\n",
      " Epoch 950/1000; Batch 22/22: [===========] - loss: 6446.1818943484418\n",
      " Epoch 951/1000; Batch 22/22: [===========] - loss: 2536.3504739135087\n",
      " Epoch 952/1000; Batch 22/22: [===========] - loss: 2508.2750329187847\n",
      " Epoch 953/1000; Batch 22/22: [===========] - loss: 839.11746321217083\n",
      " Epoch 954/1000; Batch 22/22: [===========] - loss: 1275.3307500521441\n",
      " Epoch 955/1000; Batch 22/22: [===========] - loss: 539.74204038196587\n",
      " Epoch 956/1000; Batch 22/22: [===========] - loss: 1459.3123565409983\n",
      " Epoch 957/1000; Batch 22/22: [===========] - loss: 2735.7532769078775\n",
      " Epoch 958/1000; Batch 22/22: [===========] - loss: 321.22371565499362\n",
      " Epoch 959/1000; Batch 22/22: [===========] - loss: 2336.8015873565264\n",
      " Epoch 960/1000; Batch 22/22: [===========] - loss: 433.33924433925466\n",
      " Epoch 961/1000; Batch 22/22: [===========] - loss: 369.82965584549719\n",
      " Epoch 962/1000; Batch 22/22: [===========] - loss: 1093.4465476013343\n",
      " Epoch 963/1000; Batch 22/22: [===========] - loss: 1380.5373467551516\n",
      " Epoch 964/1000; Batch 22/22: [===========] - loss: 1408.7699343634688\n",
      " Epoch 965/1000; Batch 22/22: [===========] - loss: 35.652051521441148\n",
      " Epoch 966/1000; Batch 22/22: [===========] - loss: 615.68724632064953\n",
      " Epoch 967/1000; Batch 22/22: [===========] - loss: 2196.4042254397647\n",
      " Epoch 968/1000; Batch 22/22: [===========] - loss: 2233.6347476987826\n",
      " Epoch 969/1000; Batch 22/22: [===========] - loss: 1363.8207503632414\n",
      " Epoch 970/1000; Batch 22/22: [===========] - loss: 1347.9545905421883\n",
      " Epoch 971/1000; Batch 22/22: [===========] - loss: 1464.1671875065442\n",
      " Epoch 972/1000; Batch 22/22: [===========] - loss: 538.64765799717747\n",
      " Epoch 973/1000; Batch 22/22: [===========] - loss: 1729.0015082795153\n",
      " Epoch 974/1000; Batch 22/22: [===========] - loss: 260.99340298825385\n",
      " Epoch 975/1000; Batch 22/22: [===========] - loss: 658.02067086695742\n",
      " Epoch 976/1000; Batch 22/22: [===========] - loss: 568.35371625151237\n",
      " Epoch 977/1000; Batch 22/22: [===========] - loss: 1339.0843722528507\n",
      " Epoch 978/1000; Batch 22/22: [===========] - loss: 6464.3369814168483\n",
      " Epoch 979/1000; Batch 22/22: [===========] - loss: 909.56199462622591\n",
      " Epoch 980/1000; Batch 22/22: [===========] - loss: 511.67767368528405\n",
      " Epoch 981/1000; Batch 22/22: [===========] - loss: 21.140724937087615\n",
      " Epoch 982/1000; Batch 22/22: [===========] - loss: 2715.0005046967544\n",
      " Epoch 983/1000; Batch 22/22: [===========] - loss: 636.03331829158393\n",
      " Epoch 984/1000; Batch 22/22: [===========] - loss: 803.04953636461787\n",
      " Epoch 985/1000; Batch 22/22: [===========] - loss: 884.67102343608817\n",
      " Epoch 986/1000; Batch 22/22: [===========] - loss: 48.881553854973774\n",
      " Epoch 987/1000; Batch 22/22: [===========] - loss: 814.95811042441494\n",
      " Epoch 988/1000; Batch 22/22: [===========] - loss: 3056.1300116719565\n",
      " Epoch 989/1000; Batch 22/22: [===========] - loss: 1042.8980849249513\n",
      " Epoch 990/1000; Batch 22/22: [===========] - loss: 1046.6971755219406\n",
      " Epoch 991/1000; Batch 22/22: [===========] - loss: 375.95222730328805\n",
      " Epoch 992/1000; Batch 22/22: [===========] - loss: 322.40143038421803\n",
      " Epoch 993/1000; Batch 22/22: [===========] - loss: 25.869780617400412\n",
      " Epoch 994/1000; Batch 22/22: [===========] - loss: 278.71818672414758\n",
      " Epoch 995/1000; Batch 22/22: [===========] - loss: 512.07146434119932\n",
      " Epoch 996/1000; Batch 22/22: [===========] - loss: 1667.9147789587917\n",
      " Epoch 997/1000; Batch 22/22: [===========] - loss: 393.35244299985806\n",
      " Epoch 998/1000; Batch 22/22: [===========] - loss: 3412.7562319772305\n",
      " Epoch 999/1000; Batch 22/22: [===========] - loss: 4106.6255884667447\n",
      " Epoch 1000/1000; Batch 22/22: [===========] - loss: 212.31384389515318\n"
     ]
    }
   ],
   "source": [
    "nn = NeauralNetwork(layers=[\n",
    "        Layer(units=10, input_layer=True),\n",
    "        # Layer(units=40, activation=\"sigmoid\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=1),\n",
    "    ],\n",
    "    loss_function = \"mse\",\n",
    "    learning_rate=0.0001, \n",
    "    verbose=True,\n",
    "    optimizer=\"adam\",\n",
    "    batch_size = 32,\n",
    "    epochs=1000\n",
    ")\n",
    "\n",
    "y_diab = y_diab.reshape(-1, 1) # Network requirement\n",
    "\n",
    "nn.fit(X_diab, y_diab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
