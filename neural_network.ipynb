{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X_diab, y_diab = load_diabetes(return_X_y=True) # returns diabetes data shapes: (442, 10) and (442,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=60, n_features=10, noise=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(\n",
    "            self, \n",
    "            units, \n",
    "            *, \n",
    "            input_layer: bool = False,\n",
    "            activation: str = \"linear\",\n",
    "            use_bias: bool = True,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer.\n",
    "\n",
    "        Args:\n",
    "            units (int): Count of neurons in the layer.\n",
    "            input_layer (bool, optional): Whether the layer is an input layer. Defaults to False.\n",
    "            activation (str, optional): Activation function for the layer. Can be \"linear\", \"relu\", or \"sigmoid\". Defaults to \"linear\".\n",
    "            use_bias (bool, optional): Whether to use bias in the layer. Defaults to True.\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        self.units = units\n",
    "        self.input_layer = input_layer\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.optimizer = None # Optimizer for layer\n",
    "\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "\n",
    "        self.w = None # Weights matrix\n",
    "        self._weight_gradient = None # Weights derivative matrix\n",
    "        self._bias_gradient = None # Biases derivative vector\n",
    "\n",
    "    def activationFunction(self, z):\n",
    "        \"\"\"\n",
    "        Apply the activation function to the given input.\n",
    "\n",
    "        Args:\n",
    "            z (numpy.ndarray): Input to the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output after applying the activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return z\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(z, np.zeros(z.shape))\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _weightInit(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the weights matrix based on the input size.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of the input.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return # input_layer doesn't need weights\n",
    "\n",
    "        self.w = np.random.normal(loc = 0, scale = 1 / input_size, size=(input_size, self.units))\n",
    "        # Initialize weights matrix using a normal distribution with mean 0 and variance 1 / input_size\n",
    "\n",
    "        self.bias = np.zeros((1, self.units))\n",
    "        # Initialize biases as zeros\n",
    "\n",
    "\n",
    "    def _setOptimizer(self, optimizer, beta_1, beta_2):\n",
    "        \"\"\"\n",
    "        Set the optimizer and initialize optimizer-specific variables.\n",
    "\n",
    "        Args:\n",
    "            optimizer (str): Optimization algorithm to use.\n",
    "            beta_1 (float): Value for the optimizer parameter beta_1.\n",
    "            beta_2 (float): Value for the optimizer parameter beta_2.\n",
    "\n",
    "        Notes:\n",
    "            - Only executed for layers other than the input layer.\n",
    "            - Sets the optimizer and initializes optimizer-specific variables based on the chosen optimizer.\n",
    "            - For each optimizer, the corresponding variables are initialized.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self._b1 = beta_1\n",
    "        self._b2 = beta_2\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.batch_size = 1  # SGD is the same as mini-batch gradient descent when batch_size = 1\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "            # Initialize weight-specific variables for AdaGrad\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "                # Initialize bias-specific variables for AdaGrad\n",
    "\n",
    "        if self.optimizer == 'adam':\n",
    "            self._iter = 0  # Calculate iterations\n",
    "\n",
    "            self._weight_m = np.zeros(self.w.shape)\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "            # Initialize weight-specific variables for Adam\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = np.zeros(self.bias.shape)\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "                # Initialize bias-specific variables for Adam\n",
    "\n",
    "        if self.optimizer == 'rms_prop':\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "            # Initialize weight and bias-specific variables for RMSprop\n",
    "\n",
    "        if self.optimizer == 'gdm':\n",
    "            self._weight_m = np.zeros(self.w.shape)\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = np.zeros(self.bias.shape)\n",
    "            # Initialize weight and bias-specific variables for Gradient Descent with Momentum   \n",
    "\n",
    "    def _activationDerivative(self):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the activation function.\n",
    "\n",
    "        Notes:\n",
    "            Only supports the \"linear\", \"relu\", and \"sigmoid\" activation functions.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return 1\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return (self._output > 0) * 1\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self._output * (1 - self._output)\n",
    "\n",
    "    def _setGrad(self, grad):\n",
    "        \"\"\"\n",
    "        Calculate the gradients of weights and bias for backpropagation.\n",
    "\n",
    "        Args:\n",
    "            grad (numpy.ndarray): Gradient from the previous layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient to be passed to the previous layer.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "        \n",
    "        grad = grad * self._activationDerivative()\n",
    "        self._weight_gradient = self._input.T @ grad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self._bias_gradient = grad.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return grad @ self.w.T\n",
    "    \n",
    "    def _updateGrad(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and bias based on the computed gradients.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "\n",
    "        Notes:\n",
    "            - Only executed for layers other than the input layer.\n",
    "            - Updates the weights and biases based on the computed gradients and the chosen optimizer.\n",
    "            - For each optimizer, the corresponding update rule is applied.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        eps = 10e-8 # Optimizer's epsilon\n",
    "\n",
    "        if self.optimizer == \"gd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._weight_v += np.square(self._weight_gradient)\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._weight_v) + eps)\n",
    "\n",
    "            self.w -= learning_rate_weight * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v += np.square(self._bias_gradient)\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._bias_v) + eps)\n",
    "\n",
    "                self.bias -= learning_rate_bias * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == 'adam':\n",
    "            self._iter += 1\n",
    "\n",
    "            self._weight_m = self._b1 * self._weight_m + (1- self._b1) * self._weight_gradient\n",
    "            self._weight_v = self._b2 * self._weight_v + (1- self._b2) * np.square(self._weight_gradient)\n",
    "\n",
    "            weight_m = self._weight_m / (1 - np.power(self._b1, self._iter))\n",
    "            weight_v = self._weight_v / (1 - np.power(self._b2, self._iter))\n",
    "\n",
    "            self.w -= learning_rate * weight_m / (np.sqrt(weight_v) + eps) # Updating\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = self._b1 * self._bias_m + (1- self._b1) * self._bias_gradient\n",
    "                self._bias_v = self._b2 * self._bias_v + (1- self._b2) * np.square(self._bias_gradient)\n",
    "\n",
    "                bias_m = self._bias_m / (1 - np.power(self._b1, self._iter)) \n",
    "                bias_v = self._bias_v / (1 - np.power(self._b2, self._iter))\n",
    "\n",
    "\n",
    "                self.bias -= learning_rate * bias_m / (np.sqrt(bias_v) + eps) # Updating\n",
    "\n",
    "        \n",
    "        if self.optimizer == 'rms_prop':\n",
    "            self._weight_v = self._b2 * self._weight_v + (1- self._b2) * np.square(self._weight_gradient)\n",
    "\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._weight_v) + eps)\n",
    "\n",
    "            self.w -= learning_rate_weight * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = self._b2 * self._bias_v + (1- self._b2) * np.square(self._bias_gradient)\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._bias_v) + eps)\n",
    "\n",
    "                self.bias -= learning_rate_bias * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == 'gdm':\n",
    "            self._weight_m = self._b2 * self._weight_m + (1 - self._b2) * self._weight_gradient\n",
    "\n",
    "            self.w -= learning_rate * self._weight_m\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = self._b2 * self._bias_m + (1 - self._b2) * self._bias_gradient\n",
    "\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input to the layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output of the layer after applying the activation function.\n",
    "        \"\"\"\n",
    "        if self.input_layer:\n",
    "            return X\n",
    "        \n",
    "        self._input = X\n",
    "        self._output = self.activationFunction(X @ self.w + self.bias)\n",
    "\n",
    "        return self._output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeauralNetwork:\n",
    "    def __init__(\n",
    "            self, \n",
    "            layers: list, \n",
    "            loss_function: str = \"mse\", \n",
    "            learning_rate = 0.01,\n",
    "            verbose: bool = False,\n",
    "            optimizer: str = \"gd\",\n",
    "            epochs: int = 1, \n",
    "            batch_size: int = 32,\n",
    "            beta_1: float = 0.9,\n",
    "            beta_2: float = 0.999\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network.\n",
    "\n",
    "        Args:\n",
    "            layers (list): List of Layer objects defining the network architecture. \n",
    "            loss_function (str, optional): Loss function to use. Defaults to \"mse\".\n",
    "            optimizer (str, optional): Optimization algorithm to use for updating weights during training.\n",
    "                Options include:\n",
    "                - \"gd\" (Gradient Descent): Standard gradient descent.\n",
    "                - \"sgd\" (Stochastic Gradient Descent): Update weights using a single sample at a time.\n",
    "                - \"adagrad\" (Adaptive Gradient): Adjust the learning rate based on the frequency of feature occurrences.\n",
    "                - \"adam\" (Adam): Adaptive Moment Estimation algorithm.\n",
    "                - \"rms_prop\" (Root Mean Square Propagation): Adapt the learning rate based on the moving average of squared gradients.\n",
    "                - \"gdm\" (Gradient Descent with Momentum): Add momentum to the gradient descent algorithm.\n",
    "                Defaults to \"gd\".\n",
    "\n",
    "            learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n",
    "            epochs (int, optional): Number of epochs for training. Defaults to 1.\n",
    "            batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "            verbose (bool, optional): Whether to display training progress. Defaults to False.\n",
    "\n",
    "            beta_1 (float, optional): Parameter for the optimizer. Defaults to 0.9.\n",
    "            beta_2 (float, optional): Parameter for the optimizer. Defaults to 0.999.\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.optimizer = optimizer  # Optimizer for all layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.beta_1 = beta_1  # Optimizer parameters\n",
    "        self.beta_2 = beta_2  # Optimizer parameters\n",
    "\n",
    "        # Weights initializing:\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i]._weightInit(self.layers[i - 1].units)\n",
    "            self.layers[i]._setOptimizer(self.optimizer, self.beta_1, self.beta_2)\n",
    "            # Initialize weights for each layer and set the optimizer\n",
    "\n",
    "\n",
    "    def lossFunction(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the loss between the true values and predicted values.\n",
    "\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True values.\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 0.5 * np.mean(np.linalg.norm(y_pred - y_true, axis=1)**2)\n",
    "\n",
    "        # Can be added\n",
    "\n",
    "    def _lossFunctionDerivative(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 1 / len(y_pred) * (y_pred - y_true)\n",
    "\n",
    "        # Can be added\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the neural network on the given input-output pairs.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "            y (numpy.ndarray): Output data.\n",
    "\n",
    "        Notes:\n",
    "            - Reshape y to a column vector (shape: (n_samples, output_size)).\n",
    "        \"\"\"\n",
    "        batch_separation = [(i, i + self.batch_size) for i in range(0, len(X), self.batch_size)] # Get batch indices\n",
    "        epoch_len = len(batch_separation)\n",
    "\n",
    "        indeces = np.arange(len(X))\n",
    "\n",
    "        for _ in range(self.epochs):    \n",
    "            np.random.shuffle(indeces) # Shuffle the training data\n",
    "\n",
    "            for iter, (i, j) in enumerate(batch_separation):\n",
    "                X_ = X[indeces[i:j]] # Get current batch\n",
    "                y_ = y[indeces[i:j]] # Get current batch\n",
    "\n",
    "                pred = self.forward(X_)\n",
    "\n",
    "                if self.verbose:\n",
    "                    process_percent = int(iter / epoch_len * 10)\n",
    "                    print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter}/{epoch_len}: [{process_percent * '=' + '>' + (10 - process_percent) * '-'}] - loss: {self.lossFunction(y_, pred)}\",end='')\n",
    "                \n",
    "                self.backward(pred, y_)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter + 1}/{epoch_len}: [{11 * '='}] - loss: {self.lossFunction(y_, pred)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform predictions using the trained neural network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted output data.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        X_ = np.copy(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            X_ = layer.call(X_)\n",
    "        return X_\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to update the weights of the network.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = self._lossFunctionDerivative(y_pred, y_true)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer._setGrad(gradient)\n",
    "            layer._updateGrad(self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/1000; Batch 7/7: [===========] - loss: 12665.605654271603\n",
      " Epoch 2/1000; Batch 7/7: [===========] - loss: 13893.465620920238\n",
      " Epoch 3/1000; Batch 7/7: [===========] - loss: 15448.852525260283\n",
      " Epoch 4/1000; Batch 7/7: [===========] - loss: 16100.174722641636\n",
      " Epoch 5/1000; Batch 7/7: [===========] - loss: 13697.544093117058\n",
      " Epoch 6/1000; Batch 7/7: [===========] - loss: 12517.594324157712\n",
      " Epoch 7/1000; Batch 7/7: [===========] - loss: 14199.889740742821\n",
      " Epoch 8/1000; Batch 7/7: [===========] - loss: 15030.671658410201\n",
      " Epoch 9/1000; Batch 7/7: [===========] - loss: 13795.786710743054\n",
      " Epoch 10/1000; Batch 7/7: [===========] - loss: 14490.225147221872\n",
      " Epoch 11/1000; Batch 7/7: [===========] - loss: 14530.842030764168\n",
      " Epoch 12/1000; Batch 7/7: [===========] - loss: 16059.322508722793\n",
      " Epoch 13/1000; Batch 7/7: [===========] - loss: 15569.320750353378\n",
      " Epoch 14/1000; Batch 7/7: [===========] - loss: 17446.271784194145\n",
      " Epoch 15/1000; Batch 7/7: [===========] - loss: 12939.038875118968\n",
      " Epoch 16/1000; Batch 7/7: [===========] - loss: 14120.186695981312\n",
      " Epoch 17/1000; Batch 7/7: [===========] - loss: 14406.466661754142\n",
      " Epoch 18/1000; Batch 7/7: [===========] - loss: 13767.367211813911\n",
      " Epoch 19/1000; Batch 7/7: [===========] - loss: 13611.253432922438\n",
      " Epoch 20/1000; Batch 7/7: [===========] - loss: 15757.909068237335\n",
      " Epoch 21/1000; Batch 7/7: [===========] - loss: 16375.482627232133\n",
      " Epoch 22/1000; Batch 7/7: [===========] - loss: 12314.977212917198\n",
      " Epoch 23/1000; Batch 7/7: [===========] - loss: 13495.096515976436\n",
      " Epoch 24/1000; Batch 7/7: [===========] - loss: 13993.676090695666\n",
      " Epoch 25/1000; Batch 7/7: [===========] - loss: 12787.748613576525\n",
      " Epoch 26/1000; Batch 7/7: [===========] - loss: 11354.291938875678\n",
      " Epoch 27/1000; Batch 7/7: [===========] - loss: 14466.199902139857\n",
      " Epoch 28/1000; Batch 7/7: [===========] - loss: 12361.239777311695\n",
      " Epoch 29/1000; Batch 7/7: [===========] - loss: 13261.529629350287\n",
      " Epoch 30/1000; Batch 7/7: [===========] - loss: 14467.767135264901\n",
      " Epoch 31/1000; Batch 7/7: [===========] - loss: 12720.164651952327\n",
      " Epoch 32/1000; Batch 7/7: [===========] - loss: 14269.955893006276\n",
      " Epoch 33/1000; Batch 7/7: [===========] - loss: 13102.187777048553\n",
      " Epoch 34/1000; Batch 7/7: [===========] - loss: 10642.753526309876\n",
      " Epoch 35/1000; Batch 7/7: [===========] - loss: 14146.221904508908\n",
      " Epoch 36/1000; Batch 7/7: [===========] - loss: 11419.990643659785\n",
      " Epoch 37/1000; Batch 7/7: [===========] - loss: 13623.447169527339\n",
      " Epoch 38/1000; Batch 7/7: [===========] - loss: 13072.052055879543\n",
      " Epoch 39/1000; Batch 7/7: [===========] - loss: 14588.634725735858\n",
      " Epoch 40/1000; Batch 7/7: [===========] - loss: 12370.375181935782\n",
      " Epoch 41/1000; Batch 7/7: [===========] - loss: 14349.187068374156\n",
      " Epoch 42/1000; Batch 7/7: [===========] - loss: 13627.175636317146\n",
      " Epoch 43/1000; Batch 7/7: [===========] - loss: 13615.422168876177\n",
      " Epoch 44/1000; Batch 7/7: [===========] - loss: 12445.655626706854\n",
      " Epoch 45/1000; Batch 7/7: [===========] - loss: 12067.961758092866\n",
      " Epoch 46/1000; Batch 7/7: [===========] - loss: 13297.854150197807\n",
      " Epoch 47/1000; Batch 7/7: [===========] - loss: 12569.682082898193\n",
      " Epoch 48/1000; Batch 7/7: [===========] - loss: 13953.946323296641\n",
      " Epoch 49/1000; Batch 7/7: [===========] - loss: 14962.997270964055\n",
      " Epoch 50/1000; Batch 7/7: [===========] - loss: 15904.870939346489\n",
      " Epoch 51/1000; Batch 7/7: [===========] - loss: 13010.125805702464\n",
      " Epoch 52/1000; Batch 7/7: [===========] - loss: 15439.156024737182\n",
      " Epoch 53/1000; Batch 7/7: [===========] - loss: 10954.345976978506\n",
      " Epoch 54/1000; Batch 7/7: [===========] - loss: 11986.400849013384\n",
      " Epoch 55/1000; Batch 7/7: [===========] - loss: 15650.411265827559\n",
      " Epoch 56/1000; Batch 7/7: [===========] - loss: 10147.146767024413\n",
      " Epoch 57/1000; Batch 7/7: [===========] - loss: 14457.202359328292\n",
      " Epoch 58/1000; Batch 7/7: [===========] - loss: 12898.160465118686\n",
      " Epoch 59/1000; Batch 7/7: [===========] - loss: 14132.461979943022\n",
      " Epoch 60/1000; Batch 7/7: [===========] - loss: 13493.713063732763\n",
      " Epoch 61/1000; Batch 7/7: [===========] - loss: 13284.972828844092\n",
      " Epoch 62/1000; Batch 7/7: [===========] - loss: 15556.841102103295\n",
      " Epoch 63/1000; Batch 7/7: [===========] - loss: 14746.587457790734\n",
      " Epoch 64/1000; Batch 7/7: [===========] - loss: 12323.791781346174\n",
      " Epoch 65/1000; Batch 7/7: [===========] - loss: 12217.974870060118\n",
      " Epoch 66/1000; Batch 7/7: [===========] - loss: 14428.294621381949\n",
      " Epoch 67/1000; Batch 7/7: [===========] - loss: 11426.920667176425\n",
      " Epoch 68/1000; Batch 7/7: [===========] - loss: 12950.939532769351\n",
      " Epoch 69/1000; Batch 7/7: [===========] - loss: 12950.519778213635\n",
      " Epoch 70/1000; Batch 7/7: [===========] - loss: 14169.882280501346\n",
      " Epoch 71/1000; Batch 7/7: [===========] - loss: 10036.289393868378\n",
      " Epoch 72/1000; Batch 7/7: [===========] - loss: 14998.886727901248\n",
      " Epoch 73/1000; Batch 7/7: [===========] - loss: 11627.876148554138\n",
      " Epoch 74/1000; Batch 7/7: [===========] - loss: 16826.141434459736\n",
      " Epoch 75/1000; Batch 7/7: [===========] - loss: 14906.467752236322\n",
      " Epoch 76/1000; Batch 7/7: [===========] - loss: 12610.246532062552\n",
      " Epoch 77/1000; Batch 7/7: [===========] - loss: 13546.927001862774\n",
      " Epoch 78/1000; Batch 7/7: [===========] - loss: 12371.402954218654\n",
      " Epoch 79/1000; Batch 7/7: [===========] - loss: 13413.209601407345\n",
      " Epoch 80/1000; Batch 7/7: [===========] - loss: 13634.419997538917\n",
      " Epoch 81/1000; Batch 7/7: [===========] - loss: 11219.012478998906\n",
      " Epoch 82/1000; Batch 7/7: [===========] - loss: 12268.952698539117\n",
      " Epoch 83/1000; Batch 7/7: [===========] - loss: 14447.415679998086\n",
      " Epoch 84/1000; Batch 7/7: [===========] - loss: 14389.978884837492\n",
      " Epoch 85/1000; Batch 7/7: [===========] - loss: 14146.544264566362\n",
      " Epoch 86/1000; Batch 7/7: [===========] - loss: 12138.721955615678\n",
      " Epoch 87/1000; Batch 7/7: [===========] - loss: 10830.449017034215\n",
      " Epoch 88/1000; Batch 7/7: [===========] - loss: 14773.190184098197\n",
      " Epoch 89/1000; Batch 7/7: [===========] - loss: 13231.875157025115\n",
      " Epoch 90/1000; Batch 7/7: [===========] - loss: 11012.665267860299\n",
      " Epoch 91/1000; Batch 7/7: [===========] - loss: 11200.315532943505\n",
      " Epoch 92/1000; Batch 7/7: [===========] - loss: 12726.512932732085\n",
      " Epoch 93/1000; Batch 7/7: [===========] - loss: 12438.013336617627\n",
      " Epoch 94/1000; Batch 7/7: [===========] - loss: 14180.359572577195\n",
      " Epoch 95/1000; Batch 7/7: [===========] - loss: 15127.216586264864\n",
      " Epoch 96/1000; Batch 7/7: [===========] - loss: 12286.767789449303\n",
      " Epoch 97/1000; Batch 7/7: [===========] - loss: 8855.1688950064314\n",
      " Epoch 98/1000; Batch 7/7: [===========] - loss: 9756.6855023774798\n",
      " Epoch 99/1000; Batch 7/7: [===========] - loss: 11582.781062848204\n",
      " Epoch 100/1000; Batch 7/7: [===========] - loss: 10653.600772991986\n",
      " Epoch 101/1000; Batch 7/7: [===========] - loss: 11795.763135605714\n",
      " Epoch 102/1000; Batch 7/7: [===========] - loss: 11172.171932015722\n",
      " Epoch 103/1000; Batch 7/7: [===========] - loss: 11410.738713034774\n",
      " Epoch 104/1000; Batch 7/7: [===========] - loss: 9508.6882878645315\n",
      " Epoch 105/1000; Batch 7/7: [===========] - loss: 9547.4029667271523\n",
      " Epoch 106/1000; Batch 7/7: [===========] - loss: 9754.7989603784346\n",
      " Epoch 107/1000; Batch 7/7: [===========] - loss: 9580.1906342217283\n",
      " Epoch 108/1000; Batch 7/7: [===========] - loss: 11339.698939995938\n",
      " Epoch 109/1000; Batch 7/7: [===========] - loss: 11482.234173594763\n",
      " Epoch 110/1000; Batch 7/7: [===========] - loss: 11085.942093832353\n",
      " Epoch 111/1000; Batch 7/7: [===========] - loss: 9187.0366540535527\n",
      " Epoch 112/1000; Batch 7/7: [===========] - loss: 10221.138791784128\n",
      " Epoch 113/1000; Batch 7/7: [===========] - loss: 8927.4297080445877\n",
      " Epoch 114/1000; Batch 7/7: [===========] - loss: 10562.468103192596\n",
      " Epoch 115/1000; Batch 7/7: [===========] - loss: 8373.1487297913232\n",
      " Epoch 116/1000; Batch 7/7: [===========] - loss: 8118.0510648795377\n",
      " Epoch 117/1000; Batch 7/7: [===========] - loss: 8629.8027085953094\n",
      " Epoch 118/1000; Batch 7/7: [===========] - loss: 9020.2986224993639\n",
      " Epoch 119/1000; Batch 7/7: [===========] - loss: 9297.7229377321185\n",
      " Epoch 120/1000; Batch 7/7: [===========] - loss: 5665.8080264430855\n",
      " Epoch 121/1000; Batch 7/7: [===========] - loss: 8286.2907915441085\n",
      " Epoch 122/1000; Batch 7/7: [===========] - loss: 5983.519886815187\n",
      " Epoch 123/1000; Batch 7/7: [===========] - loss: 6716.9203588547325\n",
      " Epoch 124/1000; Batch 7/7: [===========] - loss: 6454.387783963064\n",
      " Epoch 125/1000; Batch 7/7: [===========] - loss: 4744.194009517594\n",
      " Epoch 126/1000; Batch 7/7: [===========] - loss: 4842.7558245080245\n",
      " Epoch 127/1000; Batch 7/7: [===========] - loss: 5027.3277908029955\n",
      " Epoch 128/1000; Batch 7/7: [===========] - loss: 4234.358551244653\n",
      " Epoch 129/1000; Batch 7/7: [===========] - loss: 3708.1718268069267\n",
      " Epoch 130/1000; Batch 7/7: [===========] - loss: 3593.5121392438004\n",
      " Epoch 131/1000; Batch 7/7: [===========] - loss: 3064.7658828225534\n",
      " Epoch 132/1000; Batch 7/7: [===========] - loss: 2874.7342065380853\n",
      " Epoch 133/1000; Batch 7/7: [===========] - loss: 3021.9092932428053\n",
      " Epoch 134/1000; Batch 7/7: [===========] - loss: 2955.5861266075094\n",
      " Epoch 135/1000; Batch 7/7: [===========] - loss: 2761.5328561052077\n",
      " Epoch 136/1000; Batch 7/7: [===========] - loss: 2990.4100987049706\n",
      " Epoch 137/1000; Batch 7/7: [===========] - loss: 2471.2608092992527\n",
      " Epoch 138/1000; Batch 7/7: [===========] - loss: 3316.8671484635624\n",
      " Epoch 139/1000; Batch 7/7: [===========] - loss: 3080.5252893422164\n",
      " Epoch 140/1000; Batch 7/7: [===========] - loss: 3285.1748088785734\n",
      " Epoch 141/1000; Batch 7/7: [===========] - loss: 3687.9930020760095\n",
      " Epoch 142/1000; Batch 7/7: [===========] - loss: 3198.6128843129376\n",
      " Epoch 143/1000; Batch 7/7: [===========] - loss: 3107.3255829439567\n",
      " Epoch 144/1000; Batch 7/7: [===========] - loss: 3380.0723681163104\n",
      " Epoch 145/1000; Batch 7/7: [===========] - loss: 2830.5880294938847\n",
      " Epoch 146/1000; Batch 7/7: [===========] - loss: 2784.8426186817046\n",
      " Epoch 147/1000; Batch 7/7: [===========] - loss: 3181.5600019837873\n",
      " Epoch 148/1000; Batch 7/7: [===========] - loss: 2915.2096928572428\n",
      " Epoch 149/1000; Batch 7/7: [===========] - loss: 3303.1535522285382\n",
      " Epoch 150/1000; Batch 7/7: [===========] - loss: 2404.3052761574017\n",
      " Epoch 151/1000; Batch 7/7: [===========] - loss: 3534.4734052395993\n",
      " Epoch 152/1000; Batch 7/7: [===========] - loss: 3076.5101776135336\n",
      " Epoch 153/1000; Batch 7/7: [===========] - loss: 2952.5017775811744\n",
      " Epoch 154/1000; Batch 7/7: [===========] - loss: 3399.8643932872797\n",
      " Epoch 155/1000; Batch 7/7: [===========] - loss: 2866.8078450281673\n",
      " Epoch 156/1000; Batch 7/7: [===========] - loss: 2619.8034494067737\n",
      " Epoch 157/1000; Batch 7/7: [===========] - loss: 3784.3266418414305\n",
      " Epoch 158/1000; Batch 7/7: [===========] - loss: 2744.4160698624143\n",
      " Epoch 159/1000; Batch 7/7: [===========] - loss: 3010.8274841012776\n",
      " Epoch 160/1000; Batch 7/7: [===========] - loss: 3028.1484376359493\n",
      " Epoch 161/1000; Batch 7/7: [===========] - loss: 2681.4808171324923\n",
      " Epoch 162/1000; Batch 7/7: [===========] - loss: 2231.4649300575024\n",
      " Epoch 163/1000; Batch 7/7: [===========] - loss: 3173.0614687997486\n",
      " Epoch 164/1000; Batch 7/7: [===========] - loss: 2306.7632493001694\n",
      " Epoch 165/1000; Batch 7/7: [===========] - loss: 2823.7486354512935\n",
      " Epoch 166/1000; Batch 7/7: [===========] - loss: 3084.2916373985836\n",
      " Epoch 167/1000; Batch 7/7: [===========] - loss: 2968.8748967088677\n",
      " Epoch 168/1000; Batch 7/7: [===========] - loss: 2560.7799269888174\n",
      " Epoch 169/1000; Batch 7/7: [===========] - loss: 3091.2500706227827\n",
      " Epoch 170/1000; Batch 7/7: [===========] - loss: 2653.3031943881147\n",
      " Epoch 171/1000; Batch 7/7: [===========] - loss: 1837.7236674638796\n",
      " Epoch 172/1000; Batch 7/7: [===========] - loss: 3269.9757690116334\n",
      " Epoch 173/1000; Batch 7/7: [===========] - loss: 3096.0205079056755\n",
      " Epoch 174/1000; Batch 7/7: [===========] - loss: 3256.1900175609962\n",
      " Epoch 175/1000; Batch 7/7: [===========] - loss: 2625.2629870688756\n",
      " Epoch 176/1000; Batch 7/7: [===========] - loss: 2786.8743539135772\n",
      " Epoch 177/1000; Batch 7/7: [===========] - loss: 3238.7236137521027\n",
      " Epoch 178/1000; Batch 7/7: [===========] - loss: 2561.8453896026787\n",
      " Epoch 179/1000; Batch 7/7: [===========] - loss: 3052.3923528515425\n",
      " Epoch 180/1000; Batch 7/7: [===========] - loss: 2978.3162256584715\n",
      " Epoch 181/1000; Batch 7/7: [===========] - loss: 2999.5652908753623\n",
      " Epoch 182/1000; Batch 7/7: [===========] - loss: 3044.6005504108844\n",
      " Epoch 183/1000; Batch 7/7: [===========] - loss: 2762.5820077668755\n",
      " Epoch 184/1000; Batch 7/7: [===========] - loss: 2324.0099481094653\n",
      " Epoch 185/1000; Batch 7/7: [===========] - loss: 2764.7444156892226\n",
      " Epoch 186/1000; Batch 7/7: [===========] - loss: 2472.8902173898126\n",
      " Epoch 187/1000; Batch 7/7: [===========] - loss: 3204.6299481242035\n",
      " Epoch 188/1000; Batch 7/7: [===========] - loss: 2919.8114993795725\n",
      " Epoch 189/1000; Batch 7/7: [===========] - loss: 2949.2634338012637\n",
      " Epoch 190/1000; Batch 7/7: [===========] - loss: 2809.0897272371503\n",
      " Epoch 191/1000; Batch 7/7: [===========] - loss: 2972.5667791374583\n",
      " Epoch 192/1000; Batch 7/7: [===========] - loss: 2215.1676424081097\n",
      " Epoch 193/1000; Batch 7/7: [===========] - loss: 2495.9383706213032\n",
      " Epoch 194/1000; Batch 7/7: [===========] - loss: 3011.1170578571946\n",
      " Epoch 195/1000; Batch 7/7: [===========] - loss: 2391.2815341894957\n",
      " Epoch 196/1000; Batch 7/7: [===========] - loss: 2641.9978398057788\n",
      " Epoch 197/1000; Batch 7/7: [===========] - loss: 2875.9644533499763\n",
      " Epoch 198/1000; Batch 7/7: [===========] - loss: 2562.7082204286422\n",
      " Epoch 199/1000; Batch 7/7: [===========] - loss: 3321.8919378322397\n",
      " Epoch 200/1000; Batch 7/7: [===========] - loss: 2533.6449650445624\n",
      " Epoch 201/1000; Batch 7/7: [===========] - loss: 2423.0765426786417\n",
      " Epoch 202/1000; Batch 7/7: [===========] - loss: 2873.8934498079525\n",
      " Epoch 203/1000; Batch 7/7: [===========] - loss: 2533.6162227325412\n",
      " Epoch 204/1000; Batch 7/7: [===========] - loss: 2340.2887260906673\n",
      " Epoch 205/1000; Batch 7/7: [===========] - loss: 2255.2039868105644\n",
      " Epoch 206/1000; Batch 7/7: [===========] - loss: 2467.8593879813993\n",
      " Epoch 207/1000; Batch 7/7: [===========] - loss: 2369.1056777593058\n",
      " Epoch 208/1000; Batch 7/7: [===========] - loss: 2181.8576095376584\n",
      " Epoch 209/1000; Batch 7/7: [===========] - loss: 2787.8788428335548\n",
      " Epoch 210/1000; Batch 7/7: [===========] - loss: 2752.3570617413254\n",
      " Epoch 211/1000; Batch 7/7: [===========] - loss: 2310.5287177510686\n",
      " Epoch 212/1000; Batch 7/7: [===========] - loss: 2165.3347565601937\n",
      " Epoch 213/1000; Batch 7/7: [===========] - loss: 2259.9073487491023\n",
      " Epoch 214/1000; Batch 7/7: [===========] - loss: 1732.1260786425733\n",
      " Epoch 215/1000; Batch 7/7: [===========] - loss: 2818.5237896874864\n",
      " Epoch 216/1000; Batch 7/7: [===========] - loss: 2500.1741520252526\n",
      " Epoch 217/1000; Batch 7/7: [===========] - loss: 2356.9143247069333\n",
      " Epoch 218/1000; Batch 7/7: [===========] - loss: 2289.1730197574613\n",
      " Epoch 219/1000; Batch 7/7: [===========] - loss: 1773.2636026633345\n",
      " Epoch 220/1000; Batch 7/7: [===========] - loss: 1727.8873243526293\n",
      " Epoch 221/1000; Batch 7/7: [===========] - loss: 2132.8205786246097\n",
      " Epoch 222/1000; Batch 7/7: [===========] - loss: 1732.6394748658014\n",
      " Epoch 223/1000; Batch 7/7: [===========] - loss: 1691.2378499870545\n",
      " Epoch 224/1000; Batch 7/7: [===========] - loss: 2261.3103931133815\n",
      " Epoch 225/1000; Batch 7/7: [===========] - loss: 2008.5606739301036\n",
      " Epoch 226/1000; Batch 7/7: [===========] - loss: 2141.7203815049554\n",
      " Epoch 227/1000; Batch 7/7: [===========] - loss: 1953.5215323230294\n",
      " Epoch 228/1000; Batch 7/7: [===========] - loss: 2391.1885087699383\n",
      " Epoch 229/1000; Batch 7/7: [===========] - loss: 1818.6810261272906\n",
      " Epoch 230/1000; Batch 7/7: [===========] - loss: 1774.2857814697097\n",
      " Epoch 231/1000; Batch 7/7: [===========] - loss: 1979.3056256367622\n",
      " Epoch 232/1000; Batch 7/7: [===========] - loss: 1424.8252987909772\n",
      " Epoch 233/1000; Batch 7/7: [===========] - loss: 1614.2434241745924\n",
      " Epoch 234/1000; Batch 7/7: [===========] - loss: 1661.1389982805747\n",
      " Epoch 235/1000; Batch 7/7: [===========] - loss: 2487.2597671623275\n",
      " Epoch 236/1000; Batch 7/7: [===========] - loss: 1882.5602914312126\n",
      " Epoch 237/1000; Batch 7/7: [===========] - loss: 1864.8795646928873\n",
      " Epoch 238/1000; Batch 7/7: [===========] - loss: 2069.2807911756955\n",
      " Epoch 239/1000; Batch 7/7: [===========] - loss: 1646.0973143012886\n",
      " Epoch 240/1000; Batch 7/7: [===========] - loss: 1927.7069024524476\n",
      " Epoch 241/1000; Batch 7/7: [===========] - loss: 1789.3835700534676\n",
      " Epoch 242/1000; Batch 7/7: [===========] - loss: 1188.0509023307914\n",
      " Epoch 243/1000; Batch 7/7: [===========] - loss: 1904.3654504196902\n",
      " Epoch 244/1000; Batch 7/7: [===========] - loss: 1896.1200620866603\n",
      " Epoch 245/1000; Batch 7/7: [===========] - loss: 1614.3188867917067\n",
      " Epoch 246/1000; Batch 7/7: [===========] - loss: 1594.3536722186442\n",
      " Epoch 247/1000; Batch 7/7: [===========] - loss: 1746.2950643584968\n",
      " Epoch 248/1000; Batch 7/7: [===========] - loss: 1723.3254703655255\n",
      " Epoch 249/1000; Batch 7/7: [===========] - loss: 1389.2845995570717\n",
      " Epoch 250/1000; Batch 7/7: [===========] - loss: 2076.6375712562215\n",
      " Epoch 251/1000; Batch 7/7: [===========] - loss: 1923.2957238539054\n",
      " Epoch 252/1000; Batch 7/7: [===========] - loss: 1613.2656115251389\n",
      " Epoch 253/1000; Batch 7/7: [===========] - loss: 2314.3466749735476\n",
      " Epoch 254/1000; Batch 7/7: [===========] - loss: 2021.4713278796744\n",
      " Epoch 255/1000; Batch 7/7: [===========] - loss: 1702.5108300850768\n",
      " Epoch 256/1000; Batch 7/7: [===========] - loss: 2158.9378669811814\n",
      " Epoch 257/1000; Batch 7/7: [===========] - loss: 1556.7008292711669\n",
      " Epoch 258/1000; Batch 7/7: [===========] - loss: 1793.6140912775566\n",
      " Epoch 259/1000; Batch 7/7: [===========] - loss: 1805.9391749913445\n",
      " Epoch 260/1000; Batch 7/7: [===========] - loss: 1587.1151917552545\n",
      " Epoch 261/1000; Batch 7/7: [===========] - loss: 1892.4776560490227\n",
      " Epoch 262/1000; Batch 7/7: [===========] - loss: 1930.6816936671434\n",
      " Epoch 263/1000; Batch 7/7: [===========] - loss: 2053.0421061562673\n",
      " Epoch 264/1000; Batch 7/7: [===========] - loss: 1728.1523339700783\n",
      " Epoch 265/1000; Batch 7/7: [===========] - loss: 1494.8664507531859\n",
      " Epoch 266/1000; Batch 7/7: [===========] - loss: 1981.3026070301887\n",
      " Epoch 267/1000; Batch 7/7: [===========] - loss: 1537.2415226872665\n",
      " Epoch 268/1000; Batch 7/7: [===========] - loss: 1752.2294901563794\n",
      " Epoch 269/1000; Batch 7/7: [===========] - loss: 1903.6194991610157\n",
      " Epoch 270/1000; Batch 7/7: [===========] - loss: 1726.4917437459887\n",
      " Epoch 271/1000; Batch 7/7: [===========] - loss: 1786.8973215435099\n",
      " Epoch 272/1000; Batch 7/7: [===========] - loss: 1528.0567562546114\n",
      " Epoch 273/1000; Batch 7/7: [===========] - loss: 1519.3164903535146\n",
      " Epoch 274/1000; Batch 7/7: [===========] - loss: 1342.9660324135592\n",
      " Epoch 275/1000; Batch 7/7: [===========] - loss: 1630.0706415035449\n",
      " Epoch 276/1000; Batch 7/7: [===========] - loss: 1843.4026759681092\n",
      " Epoch 277/1000; Batch 7/7: [===========] - loss: 1710.6294941782792\n",
      " Epoch 278/1000; Batch 7/7: [===========] - loss: 1992.2724518319044\n",
      " Epoch 279/1000; Batch 7/7: [===========] - loss: 1758.0921510593864\n",
      " Epoch 280/1000; Batch 7/7: [===========] - loss: 1617.6378937711543\n",
      " Epoch 281/1000; Batch 7/7: [===========] - loss: 1448.5976527738498\n",
      " Epoch 282/1000; Batch 7/7: [===========] - loss: 1753.2954707499246\n",
      " Epoch 283/1000; Batch 7/7: [===========] - loss: 2017.0229957852687\n",
      " Epoch 284/1000; Batch 7/7: [===========] - loss: 1465.8296858155807\n",
      " Epoch 285/1000; Batch 7/7: [===========] - loss: 1201.4902124710942\n",
      " Epoch 286/1000; Batch 7/7: [===========] - loss: 1497.2259597027148\n",
      " Epoch 287/1000; Batch 7/7: [===========] - loss: 1762.9263868634136\n",
      " Epoch 288/1000; Batch 7/7: [===========] - loss: 1408.9285528409453\n",
      " Epoch 289/1000; Batch 7/7: [===========] - loss: 1367.5997677003693\n",
      " Epoch 290/1000; Batch 7/7: [===========] - loss: 1399.8347542357633\n",
      " Epoch 291/1000; Batch 7/7: [===========] - loss: 1903.0919806382767\n",
      " Epoch 292/1000; Batch 7/7: [===========] - loss: 1788.4947694448297\n",
      " Epoch 293/1000; Batch 7/7: [===========] - loss: 1424.4406492440748\n",
      " Epoch 294/1000; Batch 7/7: [===========] - loss: 1393.3671789623388\n",
      " Epoch 295/1000; Batch 7/7: [===========] - loss: 1386.5933548739454\n",
      " Epoch 296/1000; Batch 7/7: [===========] - loss: 1841.1191755651996\n",
      " Epoch 297/1000; Batch 7/7: [===========] - loss: 1861.0998289238576\n",
      " Epoch 298/1000; Batch 7/7: [===========] - loss: 1539.5225568332569\n",
      " Epoch 299/1000; Batch 7/7: [===========] - loss: 1519.9663904340373\n",
      " Epoch 300/1000; Batch 7/7: [===========] - loss: 1932.0650148806642\n",
      " Epoch 301/1000; Batch 7/7: [===========] - loss: 1470.9882535040651\n",
      " Epoch 302/1000; Batch 7/7: [===========] - loss: 1492.9057561339944\n",
      " Epoch 303/1000; Batch 7/7: [===========] - loss: 1415.0579085864113\n",
      " Epoch 304/1000; Batch 7/7: [===========] - loss: 1598.5782553327376\n",
      " Epoch 305/1000; Batch 7/7: [===========] - loss: 1664.4844917572866\n",
      " Epoch 306/1000; Batch 7/7: [===========] - loss: 1492.5627326042713\n",
      " Epoch 307/1000; Batch 7/7: [===========] - loss: 1775.5529328362493\n",
      " Epoch 308/1000; Batch 7/7: [===========] - loss: 1800.1818468610857\n",
      " Epoch 309/1000; Batch 7/7: [===========] - loss: 1618.1722601515148\n",
      " Epoch 310/1000; Batch 7/7: [===========] - loss: 1072.0345775752453\n",
      " Epoch 311/1000; Batch 7/7: [===========] - loss: 1747.2852193859985\n",
      " Epoch 312/1000; Batch 7/7: [===========] - loss: 1457.5786943360865\n",
      " Epoch 313/1000; Batch 7/7: [===========] - loss: 1483.0895910727695\n",
      " Epoch 314/1000; Batch 7/7: [===========] - loss: 1690.9484948707514\n",
      " Epoch 315/1000; Batch 7/7: [===========] - loss: 1534.8647312892572\n",
      " Epoch 316/1000; Batch 7/7: [===========] - loss: 1325.0358528061952\n",
      " Epoch 317/1000; Batch 7/7: [===========] - loss: 1631.3532644819386\n",
      " Epoch 318/1000; Batch 7/7: [===========] - loss: 1214.8769655826725\n",
      " Epoch 319/1000; Batch 7/7: [===========] - loss: 1524.4499311294383\n",
      " Epoch 320/1000; Batch 7/7: [===========] - loss: 1576.3787465086903\n",
      " Epoch 321/1000; Batch 7/7: [===========] - loss: 1293.0150139768325\n",
      " Epoch 322/1000; Batch 7/7: [===========] - loss: 1594.2363304249918\n",
      " Epoch 323/1000; Batch 7/7: [===========] - loss: 1554.4489756143862\n",
      " Epoch 324/1000; Batch 7/7: [===========] - loss: 1576.4906540155417\n",
      " Epoch 325/1000; Batch 7/7: [===========] - loss: 1623.2302228710362\n",
      " Epoch 326/1000; Batch 7/7: [===========] - loss: 2025.8635357612212\n",
      " Epoch 327/1000; Batch 7/7: [===========] - loss: 1557.8588663081487\n",
      " Epoch 328/1000; Batch 7/7: [===========] - loss: 1450.6787747558845\n",
      " Epoch 329/1000; Batch 7/7: [===========] - loss: 2365.4077712517737\n",
      " Epoch 330/1000; Batch 7/7: [===========] - loss: 1596.7173852479848\n",
      " Epoch 331/1000; Batch 7/7: [===========] - loss: 1845.0017087587983\n",
      " Epoch 332/1000; Batch 7/7: [===========] - loss: 1240.2442379394315\n",
      " Epoch 333/1000; Batch 7/7: [===========] - loss: 1023.3924371775042\n",
      " Epoch 334/1000; Batch 7/7: [===========] - loss: 1169.9545034110465\n",
      " Epoch 335/1000; Batch 7/7: [===========] - loss: 1395.2337134062398\n",
      " Epoch 336/1000; Batch 7/7: [===========] - loss: 1862.2585948650071\n",
      " Epoch 337/1000; Batch 7/7: [===========] - loss: 1699.6807610848582\n",
      " Epoch 338/1000; Batch 7/7: [===========] - loss: 1268.2122187820553\n",
      " Epoch 339/1000; Batch 7/7: [===========] - loss: 1411.4959188675693\n",
      " Epoch 340/1000; Batch 7/7: [===========] - loss: 1517.7997606255617\n",
      " Epoch 341/1000; Batch 7/7: [===========] - loss: 1333.9999430928178\n",
      " Epoch 342/1000; Batch 7/7: [===========] - loss: 1438.9585602853708\n",
      " Epoch 343/1000; Batch 7/7: [===========] - loss: 1700.3182612198395\n",
      " Epoch 344/1000; Batch 7/7: [===========] - loss: 2423.5291834233685\n",
      " Epoch 345/1000; Batch 7/7: [===========] - loss: 1751.1832098663298\n",
      " Epoch 346/1000; Batch 7/7: [===========] - loss: 1885.0963231993505\n",
      " Epoch 347/1000; Batch 7/7: [===========] - loss: 1683.8501190511492\n",
      " Epoch 348/1000; Batch 7/7: [===========] - loss: 1249.2112443330926\n",
      " Epoch 349/1000; Batch 7/7: [===========] - loss: 1508.8311638746789\n",
      " Epoch 350/1000; Batch 7/7: [===========] - loss: 1882.5587022834636\n",
      " Epoch 351/1000; Batch 7/7: [===========] - loss: 1811.1247890358544\n",
      " Epoch 352/1000; Batch 7/7: [===========] - loss: 1482.5851091008535\n",
      " Epoch 353/1000; Batch 7/7: [===========] - loss: 1279.8793511743092\n",
      " Epoch 354/1000; Batch 7/7: [===========] - loss: 1500.7853892512593\n",
      " Epoch 355/1000; Batch 7/7: [===========] - loss: 1431.9272289787314\n",
      " Epoch 356/1000; Batch 7/7: [===========] - loss: 1283.8670978678333\n",
      " Epoch 357/1000; Batch 7/7: [===========] - loss: 1707.4800704603072\n",
      " Epoch 358/1000; Batch 7/7: [===========] - loss: 1659.5922640404343\n",
      " Epoch 359/1000; Batch 7/7: [===========] - loss: 1488.9812142436401\n",
      " Epoch 360/1000; Batch 7/7: [===========] - loss: 1779.2884492728888\n",
      " Epoch 361/1000; Batch 7/7: [===========] - loss: 1922.7981270970859\n",
      " Epoch 362/1000; Batch 7/7: [===========] - loss: 1346.4055442818133\n",
      " Epoch 363/1000; Batch 7/7: [===========] - loss: 1946.6866357852793\n",
      " Epoch 364/1000; Batch 7/7: [===========] - loss: 1772.9805989722673\n",
      " Epoch 365/1000; Batch 7/7: [===========] - loss: 1584.8995996724202\n",
      " Epoch 366/1000; Batch 7/7: [===========] - loss: 1414.1893560011158\n",
      " Epoch 367/1000; Batch 7/7: [===========] - loss: 1224.9467463904289\n",
      " Epoch 368/1000; Batch 7/7: [===========] - loss: 1484.1579868567258\n",
      " Epoch 369/1000; Batch 7/7: [===========] - loss: 1686.8923822441106\n",
      " Epoch 370/1000; Batch 7/7: [===========] - loss: 1201.8438679605175\n",
      " Epoch 371/1000; Batch 7/7: [===========] - loss: 1606.0076315914605\n",
      " Epoch 372/1000; Batch 7/7: [===========] - loss: 1193.9411816904308\n",
      " Epoch 373/1000; Batch 7/7: [===========] - loss: 1965.1029465744261\n",
      " Epoch 374/1000; Batch 7/7: [===========] - loss: 1404.4911424358952\n",
      " Epoch 375/1000; Batch 7/7: [===========] - loss: 1751.5559602097305\n",
      " Epoch 376/1000; Batch 7/7: [===========] - loss: 1213.7933257760474\n",
      " Epoch 377/1000; Batch 7/7: [===========] - loss: 1785.1560167980656\n",
      " Epoch 378/1000; Batch 7/7: [===========] - loss: 1569.2196676677386\n",
      " Epoch 379/1000; Batch 7/7: [===========] - loss: 1218.3141892530039\n",
      " Epoch 380/1000; Batch 7/7: [===========] - loss: 1578.9317520528425\n",
      " Epoch 381/1000; Batch 7/7: [===========] - loss: 1197.5729828217218\n",
      " Epoch 382/1000; Batch 7/7: [===========] - loss: 1776.8872085053257\n",
      " Epoch 383/1000; Batch 7/7: [===========] - loss: 1376.8775380957557\n",
      " Epoch 384/1000; Batch 7/7: [===========] - loss: 1917.9472190737356\n",
      " Epoch 385/1000; Batch 7/7: [===========] - loss: 1419.7412583169603\n",
      " Epoch 386/1000; Batch 7/7: [===========] - loss: 1424.3070134911948\n",
      " Epoch 387/1000; Batch 7/7: [===========] - loss: 1084.4894975109832\n",
      " Epoch 388/1000; Batch 7/7: [===========] - loss: 1215.9686316011923\n",
      " Epoch 389/1000; Batch 7/7: [===========] - loss: 1784.0721296973827\n",
      " Epoch 390/1000; Batch 7/7: [===========] - loss: 1365.3685384753267\n",
      " Epoch 391/1000; Batch 7/7: [===========] - loss: 1165.3699561849858\n",
      " Epoch 392/1000; Batch 7/7: [===========] - loss: 1354.7506300723978\n",
      " Epoch 393/1000; Batch 7/7: [===========] - loss: 2497.8431821464624\n",
      " Epoch 394/1000; Batch 7/7: [===========] - loss: 1728.6516137533172\n",
      " Epoch 395/1000; Batch 7/7: [===========] - loss: 1200.4311316699625\n",
      " Epoch 396/1000; Batch 7/7: [===========] - loss: 1569.1013273201486\n",
      " Epoch 397/1000; Batch 7/7: [===========] - loss: 1348.6395888911443\n",
      " Epoch 398/1000; Batch 7/7: [===========] - loss: 1575.0216105190352\n",
      " Epoch 399/1000; Batch 7/7: [===========] - loss: 1410.3311148042466\n",
      " Epoch 400/1000; Batch 7/7: [===========] - loss: 1800.4207539324327\n",
      " Epoch 401/1000; Batch 7/7: [===========] - loss: 1334.9640754135332\n",
      " Epoch 402/1000; Batch 7/7: [===========] - loss: 1436.9808674026333\n",
      " Epoch 403/1000; Batch 7/7: [===========] - loss: 1551.0057602505608\n",
      " Epoch 404/1000; Batch 7/7: [===========] - loss: 1299.8212416553401\n",
      " Epoch 405/1000; Batch 7/7: [===========] - loss: 1054.0192564213362\n",
      " Epoch 406/1000; Batch 7/7: [===========] - loss: 1456.8536025759756\n",
      " Epoch 407/1000; Batch 7/7: [===========] - loss: 1742.1482662246044\n",
      " Epoch 408/1000; Batch 7/7: [===========] - loss: 1324.6186716766135\n",
      " Epoch 409/1000; Batch 7/7: [===========] - loss: 1728.9609603143974\n",
      " Epoch 410/1000; Batch 7/7: [===========] - loss: 1693.9804173463046\n",
      " Epoch 411/1000; Batch 7/7: [===========] - loss: 2686.2812393608783\n",
      " Epoch 412/1000; Batch 7/7: [===========] - loss: 2554.1063850761125\n",
      " Epoch 413/1000; Batch 7/7: [===========] - loss: 1565.4345452866223\n",
      " Epoch 414/1000; Batch 7/7: [===========] - loss: 1708.9413696729023\n",
      " Epoch 415/1000; Batch 7/7: [===========] - loss: 1730.8510888949356\n",
      " Epoch 416/1000; Batch 7/7: [===========] - loss: 2529.7263518790993\n",
      " Epoch 417/1000; Batch 7/7: [===========] - loss: 1618.4313822026456\n",
      " Epoch 418/1000; Batch 7/7: [===========] - loss: 1298.1266579817382\n",
      " Epoch 419/1000; Batch 7/7: [===========] - loss: 1459.7239132052257\n",
      " Epoch 420/1000; Batch 7/7: [===========] - loss: 2203.5470289927853\n",
      " Epoch 421/1000; Batch 7/7: [===========] - loss: 1648.4438059697177\n",
      " Epoch 422/1000; Batch 7/7: [===========] - loss: 1482.5252543740996\n",
      " Epoch 423/1000; Batch 7/7: [===========] - loss: 1837.0246646124801\n",
      " Epoch 424/1000; Batch 7/7: [===========] - loss: 2269.9316045321022\n",
      " Epoch 425/1000; Batch 7/7: [===========] - loss: 1398.0408622359718\n",
      " Epoch 426/1000; Batch 7/7: [===========] - loss: 1513.0595098672215\n",
      " Epoch 427/1000; Batch 7/7: [===========] - loss: 2117.3223640893485\n",
      " Epoch 428/1000; Batch 7/7: [===========] - loss: 1651.9860135596584\n",
      " Epoch 429/1000; Batch 7/7: [===========] - loss: 1811.6668796363936\n",
      " Epoch 430/1000; Batch 7/7: [===========] - loss: 1617.4241571826844\n",
      " Epoch 431/1000; Batch 7/7: [===========] - loss: 1671.2150944038833\n",
      " Epoch 432/1000; Batch 7/7: [===========] - loss: 1718.7323844326615\n",
      " Epoch 433/1000; Batch 7/7: [===========] - loss: 2072.8842505426796\n",
      " Epoch 434/1000; Batch 7/7: [===========] - loss: 1436.8415112513233\n",
      " Epoch 435/1000; Batch 7/7: [===========] - loss: 1561.6900270708068\n",
      " Epoch 436/1000; Batch 7/7: [===========] - loss: 1531.8202733505782\n",
      " Epoch 437/1000; Batch 7/7: [===========] - loss: 1779.3447954083292\n",
      " Epoch 438/1000; Batch 7/7: [===========] - loss: 1210.1976822617094\n",
      " Epoch 439/1000; Batch 7/7: [===========] - loss: 1447.2465202234888\n",
      " Epoch 440/1000; Batch 7/7: [===========] - loss: 2021.4108309060798\n",
      " Epoch 441/1000; Batch 7/7: [===========] - loss: 1655.3288809981586\n",
      " Epoch 442/1000; Batch 7/7: [===========] - loss: 1120.7497206098424\n",
      " Epoch 443/1000; Batch 7/7: [===========] - loss: 1578.6150985015915\n",
      " Epoch 444/1000; Batch 7/7: [===========] - loss: 1996.2390213124856\n",
      " Epoch 445/1000; Batch 7/7: [===========] - loss: 2152.4221910100683\n",
      " Epoch 446/1000; Batch 7/7: [===========] - loss: 1466.9864833185295\n",
      " Epoch 447/1000; Batch 7/7: [===========] - loss: 1711.2321242096166\n",
      " Epoch 448/1000; Batch 7/7: [===========] - loss: 1799.7879426299255\n",
      " Epoch 449/1000; Batch 7/7: [===========] - loss: 1529.7192207709143\n",
      " Epoch 450/1000; Batch 7/7: [===========] - loss: 1326.8312745301311\n",
      " Epoch 451/1000; Batch 7/7: [===========] - loss: 1715.1572222724444\n",
      " Epoch 452/1000; Batch 7/7: [===========] - loss: 1361.4853048155137\n",
      " Epoch 453/1000; Batch 7/7: [===========] - loss: 1591.6372330863474\n",
      " Epoch 454/1000; Batch 7/7: [===========] - loss: 1538.6113055741619\n",
      " Epoch 455/1000; Batch 7/7: [===========] - loss: 1187.7487125202513\n",
      " Epoch 456/1000; Batch 7/7: [===========] - loss: 898.43198182994118\n",
      " Epoch 457/1000; Batch 7/7: [===========] - loss: 1203.6333853262363\n",
      " Epoch 458/1000; Batch 7/7: [===========] - loss: 1820.5211119063665\n",
      " Epoch 459/1000; Batch 7/7: [===========] - loss: 1600.5061280749692\n",
      " Epoch 460/1000; Batch 7/7: [===========] - loss: 1420.0911110994195\n",
      " Epoch 461/1000; Batch 7/7: [===========] - loss: 1349.8636889735378\n",
      " Epoch 462/1000; Batch 7/7: [===========] - loss: 1733.4621839625959\n",
      " Epoch 463/1000; Batch 7/7: [===========] - loss: 1333.6081709283703\n",
      " Epoch 464/1000; Batch 7/7: [===========] - loss: 1261.9883032813623\n",
      " Epoch 465/1000; Batch 7/7: [===========] - loss: 1600.0851946845014\n",
      " Epoch 466/1000; Batch 7/7: [===========] - loss: 1781.1165944186473\n",
      " Epoch 467/1000; Batch 7/7: [===========] - loss: 1120.5269557481035\n",
      " Epoch 468/1000; Batch 7/7: [===========] - loss: 1129.7687149899025\n",
      " Epoch 469/1000; Batch 7/7: [===========] - loss: 1187.4878188153887\n",
      " Epoch 470/1000; Batch 7/7: [===========] - loss: 1220.3981775200387\n",
      " Epoch 471/1000; Batch 7/7: [===========] - loss: 1280.1410591195236\n",
      " Epoch 472/1000; Batch 7/7: [===========] - loss: 1691.5657700000752\n",
      " Epoch 473/1000; Batch 7/7: [===========] - loss: 1486.4862638287557\n",
      " Epoch 474/1000; Batch 7/7: [===========] - loss: 1383.8749323751495\n",
      " Epoch 475/1000; Batch 7/7: [===========] - loss: 2019.2322737095344\n",
      " Epoch 476/1000; Batch 7/7: [===========] - loss: 1597.6320744050852\n",
      " Epoch 477/1000; Batch 7/7: [===========] - loss: 1609.2999088068893\n",
      " Epoch 478/1000; Batch 7/7: [===========] - loss: 1383.1308374738757\n",
      " Epoch 479/1000; Batch 7/7: [===========] - loss: 1450.1102642233989\n",
      " Epoch 480/1000; Batch 7/7: [===========] - loss: 1215.4277467975453\n",
      " Epoch 481/1000; Batch 7/7: [===========] - loss: 1354.2058175180466\n",
      " Epoch 482/1000; Batch 7/7: [===========] - loss: 1084.9393388073427\n",
      " Epoch 483/1000; Batch 7/7: [===========] - loss: 1847.8393840305773\n",
      " Epoch 484/1000; Batch 7/7: [===========] - loss: 967.12082960543942\n",
      " Epoch 485/1000; Batch 7/7: [===========] - loss: 1610.3381031105012\n",
      " Epoch 486/1000; Batch 7/7: [===========] - loss: 1582.8292575054896\n",
      " Epoch 487/1000; Batch 7/7: [===========] - loss: 1375.4376710490317\n",
      " Epoch 488/1000; Batch 7/7: [===========] - loss: 1225.5249634150769\n",
      " Epoch 489/1000; Batch 7/7: [===========] - loss: 1225.9889022312368\n",
      " Epoch 490/1000; Batch 7/7: [===========] - loss: 1843.6083419692316\n",
      " Epoch 491/1000; Batch 7/7: [===========] - loss: 1391.9969344840329\n",
      " Epoch 492/1000; Batch 7/7: [===========] - loss: 1294.6933325301643\n",
      " Epoch 493/1000; Batch 7/7: [===========] - loss: 1128.0740908891646\n",
      " Epoch 494/1000; Batch 7/7: [===========] - loss: 2177.5793581606526\n",
      " Epoch 495/1000; Batch 7/7: [===========] - loss: 1241.2759662721612\n",
      " Epoch 496/1000; Batch 7/7: [===========] - loss: 1303.8589957705497\n",
      " Epoch 497/1000; Batch 7/7: [===========] - loss: 1482.0895763319102\n",
      " Epoch 498/1000; Batch 7/7: [===========] - loss: 1773.6792833208044\n",
      " Epoch 499/1000; Batch 7/7: [===========] - loss: 1709.8983739793998\n",
      " Epoch 500/1000; Batch 7/7: [===========] - loss: 1723.3000694749796\n",
      " Epoch 501/1000; Batch 7/7: [===========] - loss: 1419.5930000340231\n",
      " Epoch 502/1000; Batch 7/7: [===========] - loss: 1232.3219448703985\n",
      " Epoch 503/1000; Batch 7/7: [===========] - loss: 1174.9125850341278\n",
      " Epoch 504/1000; Batch 7/7: [===========] - loss: 1412.7453405952651\n",
      " Epoch 505/1000; Batch 7/7: [===========] - loss: 1395.6591871775631\n",
      " Epoch 506/1000; Batch 7/7: [===========] - loss: 935.52719830965093\n",
      " Epoch 507/1000; Batch 7/7: [===========] - loss: 1488.8946340975906\n",
      " Epoch 508/1000; Batch 7/7: [===========] - loss: 1543.7817508366381\n",
      " Epoch 509/1000; Batch 7/7: [===========] - loss: 1768.8993247572073\n",
      " Epoch 510/1000; Batch 7/7: [===========] - loss: 1210.7376140089668\n",
      " Epoch 511/1000; Batch 7/7: [===========] - loss: 1265.8420282170148\n",
      " Epoch 512/1000; Batch 7/7: [===========] - loss: 1292.3928675209377\n",
      " Epoch 513/1000; Batch 7/7: [===========] - loss: 1244.9005100790546\n",
      " Epoch 514/1000; Batch 7/7: [===========] - loss: 1549.7831732831712\n",
      " Epoch 515/1000; Batch 7/7: [===========] - loss: 1332.8484668760937\n",
      " Epoch 516/1000; Batch 7/7: [===========] - loss: 1282.9435441694998\n",
      " Epoch 517/1000; Batch 7/7: [===========] - loss: 1308.8129597759253\n",
      " Epoch 518/1000; Batch 7/7: [===========] - loss: 1062.8378374506633\n",
      " Epoch 519/1000; Batch 7/7: [===========] - loss: 1415.2937839707553\n",
      " Epoch 520/1000; Batch 7/7: [===========] - loss: 1224.0910330251395\n",
      " Epoch 521/1000; Batch 7/7: [===========] - loss: 1668.6820331713818\n",
      " Epoch 522/1000; Batch 7/7: [===========] - loss: 1594.3103461831004\n",
      " Epoch 523/1000; Batch 7/7: [===========] - loss: 1520.0923265744478\n",
      " Epoch 524/1000; Batch 7/7: [===========] - loss: 1106.2891623657745\n",
      " Epoch 525/1000; Batch 7/7: [===========] - loss: 2425.3058655947633\n",
      " Epoch 526/1000; Batch 7/7: [===========] - loss: 1526.2660777141145\n",
      " Epoch 527/1000; Batch 7/7: [===========] - loss: 1054.1172870292712\n",
      " Epoch 528/1000; Batch 7/7: [===========] - loss: 1379.3877916300528\n",
      " Epoch 529/1000; Batch 7/7: [===========] - loss: 1297.9944358726696\n",
      " Epoch 530/1000; Batch 7/7: [===========] - loss: 1592.0019094131717\n",
      " Epoch 531/1000; Batch 7/7: [===========] - loss: 1570.0684757873098\n",
      " Epoch 532/1000; Batch 7/7: [===========] - loss: 1072.4392610119373\n",
      " Epoch 533/1000; Batch 7/7: [===========] - loss: 1386.6608861082632\n",
      " Epoch 534/1000; Batch 7/7: [===========] - loss: 1720.0277410931328\n",
      " Epoch 535/1000; Batch 7/7: [===========] - loss: 1193.6768458916931\n",
      " Epoch 536/1000; Batch 7/7: [===========] - loss: 875.61202183970618\n",
      " Epoch 537/1000; Batch 7/7: [===========] - loss: 1157.8275732699728\n",
      " Epoch 538/1000; Batch 7/7: [===========] - loss: 1977.3244298567736\n",
      " Epoch 539/1000; Batch 7/7: [===========] - loss: 1413.1892405233023\n",
      " Epoch 540/1000; Batch 7/7: [===========] - loss: 1667.4990147105386\n",
      " Epoch 541/1000; Batch 7/7: [===========] - loss: 1482.8314059911559\n",
      " Epoch 542/1000; Batch 7/7: [===========] - loss: 1556.4212797537693\n",
      " Epoch 543/1000; Batch 7/7: [===========] - loss: 1495.9324968432157\n",
      " Epoch 544/1000; Batch 7/7: [===========] - loss: 1232.1021857042301\n",
      " Epoch 545/1000; Batch 7/7: [===========] - loss: 1353.8067654567941\n",
      " Epoch 546/1000; Batch 7/7: [===========] - loss: 1909.1033100579875\n",
      " Epoch 547/1000; Batch 7/7: [===========] - loss: 1187.2366669793423\n",
      " Epoch 548/1000; Batch 7/7: [===========] - loss: 1286.0653706438736\n",
      " Epoch 549/1000; Batch 7/7: [===========] - loss: 1214.5001488813548\n",
      " Epoch 550/1000; Batch 7/7: [===========] - loss: 1585.8719891090892\n",
      " Epoch 551/1000; Batch 7/7: [===========] - loss: 1436.5579891734958\n",
      " Epoch 552/1000; Batch 7/7: [===========] - loss: 1781.6829702243535\n",
      " Epoch 553/1000; Batch 7/7: [===========] - loss: 1370.2513403532967\n",
      " Epoch 554/1000; Batch 7/7: [===========] - loss: 1697.7027595518698\n",
      " Epoch 555/1000; Batch 7/7: [===========] - loss: 1165.3491014983086\n",
      " Epoch 556/1000; Batch 7/7: [===========] - loss: 1235.3703764395395\n",
      " Epoch 557/1000; Batch 7/7: [===========] - loss: 1254.0605294428462\n",
      " Epoch 558/1000; Batch 7/7: [===========] - loss: 1447.6993604109139\n",
      " Epoch 559/1000; Batch 7/7: [===========] - loss: 1451.7718583400688\n",
      " Epoch 560/1000; Batch 7/7: [===========] - loss: 2026.0964972071873\n",
      " Epoch 561/1000; Batch 7/7: [===========] - loss: 1542.2653539785092\n",
      " Epoch 562/1000; Batch 7/7: [===========] - loss: 1407.3017161958147\n",
      " Epoch 563/1000; Batch 7/7: [===========] - loss: 1912.2543339184197\n",
      " Epoch 564/1000; Batch 7/7: [===========] - loss: 1394.4765304625316\n",
      " Epoch 565/1000; Batch 7/7: [===========] - loss: 1550.3861909821653\n",
      " Epoch 566/1000; Batch 7/7: [===========] - loss: 1145.6200132064487\n",
      " Epoch 567/1000; Batch 7/7: [===========] - loss: 1368.6256106828498\n",
      " Epoch 568/1000; Batch 7/7: [===========] - loss: 1698.5534430220619\n",
      " Epoch 569/1000; Batch 7/7: [===========] - loss: 1233.6631758254398\n",
      " Epoch 570/1000; Batch 7/7: [===========] - loss: 1806.8995287240896\n",
      " Epoch 571/1000; Batch 7/7: [===========] - loss: 1549.3729470077474\n",
      " Epoch 572/1000; Batch 7/7: [===========] - loss: 1204.2115327130593\n",
      " Epoch 573/1000; Batch 7/7: [===========] - loss: 1307.9212910978083\n",
      " Epoch 574/1000; Batch 7/7: [===========] - loss: 1726.1839058434362\n",
      " Epoch 575/1000; Batch 7/7: [===========] - loss: 1547.0801575686369\n",
      " Epoch 576/1000; Batch 7/7: [===========] - loss: 1203.6902490290747\n",
      " Epoch 577/1000; Batch 7/7: [===========] - loss: 1550.0799926050047\n",
      " Epoch 578/1000; Batch 7/7: [===========] - loss: 1512.8964705037964\n",
      " Epoch 579/1000; Batch 7/7: [===========] - loss: 1012.9201112176984\n",
      " Epoch 580/1000; Batch 7/7: [===========] - loss: 1183.4516049074637\n",
      " Epoch 581/1000; Batch 7/7: [===========] - loss: 1271.5852854168456\n",
      " Epoch 582/1000; Batch 7/7: [===========] - loss: 1675.0452662581934\n",
      " Epoch 583/1000; Batch 7/7: [===========] - loss: 1817.5638299045046\n",
      " Epoch 584/1000; Batch 7/7: [===========] - loss: 1414.9796420045028\n",
      " Epoch 585/1000; Batch 7/7: [===========] - loss: 1454.1265524007626\n",
      " Epoch 586/1000; Batch 7/7: [===========] - loss: 1731.7621614990242\n",
      " Epoch 587/1000; Batch 7/7: [===========] - loss: 1767.0018391318474\n",
      " Epoch 588/1000; Batch 7/7: [===========] - loss: 1445.6904495717163\n",
      " Epoch 589/1000; Batch 7/7: [===========] - loss: 1553.2153241929843\n",
      " Epoch 590/1000; Batch 7/7: [===========] - loss: 1661.2281253145104\n",
      " Epoch 591/1000; Batch 7/7: [===========] - loss: 1292.5049127231629\n",
      " Epoch 592/1000; Batch 7/7: [===========] - loss: 1537.2774370902946\n",
      " Epoch 593/1000; Batch 7/7: [===========] - loss: 1150.6726176622733\n",
      " Epoch 594/1000; Batch 7/7: [===========] - loss: 1109.1727835316735\n",
      " Epoch 595/1000; Batch 7/7: [===========] - loss: 1397.5336598231428\n",
      " Epoch 596/1000; Batch 7/7: [===========] - loss: 2365.9905188235351\n",
      " Epoch 597/1000; Batch 7/7: [===========] - loss: 1418.4064348193479\n",
      " Epoch 598/1000; Batch 7/7: [===========] - loss: 1793.6435104374523\n",
      " Epoch 599/1000; Batch 7/7: [===========] - loss: 1299.4539394294536\n",
      " Epoch 600/1000; Batch 7/7: [===========] - loss: 1543.5156650223384\n",
      " Epoch 601/1000; Batch 7/7: [===========] - loss: 1340.8969819272834\n",
      " Epoch 602/1000; Batch 7/7: [===========] - loss: 1540.1682387612361\n",
      " Epoch 603/1000; Batch 7/7: [===========] - loss: 1068.6083583467499\n",
      " Epoch 604/1000; Batch 7/7: [===========] - loss: 1879.2010949199175\n",
      " Epoch 605/1000; Batch 7/7: [===========] - loss: 1327.0532408403626\n",
      " Epoch 606/1000; Batch 7/7: [===========] - loss: 1441.9477077335525\n",
      " Epoch 607/1000; Batch 7/7: [===========] - loss: 1694.9507781099774\n",
      " Epoch 608/1000; Batch 7/7: [===========] - loss: 1468.8518441549904\n",
      " Epoch 609/1000; Batch 7/7: [===========] - loss: 1810.3724397408319\n",
      " Epoch 610/1000; Batch 7/7: [===========] - loss: 1735.4055213312313\n",
      " Epoch 611/1000; Batch 7/7: [===========] - loss: 1444.7636076303368\n",
      " Epoch 612/1000; Batch 7/7: [===========] - loss: 1432.0977616190532\n",
      " Epoch 613/1000; Batch 7/7: [===========] - loss: 1443.4675409027487\n",
      " Epoch 614/1000; Batch 7/7: [===========] - loss: 1425.3329115698596\n",
      " Epoch 615/1000; Batch 7/7: [===========] - loss: 1263.4650705699937\n",
      " Epoch 616/1000; Batch 7/7: [===========] - loss: 2032.3699069855234\n",
      " Epoch 617/1000; Batch 7/7: [===========] - loss: 1195.2616028503865\n",
      " Epoch 618/1000; Batch 7/7: [===========] - loss: 2005.1702458463576\n",
      " Epoch 619/1000; Batch 7/7: [===========] - loss: 1319.5985371554973\n",
      " Epoch 620/1000; Batch 7/7: [===========] - loss: 1367.5483253356913\n",
      " Epoch 621/1000; Batch 7/7: [===========] - loss: 1805.7416252229598\n",
      " Epoch 622/1000; Batch 7/7: [===========] - loss: 1230.7142482050479\n",
      " Epoch 623/1000; Batch 7/7: [===========] - loss: 1251.2640914418948\n",
      " Epoch 624/1000; Batch 7/7: [===========] - loss: 1416.7940844710017\n",
      " Epoch 625/1000; Batch 7/7: [===========] - loss: 1705.5545889597645\n",
      " Epoch 626/1000; Batch 7/7: [===========] - loss: 1520.7783363163797\n",
      " Epoch 627/1000; Batch 7/7: [===========] - loss: 1828.7243739836051\n",
      " Epoch 628/1000; Batch 7/7: [===========] - loss: 1127.5094217042865\n",
      " Epoch 629/1000; Batch 7/7: [===========] - loss: 1301.4167662108653\n",
      " Epoch 630/1000; Batch 7/7: [===========] - loss: 1361.3097141073495\n",
      " Epoch 631/1000; Batch 7/7: [===========] - loss: 1158.4531838573262\n",
      " Epoch 632/1000; Batch 7/7: [===========] - loss: 1590.3108626411413\n",
      " Epoch 633/1000; Batch 7/7: [===========] - loss: 1387.0217631512458\n",
      " Epoch 634/1000; Batch 7/7: [===========] - loss: 1417.6527810631205\n",
      " Epoch 635/1000; Batch 7/7: [===========] - loss: 1772.9853124574393\n",
      " Epoch 636/1000; Batch 7/7: [===========] - loss: 1557.6478118612595\n",
      " Epoch 637/1000; Batch 7/7: [===========] - loss: 955.32709855280094\n",
      " Epoch 638/1000; Batch 7/7: [===========] - loss: 1448.3073318834517\n",
      " Epoch 639/1000; Batch 7/7: [===========] - loss: 1922.1512472128006\n",
      " Epoch 640/1000; Batch 7/7: [===========] - loss: 1913.2809904930546\n",
      " Epoch 641/1000; Batch 7/7: [===========] - loss: 1279.3175816173245\n",
      " Epoch 642/1000; Batch 7/7: [===========] - loss: 1443.4562141901056\n",
      " Epoch 643/1000; Batch 7/7: [===========] - loss: 1300.6408063037618\n",
      " Epoch 644/1000; Batch 7/7: [===========] - loss: 1390.0902874581839\n",
      " Epoch 645/1000; Batch 7/7: [===========] - loss: 1619.9099770389785\n",
      " Epoch 646/1000; Batch 7/7: [===========] - loss: 1905.3194487191504\n",
      " Epoch 647/1000; Batch 7/7: [===========] - loss: 1816.6595509072784\n",
      " Epoch 648/1000; Batch 7/7: [===========] - loss: 1427.1809999076356\n",
      " Epoch 649/1000; Batch 7/7: [===========] - loss: 1472.9440824013677\n",
      " Epoch 650/1000; Batch 7/7: [===========] - loss: 1295.1556012310846\n",
      " Epoch 651/1000; Batch 7/7: [===========] - loss: 1448.5236318152458\n",
      " Epoch 652/1000; Batch 7/7: [===========] - loss: 1187.3362437357534\n",
      " Epoch 653/1000; Batch 7/7: [===========] - loss: 1691.3031946756634\n",
      " Epoch 654/1000; Batch 7/7: [===========] - loss: 1583.8879842666713\n",
      " Epoch 655/1000; Batch 7/7: [===========] - loss: 1062.3640138551025\n",
      " Epoch 656/1000; Batch 7/7: [===========] - loss: 1406.9826135778515\n",
      " Epoch 657/1000; Batch 7/7: [===========] - loss: 1770.1016044974135\n",
      " Epoch 658/1000; Batch 7/7: [===========] - loss: 1274.8225282183791\n",
      " Epoch 659/1000; Batch 7/7: [===========] - loss: 1710.6470543370394\n",
      " Epoch 660/1000; Batch 7/7: [===========] - loss: 1517.2426043258265\n",
      " Epoch 661/1000; Batch 7/7: [===========] - loss: 1364.9883295669812\n",
      " Epoch 662/1000; Batch 7/7: [===========] - loss: 1213.9859226006922\n",
      " Epoch 663/1000; Batch 7/7: [===========] - loss: 1701.6189732094322\n",
      " Epoch 664/1000; Batch 7/7: [===========] - loss: 1822.9863967435804\n",
      " Epoch 665/1000; Batch 7/7: [===========] - loss: 1661.5558082933253\n",
      " Epoch 666/1000; Batch 7/7: [===========] - loss: 1620.2369114841828\n",
      " Epoch 667/1000; Batch 7/7: [===========] - loss: 1320.4522067419337\n",
      " Epoch 668/1000; Batch 7/7: [===========] - loss: 1452.0304100702008\n",
      " Epoch 669/1000; Batch 7/7: [===========] - loss: 1229.8288322298129\n",
      " Epoch 670/1000; Batch 7/7: [===========] - loss: 1712.4857814605332\n",
      " Epoch 671/1000; Batch 7/7: [===========] - loss: 1371.4044312365136\n",
      " Epoch 672/1000; Batch 7/7: [===========] - loss: 1389.9672880521812\n",
      " Epoch 673/1000; Batch 7/7: [===========] - loss: 1314.7141500011912\n",
      " Epoch 674/1000; Batch 7/7: [===========] - loss: 1439.6286241471655\n",
      " Epoch 675/1000; Batch 7/7: [===========] - loss: 1886.2575251279904\n",
      " Epoch 676/1000; Batch 7/7: [===========] - loss: 1327.3768657672504\n",
      " Epoch 677/1000; Batch 7/7: [===========] - loss: 1438.7731816437422\n",
      " Epoch 678/1000; Batch 7/7: [===========] - loss: 1471.3440276808512\n",
      " Epoch 679/1000; Batch 7/7: [===========] - loss: 1486.8612367820155\n",
      " Epoch 680/1000; Batch 7/7: [===========] - loss: 1513.7148296350032\n",
      " Epoch 681/1000; Batch 7/7: [===========] - loss: 1551.5095632668388\n",
      " Epoch 682/1000; Batch 7/7: [===========] - loss: 1032.9736875011413\n",
      " Epoch 683/1000; Batch 7/7: [===========] - loss: 1587.5486012296901\n",
      " Epoch 684/1000; Batch 7/7: [===========] - loss: 1446.2344242780598\n",
      " Epoch 685/1000; Batch 7/7: [===========] - loss: 1451.6865529061745\n",
      " Epoch 686/1000; Batch 7/7: [===========] - loss: 1354.1517641009893\n",
      " Epoch 687/1000; Batch 7/7: [===========] - loss: 1452.8773042321816\n",
      " Epoch 688/1000; Batch 7/7: [===========] - loss: 1807.6549788631369\n",
      " Epoch 689/1000; Batch 7/7: [===========] - loss: 1513.0629965813514\n",
      " Epoch 690/1000; Batch 7/7: [===========] - loss: 1243.7309371341282\n",
      " Epoch 691/1000; Batch 7/7: [===========] - loss: 1232.4970946404223\n",
      " Epoch 692/1000; Batch 7/7: [===========] - loss: 1268.4469247003124\n",
      " Epoch 693/1000; Batch 7/7: [===========] - loss: 1195.2117710913617\n",
      " Epoch 694/1000; Batch 7/7: [===========] - loss: 1177.4515761536338\n",
      " Epoch 695/1000; Batch 7/7: [===========] - loss: 1507.8664476268618\n",
      " Epoch 696/1000; Batch 7/7: [===========] - loss: 1549.7249024215115\n",
      " Epoch 697/1000; Batch 7/7: [===========] - loss: 1363.9328510577684\n",
      " Epoch 698/1000; Batch 7/7: [===========] - loss: 1327.4396504270082\n",
      " Epoch 699/1000; Batch 7/7: [===========] - loss: 1025.6715864395512\n",
      " Epoch 700/1000; Batch 7/7: [===========] - loss: 1457.1303264874166\n",
      " Epoch 701/1000; Batch 7/7: [===========] - loss: 1498.5062462559792\n",
      " Epoch 702/1000; Batch 7/7: [===========] - loss: 1782.6039494432013\n",
      " Epoch 703/1000; Batch 7/7: [===========] - loss: 1471.8173027420446\n",
      " Epoch 704/1000; Batch 7/7: [===========] - loss: 1681.2343773132185\n",
      " Epoch 705/1000; Batch 7/7: [===========] - loss: 1491.0094053056634\n",
      " Epoch 706/1000; Batch 7/7: [===========] - loss: 1551.0125913787317\n",
      " Epoch 707/1000; Batch 7/7: [===========] - loss: 1413.2892741390872\n",
      " Epoch 708/1000; Batch 7/7: [===========] - loss: 1454.7443581887818\n",
      " Epoch 709/1000; Batch 7/7: [===========] - loss: 1147.1446627651735\n",
      " Epoch 710/1000; Batch 7/7: [===========] - loss: 1092.6421845435016\n",
      " Epoch 711/1000; Batch 7/7: [===========] - loss: 1089.7262406476946\n",
      " Epoch 712/1000; Batch 7/7: [===========] - loss: 1366.7652616478906\n",
      " Epoch 713/1000; Batch 7/7: [===========] - loss: 1661.4734452842629\n",
      " Epoch 714/1000; Batch 7/7: [===========] - loss: 1165.5762965709773\n",
      " Epoch 715/1000; Batch 7/7: [===========] - loss: 1130.6096194013176\n",
      " Epoch 716/1000; Batch 7/7: [===========] - loss: 1428.4156249601652\n",
      " Epoch 717/1000; Batch 7/7: [===========] - loss: 1066.3381235133845\n",
      " Epoch 718/1000; Batch 7/7: [===========] - loss: 1790.2382837748444\n",
      " Epoch 719/1000; Batch 7/7: [===========] - loss: 1336.8016726156154\n",
      " Epoch 720/1000; Batch 7/7: [===========] - loss: 1453.1166342154581\n",
      " Epoch 721/1000; Batch 7/7: [===========] - loss: 1573.6603785061359\n",
      " Epoch 722/1000; Batch 7/7: [===========] - loss: 1513.8867980157133\n",
      " Epoch 723/1000; Batch 7/7: [===========] - loss: 1766.9857223103224\n",
      " Epoch 724/1000; Batch 7/7: [===========] - loss: 1193.2876310485897\n",
      " Epoch 725/1000; Batch 7/7: [===========] - loss: 1919.1017060874417\n",
      " Epoch 726/1000; Batch 7/7: [===========] - loss: 1625.3437075643844\n",
      " Epoch 727/1000; Batch 7/7: [===========] - loss: 1657.4845302273935\n",
      " Epoch 728/1000; Batch 7/7: [===========] - loss: 1327.8993124489677\n",
      " Epoch 729/1000; Batch 7/7: [===========] - loss: 1519.9635448479598\n",
      " Epoch 730/1000; Batch 7/7: [===========] - loss: 1412.9335016409873\n",
      " Epoch 731/1000; Batch 7/7: [===========] - loss: 1453.5584634589612\n",
      " Epoch 732/1000; Batch 7/7: [===========] - loss: 1322.0171797108226\n",
      " Epoch 733/1000; Batch 7/7: [===========] - loss: 1161.4884958313467\n",
      " Epoch 734/1000; Batch 7/7: [===========] - loss: 1444.7809679891677\n",
      " Epoch 735/1000; Batch 7/7: [===========] - loss: 1063.6820856832403\n",
      " Epoch 736/1000; Batch 7/7: [===========] - loss: 1177.8259938695144\n",
      " Epoch 737/1000; Batch 7/7: [===========] - loss: 1286.9816250482947\n",
      " Epoch 738/1000; Batch 7/7: [===========] - loss: 1925.4279252016495\n",
      " Epoch 739/1000; Batch 7/7: [===========] - loss: 1513.8950268929418\n",
      " Epoch 740/1000; Batch 7/7: [===========] - loss: 1056.8346803447283\n",
      " Epoch 741/1000; Batch 7/7: [===========] - loss: 1639.5876567314556\n",
      " Epoch 742/1000; Batch 7/7: [===========] - loss: 1194.4233090826757\n",
      " Epoch 743/1000; Batch 7/7: [===========] - loss: 1346.6921720628166\n",
      " Epoch 744/1000; Batch 7/7: [===========] - loss: 1484.5021440664627\n",
      " Epoch 745/1000; Batch 7/7: [===========] - loss: 1415.8979348038526\n",
      " Epoch 746/1000; Batch 7/7: [===========] - loss: 1541.7359061725551\n",
      " Epoch 747/1000; Batch 7/7: [===========] - loss: 1411.5934646334645\n",
      " Epoch 748/1000; Batch 7/7: [===========] - loss: 1990.3483087162663\n",
      " Epoch 749/1000; Batch 7/7: [===========] - loss: 1668.2469348364411\n",
      " Epoch 750/1000; Batch 7/7: [===========] - loss: 1193.7071463846194\n",
      " Epoch 751/1000; Batch 7/7: [===========] - loss: 1253.2403370672322\n",
      " Epoch 752/1000; Batch 7/7: [===========] - loss: 1920.5326680614376\n",
      " Epoch 753/1000; Batch 7/7: [===========] - loss: 1308.8900712813745\n",
      " Epoch 754/1000; Batch 7/7: [===========] - loss: 1726.7429717914688\n",
      " Epoch 755/1000; Batch 7/7: [===========] - loss: 1546.3820629846022\n",
      " Epoch 756/1000; Batch 7/7: [===========] - loss: 1315.0587239439782\n",
      " Epoch 757/1000; Batch 7/7: [===========] - loss: 1690.5898869715657\n",
      " Epoch 758/1000; Batch 7/7: [===========] - loss: 2057.2664613151182\n",
      " Epoch 759/1000; Batch 7/7: [===========] - loss: 1755.3921078019007\n",
      " Epoch 760/1000; Batch 7/7: [===========] - loss: 1252.2707878066012\n",
      " Epoch 761/1000; Batch 7/7: [===========] - loss: 1573.5478908297869\n",
      " Epoch 762/1000; Batch 7/7: [===========] - loss: 1386.8890253940888\n",
      " Epoch 763/1000; Batch 7/7: [===========] - loss: 1659.2450343927815\n",
      " Epoch 764/1000; Batch 7/7: [===========] - loss: 995.29960590928433\n",
      " Epoch 765/1000; Batch 7/7: [===========] - loss: 1420.8305659091832\n",
      " Epoch 766/1000; Batch 7/7: [===========] - loss: 1453.7733579468681\n",
      " Epoch 767/1000; Batch 7/7: [===========] - loss: 1773.6564967377897\n",
      " Epoch 768/1000; Batch 7/7: [===========] - loss: 1579.8253145955637\n",
      " Epoch 769/1000; Batch 7/7: [===========] - loss: 1401.0092542341308\n",
      " Epoch 770/1000; Batch 7/7: [===========] - loss: 1174.7648465885798\n",
      " Epoch 771/1000; Batch 7/7: [===========] - loss: 1718.2003178592229\n",
      " Epoch 772/1000; Batch 7/7: [===========] - loss: 1571.3757281265473\n",
      " Epoch 773/1000; Batch 7/7: [===========] - loss: 1731.5017262153248\n",
      " Epoch 774/1000; Batch 7/7: [===========] - loss: 1745.0938776321045\n",
      " Epoch 775/1000; Batch 7/7: [===========] - loss: 1190.4218937200878\n",
      " Epoch 776/1000; Batch 7/7: [===========] - loss: 1132.0536538851284\n",
      " Epoch 777/1000; Batch 7/7: [===========] - loss: 1178.2472122713037\n",
      " Epoch 778/1000; Batch 7/7: [===========] - loss: 1477.8467416785657\n",
      " Epoch 779/1000; Batch 7/7: [===========] - loss: 1186.1388307528848\n",
      " Epoch 780/1000; Batch 7/7: [===========] - loss: 1611.0649666289835\n",
      " Epoch 781/1000; Batch 7/7: [===========] - loss: 1114.6652754708332\n",
      " Epoch 782/1000; Batch 7/7: [===========] - loss: 1066.5203375277104\n",
      " Epoch 783/1000; Batch 7/7: [===========] - loss: 1450.4992658305305\n",
      " Epoch 784/1000; Batch 7/7: [===========] - loss: 1547.6362435833153\n",
      " Epoch 785/1000; Batch 7/7: [===========] - loss: 1444.9190810941825\n",
      " Epoch 786/1000; Batch 7/7: [===========] - loss: 1605.6164777672125\n",
      " Epoch 787/1000; Batch 7/7: [===========] - loss: 1737.4102719860336\n",
      " Epoch 788/1000; Batch 7/7: [===========] - loss: 1488.6749829575774\n",
      " Epoch 789/1000; Batch 7/7: [===========] - loss: 1414.5623383339465\n",
      " Epoch 790/1000; Batch 7/7: [===========] - loss: 1069.9828636381094\n",
      " Epoch 791/1000; Batch 7/7: [===========] - loss: 1470.0803666027305\n",
      " Epoch 792/1000; Batch 7/7: [===========] - loss: 1223.3573962898345\n",
      " Epoch 793/1000; Batch 7/7: [===========] - loss: 1159.3675047606255\n",
      " Epoch 794/1000; Batch 7/7: [===========] - loss: 1651.8075692385319\n",
      " Epoch 795/1000; Batch 7/7: [===========] - loss: 1482.7260332606086\n",
      " Epoch 796/1000; Batch 7/7: [===========] - loss: 1743.5284885680012\n",
      " Epoch 797/1000; Batch 7/7: [===========] - loss: 1220.1757496494085\n",
      " Epoch 798/1000; Batch 7/7: [===========] - loss: 1445.6734833908858\n",
      " Epoch 799/1000; Batch 7/7: [===========] - loss: 1636.0497169016726\n",
      " Epoch 800/1000; Batch 7/7: [===========] - loss: 1904.6094964464721\n",
      " Epoch 801/1000; Batch 7/7: [===========] - loss: 1896.5607903469878\n",
      " Epoch 802/1000; Batch 7/7: [===========] - loss: 1316.2935440273247\n",
      " Epoch 803/1000; Batch 7/7: [===========] - loss: 1475.9582149352987\n",
      " Epoch 804/1000; Batch 7/7: [===========] - loss: 1224.6453914658816\n",
      " Epoch 805/1000; Batch 7/7: [===========] - loss: 1373.5361473926494\n",
      " Epoch 806/1000; Batch 7/7: [===========] - loss: 1107.2279374724942\n",
      " Epoch 807/1000; Batch 7/7: [===========] - loss: 1156.8976479013222\n",
      " Epoch 808/1000; Batch 7/7: [===========] - loss: 1034.5352761838314\n",
      " Epoch 809/1000; Batch 7/7: [===========] - loss: 1757.4426408223428\n",
      " Epoch 810/1000; Batch 7/7: [===========] - loss: 1548.3162881033056\n",
      " Epoch 811/1000; Batch 7/7: [===========] - loss: 1532.9238302079693\n",
      " Epoch 812/1000; Batch 7/7: [===========] - loss: 1156.2672164850744\n",
      " Epoch 813/1000; Batch 7/7: [===========] - loss: 1239.4743261728304\n",
      " Epoch 814/1000; Batch 7/7: [===========] - loss: 1800.2933859958969\n",
      " Epoch 815/1000; Batch 7/7: [===========] - loss: 1383.6886548626144\n",
      " Epoch 816/1000; Batch 7/7: [===========] - loss: 1389.6586295200498\n",
      " Epoch 817/1000; Batch 7/7: [===========] - loss: 1582.6756294078766\n",
      " Epoch 818/1000; Batch 7/7: [===========] - loss: 1168.2366777911786\n",
      " Epoch 819/1000; Batch 7/7: [===========] - loss: 1150.5952792579103\n",
      " Epoch 820/1000; Batch 7/7: [===========] - loss: 1492.4128269250857\n",
      " Epoch 821/1000; Batch 7/7: [===========] - loss: 1817.3505337675238\n",
      " Epoch 822/1000; Batch 7/7: [===========] - loss: 1060.3901809088004\n",
      " Epoch 823/1000; Batch 7/7: [===========] - loss: 1521.9009785873407\n",
      " Epoch 824/1000; Batch 7/7: [===========] - loss: 1654.1098897818644\n",
      " Epoch 825/1000; Batch 7/7: [===========] - loss: 1769.1812569958363\n",
      " Epoch 826/1000; Batch 7/7: [===========] - loss: 1767.1829836168324\n",
      " Epoch 827/1000; Batch 7/7: [===========] - loss: 1790.2396601075632\n",
      " Epoch 828/1000; Batch 7/7: [===========] - loss: 1199.5030663798564\n",
      " Epoch 829/1000; Batch 7/7: [===========] - loss: 1470.0226635083343\n",
      " Epoch 830/1000; Batch 7/7: [===========] - loss: 1116.0823267779945\n",
      " Epoch 831/1000; Batch 7/7: [===========] - loss: 1508.1918981499478\n",
      " Epoch 832/1000; Batch 7/7: [===========] - loss: 976.77073942302018\n",
      " Epoch 833/1000; Batch 7/7: [===========] - loss: 1791.5050736622636\n",
      " Epoch 834/1000; Batch 7/7: [===========] - loss: 1466.8115110437658\n",
      " Epoch 835/1000; Batch 7/7: [===========] - loss: 1910.0313166886613\n",
      " Epoch 836/1000; Batch 7/7: [===========] - loss: 1241.5102234202113\n",
      " Epoch 837/1000; Batch 7/7: [===========] - loss: 1680.2309372675283\n",
      " Epoch 838/1000; Batch 7/7: [===========] - loss: 1157.0836196337371\n",
      " Epoch 839/1000; Batch 7/7: [===========] - loss: 1476.0328869985105\n",
      " Epoch 840/1000; Batch 7/7: [===========] - loss: 1475.6320005570942\n",
      " Epoch 841/1000; Batch 7/7: [===========] - loss: 1144.9401215910377\n",
      " Epoch 842/1000; Batch 7/7: [===========] - loss: 1293.6807081629996\n",
      " Epoch 843/1000; Batch 7/7: [===========] - loss: 1696.7721386386602\n",
      " Epoch 844/1000; Batch 7/7: [===========] - loss: 1300.6652842924368\n",
      " Epoch 845/1000; Batch 7/7: [===========] - loss: 1494.9060982850872\n",
      " Epoch 846/1000; Batch 7/7: [===========] - loss: 1372.6860178035258\n",
      " Epoch 847/1000; Batch 7/7: [===========] - loss: 1711.2029298092525\n",
      " Epoch 848/1000; Batch 7/7: [===========] - loss: 1306.3841510422105\n",
      " Epoch 849/1000; Batch 7/7: [===========] - loss: 1112.1553517057627\n",
      " Epoch 850/1000; Batch 7/7: [===========] - loss: 2058.3200796706845\n",
      " Epoch 851/1000; Batch 7/7: [===========] - loss: 1840.5940755757167\n",
      " Epoch 852/1000; Batch 7/7: [===========] - loss: 1226.3917062278904\n",
      " Epoch 853/1000; Batch 7/7: [===========] - loss: 1666.3489194996678\n",
      " Epoch 854/1000; Batch 7/7: [===========] - loss: 1382.0382335398085\n",
      " Epoch 855/1000; Batch 7/7: [===========] - loss: 1234.9036171895818\n",
      " Epoch 856/1000; Batch 7/7: [===========] - loss: 1503.5733958539186\n",
      " Epoch 857/1000; Batch 7/7: [===========] - loss: 1361.1903548822033\n",
      " Epoch 858/1000; Batch 7/7: [===========] - loss: 1640.5769861794695\n",
      " Epoch 859/1000; Batch 7/7: [===========] - loss: 2253.5492068034345\n",
      " Epoch 860/1000; Batch 7/7: [===========] - loss: 1354.1931737282932\n",
      " Epoch 861/1000; Batch 7/7: [===========] - loss: 1546.5705285390559\n",
      " Epoch 862/1000; Batch 7/7: [===========] - loss: 1629.5769825340658\n",
      " Epoch 863/1000; Batch 7/7: [===========] - loss: 1510.9094679809404\n",
      " Epoch 864/1000; Batch 7/7: [===========] - loss: 1759.9069647599994\n",
      " Epoch 865/1000; Batch 7/7: [===========] - loss: 1174.7864362893495\n",
      " Epoch 866/1000; Batch 7/7: [===========] - loss: 1654.9563191128953\n",
      " Epoch 867/1000; Batch 7/7: [===========] - loss: 1343.2743369934783\n",
      " Epoch 868/1000; Batch 7/7: [===========] - loss: 1091.4015782814145\n",
      " Epoch 869/1000; Batch 7/7: [===========] - loss: 1366.7757837555831\n",
      " Epoch 870/1000; Batch 7/7: [===========] - loss: 1447.1997948287408\n",
      " Epoch 871/1000; Batch 7/7: [===========] - loss: 1300.9296695783273\n",
      " Epoch 872/1000; Batch 7/7: [===========] - loss: 1588.4018558427417\n",
      " Epoch 873/1000; Batch 7/7: [===========] - loss: 1918.1623549156034\n",
      " Epoch 874/1000; Batch 7/7: [===========] - loss: 1125.6025605550198\n",
      " Epoch 875/1000; Batch 7/7: [===========] - loss: 1290.1163159243388\n",
      " Epoch 876/1000; Batch 7/7: [===========] - loss: 1364.4278938619104\n",
      " Epoch 877/1000; Batch 7/7: [===========] - loss: 1973.5491048209499\n",
      " Epoch 878/1000; Batch 7/7: [===========] - loss: 1943.5222096514308\n",
      " Epoch 879/1000; Batch 7/7: [===========] - loss: 1310.2684330152284\n",
      " Epoch 880/1000; Batch 7/7: [===========] - loss: 1706.6805885412896\n",
      " Epoch 881/1000; Batch 7/7: [===========] - loss: 1542.4929877483868\n",
      " Epoch 882/1000; Batch 7/7: [===========] - loss: 1020.8046367161743\n",
      " Epoch 883/1000; Batch 7/7: [===========] - loss: 1945.1409842697994\n",
      " Epoch 884/1000; Batch 7/7: [===========] - loss: 1300.4705066503502\n",
      " Epoch 885/1000; Batch 7/7: [===========] - loss: 1235.8141192431217\n",
      " Epoch 886/1000; Batch 7/7: [===========] - loss: 1792.7289907378324\n",
      " Epoch 887/1000; Batch 7/7: [===========] - loss: 1173.9947939742976\n",
      " Epoch 888/1000; Batch 7/7: [===========] - loss: 1405.5578447090759\n",
      " Epoch 889/1000; Batch 7/7: [===========] - loss: 1318.4955743488756\n",
      " Epoch 890/1000; Batch 7/7: [===========] - loss: 1371.7036042419852\n",
      " Epoch 891/1000; Batch 7/7: [===========] - loss: 1424.4890494913776\n",
      " Epoch 892/1000; Batch 7/7: [===========] - loss: 1534.2895132033905\n",
      " Epoch 893/1000; Batch 7/7: [===========] - loss: 1481.5750182505126\n",
      " Epoch 894/1000; Batch 7/7: [===========] - loss: 1218.1349162404497\n",
      " Epoch 895/1000; Batch 7/7: [===========] - loss: 1336.0842571291337\n",
      " Epoch 896/1000; Batch 7/7: [===========] - loss: 1078.5139357738615\n",
      " Epoch 897/1000; Batch 7/7: [===========] - loss: 1705.1402471498766\n",
      " Epoch 898/1000; Batch 7/7: [===========] - loss: 1258.5315687416976\n",
      " Epoch 899/1000; Batch 7/7: [===========] - loss: 1216.1141956876652\n",
      " Epoch 900/1000; Batch 7/7: [===========] - loss: 1480.8064722705171\n",
      " Epoch 901/1000; Batch 7/7: [===========] - loss: 1381.1318134038318\n",
      " Epoch 902/1000; Batch 7/7: [===========] - loss: 1234.9492425189833\n",
      " Epoch 903/1000; Batch 7/7: [===========] - loss: 1769.8199664246704\n",
      " Epoch 904/1000; Batch 7/7: [===========] - loss: 1239.9680319377434\n",
      " Epoch 905/1000; Batch 7/7: [===========] - loss: 1604.4610455417426\n",
      " Epoch 906/1000; Batch 7/7: [===========] - loss: 1632.3514167527835\n",
      " Epoch 907/1000; Batch 7/7: [===========] - loss: 1477.7169186346914\n",
      " Epoch 908/1000; Batch 7/7: [===========] - loss: 1106.4727960250527\n",
      " Epoch 909/1000; Batch 7/7: [===========] - loss: 1107.1042066355074\n",
      " Epoch 910/1000; Batch 7/7: [===========] - loss: 1628.6342280646734\n",
      " Epoch 911/1000; Batch 7/7: [===========] - loss: 1467.4290775868046\n",
      " Epoch 912/1000; Batch 7/7: [===========] - loss: 1732.7398903406656\n",
      " Epoch 913/1000; Batch 7/7: [===========] - loss: 1351.3157137010016\n",
      " Epoch 914/1000; Batch 7/7: [===========] - loss: 1974.5782720443933\n",
      " Epoch 915/1000; Batch 7/7: [===========] - loss: 1273.5510028597375\n",
      " Epoch 916/1000; Batch 7/7: [===========] - loss: 1447.7496924182394\n",
      " Epoch 917/1000; Batch 7/7: [===========] - loss: 1171.6411421586308\n",
      " Epoch 918/1000; Batch 7/7: [===========] - loss: 1806.5572383097094\n",
      " Epoch 919/1000; Batch 7/7: [===========] - loss: 1276.3744299938069\n",
      " Epoch 920/1000; Batch 7/7: [===========] - loss: 1342.6883008777104\n",
      " Epoch 921/1000; Batch 7/7: [===========] - loss: 1578.0554837998302\n",
      " Epoch 922/1000; Batch 7/7: [===========] - loss: 1450.6762336723414\n",
      " Epoch 923/1000; Batch 7/7: [===========] - loss: 1735.0508095982487\n",
      " Epoch 924/1000; Batch 7/7: [===========] - loss: 1438.2893146916513\n",
      " Epoch 925/1000; Batch 7/7: [===========] - loss: 1174.7783401130418\n",
      " Epoch 926/1000; Batch 7/7: [===========] - loss: 1516.2063668098026\n",
      " Epoch 927/1000; Batch 7/7: [===========] - loss: 1567.2026911100713\n",
      " Epoch 928/1000; Batch 7/7: [===========] - loss: 1282.1118078435654\n",
      " Epoch 929/1000; Batch 7/7: [===========] - loss: 1344.2877658277575\n",
      " Epoch 930/1000; Batch 7/7: [===========] - loss: 1208.5887958676063\n",
      " Epoch 931/1000; Batch 7/7: [===========] - loss: 1269.2646050211083\n",
      " Epoch 932/1000; Batch 7/7: [===========] - loss: 1375.7297234158598\n",
      " Epoch 933/1000; Batch 7/7: [===========] - loss: 958.32831112611328\n",
      " Epoch 934/1000; Batch 7/7: [===========] - loss: 1325.3483590891773\n",
      " Epoch 935/1000; Batch 7/7: [===========] - loss: 1435.5614518622372\n",
      " Epoch 936/1000; Batch 7/7: [===========] - loss: 1734.5188653418261\n",
      " Epoch 937/1000; Batch 7/7: [===========] - loss: 1634.5310619148042\n",
      " Epoch 938/1000; Batch 7/7: [===========] - loss: 1519.0542712701498\n",
      " Epoch 939/1000; Batch 7/7: [===========] - loss: 1191.9933753794407\n",
      " Epoch 940/1000; Batch 7/7: [===========] - loss: 2053.9455796857982\n",
      " Epoch 941/1000; Batch 7/7: [===========] - loss: 1390.2207116209413\n",
      " Epoch 942/1000; Batch 7/7: [===========] - loss: 1291.6065633522646\n",
      " Epoch 943/1000; Batch 7/7: [===========] - loss: 1112.7560113440527\n",
      " Epoch 944/1000; Batch 7/7: [===========] - loss: 1172.6099792412513\n",
      " Epoch 945/1000; Batch 7/7: [===========] - loss: 1277.4380368686764\n",
      " Epoch 946/1000; Batch 7/7: [===========] - loss: 1853.2210274557394\n",
      " Epoch 947/1000; Batch 7/7: [===========] - loss: 1839.1523714954799\n",
      " Epoch 948/1000; Batch 7/7: [===========] - loss: 957.43179916840785\n",
      " Epoch 949/1000; Batch 7/7: [===========] - loss: 1441.0086724436158\n",
      " Epoch 950/1000; Batch 7/7: [===========] - loss: 1024.4599210901802\n",
      " Epoch 951/1000; Batch 7/7: [===========] - loss: 1429.8975386768072\n",
      " Epoch 952/1000; Batch 7/7: [===========] - loss: 1938.2228430750101\n",
      " Epoch 953/1000; Batch 7/7: [===========] - loss: 1536.0876744478173\n",
      " Epoch 954/1000; Batch 7/7: [===========] - loss: 1112.8911778221548\n",
      " Epoch 955/1000; Batch 7/7: [===========] - loss: 1292.2004887553685\n",
      " Epoch 956/1000; Batch 7/7: [===========] - loss: 967.11148754096285\n",
      " Epoch 957/1000; Batch 7/7: [===========] - loss: 1431.7029574581188\n",
      " Epoch 958/1000; Batch 7/7: [===========] - loss: 1427.1319899911973\n",
      " Epoch 959/1000; Batch 7/7: [===========] - loss: 1437.6363673016776\n",
      " Epoch 960/1000; Batch 7/7: [===========] - loss: 1724.4704373076208\n",
      " Epoch 961/1000; Batch 7/7: [===========] - loss: 1038.1414645270797\n",
      " Epoch 962/1000; Batch 7/7: [===========] - loss: 1494.7386394557882\n",
      " Epoch 963/1000; Batch 7/7: [===========] - loss: 1438.2874220332935\n",
      " Epoch 964/1000; Batch 7/7: [===========] - loss: 1071.7948777165088\n",
      " Epoch 965/1000; Batch 7/7: [===========] - loss: 1119.3259018491985\n",
      " Epoch 966/1000; Batch 7/7: [===========] - loss: 1537.5190587894862\n",
      " Epoch 967/1000; Batch 7/7: [===========] - loss: 1481.8458922214088\n",
      " Epoch 968/1000; Batch 7/7: [===========] - loss: 1356.1202990491904\n",
      " Epoch 969/1000; Batch 7/7: [===========] - loss: 1495.0627219826656\n",
      " Epoch 970/1000; Batch 7/7: [===========] - loss: 1219.6633818598025\n",
      " Epoch 971/1000; Batch 7/7: [===========] - loss: 1379.4287510035485\n",
      " Epoch 972/1000; Batch 7/7: [===========] - loss: 1546.5398677963682\n",
      " Epoch 973/1000; Batch 7/7: [===========] - loss: 1500.5914488758306\n",
      " Epoch 974/1000; Batch 7/7: [===========] - loss: 1092.1307743485993\n",
      " Epoch 975/1000; Batch 7/7: [===========] - loss: 1407.6600667897198\n",
      " Epoch 976/1000; Batch 7/7: [===========] - loss: 1633.3074157049682\n",
      " Epoch 977/1000; Batch 7/7: [===========] - loss: 1095.9913391488258\n",
      " Epoch 978/1000; Batch 7/7: [===========] - loss: 1655.1267819573766\n",
      " Epoch 979/1000; Batch 7/7: [===========] - loss: 1108.5248842896544\n",
      " Epoch 980/1000; Batch 7/7: [===========] - loss: 1339.9430279849723\n",
      " Epoch 981/1000; Batch 7/7: [===========] - loss: 1618.2541873721136\n",
      " Epoch 982/1000; Batch 7/7: [===========] - loss: 1136.7728210924233\n",
      " Epoch 983/1000; Batch 7/7: [===========] - loss: 1050.1367111943595\n",
      " Epoch 984/1000; Batch 7/7: [===========] - loss: 1469.8790715792422\n",
      " Epoch 985/1000; Batch 7/7: [===========] - loss: 1273.5502823029015\n",
      " Epoch 986/1000; Batch 7/7: [===========] - loss: 1330.3892475593134\n",
      " Epoch 987/1000; Batch 7/7: [===========] - loss: 1213.2904966364567\n",
      " Epoch 988/1000; Batch 7/7: [===========] - loss: 1324.8198576853479\n",
      " Epoch 989/1000; Batch 7/7: [===========] - loss: 1538.3529853963748\n",
      " Epoch 990/1000; Batch 7/7: [===========] - loss: 1881.0233534693326\n",
      " Epoch 991/1000; Batch 7/7: [===========] - loss: 1381.2342682182407\n",
      " Epoch 992/1000; Batch 7/7: [===========] - loss: 1415.5201770338616\n",
      " Epoch 993/1000; Batch 7/7: [===========] - loss: 1307.3427441860383\n",
      " Epoch 994/1000; Batch 7/7: [===========] - loss: 1288.0418473650036\n",
      " Epoch 995/1000; Batch 7/7: [===========] - loss: 1619.0995463541387\n",
      " Epoch 996/1000; Batch 7/7: [===========] - loss: 1505.9960437339198\n",
      " Epoch 997/1000; Batch 7/7: [===========] - loss: 1469.3371849994976\n",
      " Epoch 998/1000; Batch 7/7: [===========] - loss: 1567.6546396339645\n",
      " Epoch 999/1000; Batch 7/7: [===========] - loss: 1220.4328247301311\n",
      " Epoch 1000/1000; Batch 7/7: [===========] - loss: 1342.9806060749868\n"
     ]
    }
   ],
   "source": [
    "nn = NeauralNetwork(layers=[\n",
    "        Layer(units=10, input_layer=True),\n",
    "        # Layer(units=40, activation=\"sigmoid\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=1),\n",
    "    ],\n",
    "    loss_function = \"mse\",\n",
    "    learning_rate=0.0001, \n",
    "    verbose=True,\n",
    "    optimizer=\"gdm\",\n",
    "    batch_size = 64,\n",
    "    epochs=1000\n",
    ")\n",
    "\n",
    "y_diab = y_diab.reshape(-1, 1) # Network requirement\n",
    "\n",
    "nn.fit(X_diab, y_diab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
