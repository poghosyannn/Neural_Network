{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X_diab, y_diab = load_diabetes(return_X_y=True) # returns diabetes data shapes: (442, 10) and (442,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=60, n_features=10, noise=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(\n",
    "            self, \n",
    "            units, \n",
    "            *, \n",
    "            input_layer: bool = False,\n",
    "            activation: str = \"linear\",\n",
    "            use_bias: bool = True,\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer.\n",
    "\n",
    "        Args:\n",
    "            units (int): Count of neurons in the layer.\n",
    "            input_layer (bool, optional): Whether the layer is an input layer. Defaults to False.\n",
    "            activation (str, optional): Activation function for the layer. Can be \"linear\", \"relu\", or \"sigmoid\". Defaults to \"linear\".\n",
    "            use_bias (bool, optional): Whether to use bias in the layer. Defaults to True.\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        self.units = units\n",
    "        self.input_layer = input_layer\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "\n",
    "        self.optimizer = None # Optimizer for layer\n",
    "\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "\n",
    "        self.w = None # Weights matrix\n",
    "        self._weight_gradient = None # Weights derivative matrix\n",
    "        self._bias_gradient = None # Biases derivative vector\n",
    "\n",
    "    def activationFunction(self, z):\n",
    "        \"\"\"\n",
    "        Apply the activation function to the given input.\n",
    "\n",
    "        Args:\n",
    "            z (numpy.ndarray): Input to the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output after applying the activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return z\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(z, np.zeros(z.shape))\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _weightInit(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the weights matrix based on the input size.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of the input.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return # input_layer doesn't need weights\n",
    "\n",
    "        self.w = np.random.normal(loc = 0, scale = 1 / input_size, size=(input_size, self.units))\n",
    "        # Initialize weights matrix using a normal distribution with mean 0 and variance 1 / input_size\n",
    "\n",
    "        self.bias = np.zeros((1, self.units))\n",
    "        # Initialize biases as zeros\n",
    "\n",
    "\n",
    "    def _setOptimizer(self, optimizer, beta_1, beta_2):\n",
    "        \"\"\"\n",
    "        Set the optimizer and initialize optimizer-specific variables.\n",
    "\n",
    "        Args:\n",
    "            optimizer (str): Optimization algorithm to use.\n",
    "            beta_1 (float): Value for the optimizer parameter beta_1.\n",
    "            beta_2 (float): Value for the optimizer parameter beta_2.\n",
    "\n",
    "        Notes:\n",
    "            - Only executed for layers other than the input layer.\n",
    "            - Sets the optimizer and initializes optimizer-specific variables based on the chosen optimizer.\n",
    "            - For each optimizer, the corresponding variables are initialized.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self._b1 = beta_1\n",
    "        self._b2 = beta_2\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.batch_size = 1  # SGD is the same as mini-batch gradient descent when batch_size = 1\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "            # Initialize weight-specific variables for AdaGrad\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "                # Initialize bias-specific variables for AdaGrad\n",
    "\n",
    "        if self.optimizer == 'adam':\n",
    "            self._iter = 0  # Calculate iterations\n",
    "\n",
    "            self._weight_m = np.zeros(self.w.shape)\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "            # Initialize weight-specific variables for Adam\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = np.zeros(self.bias.shape)\n",
    "                self._bias_v = np.zeros(self.bias.shape)\n",
    "                # Initialize bias-specific variables for Adam\n",
    "\n",
    "        if self.optimizer == 'rms_prop':\n",
    "            self._weight_v = np.zeros(self.w.shape)\n",
    "            self._bias_v = np.zeros(self.bias.shape)\n",
    "            # Initialize weight and bias-specific variables for RMSprop\n",
    "\n",
    "        if self.optimizer == 'gdm':\n",
    "            self._weight_m = np.zeros(self.w.shape)\n",
    "            self._bias_m = np.zeros(self.bias.shape)\n",
    "            # Initialize weight and bias-specific variables for Gradient Descent with Momentum   \n",
    "\n",
    "    def _activationDerivative(self):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the activation function.\n",
    "\n",
    "        Notes:\n",
    "            Only supports the \"linear\", \"relu\", and \"sigmoid\" activation functions.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return 1\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return (self._output > 0) * 1\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self._output * (1 - self._output)\n",
    "\n",
    "    def _setGrad(self, grad):\n",
    "        \"\"\"\n",
    "        Calculate the gradients of weights and bias for backpropagation.\n",
    "\n",
    "        Args:\n",
    "            grad (numpy.ndarray): Gradient from the previous layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient to be passed to the previous layer.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "        \n",
    "        grad = grad * self._activationDerivative()\n",
    "        self._weight_gradient = self._input.T @ grad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self._bias_gradient = grad.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return grad @ self.w.T\n",
    "    \n",
    "    def _updateGrad(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and bias based on the computed gradients.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "\n",
    "        Notes:\n",
    "            - Only executed for layers other than the input layer.\n",
    "            - Updates the weights and biases based on the computed gradients and the chosen optimizer.\n",
    "            - For each optimizer, the corresponding update rule is applied.\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        eps = 10e-8 # Optimizer's epsilon\n",
    "\n",
    "        if self.optimizer == \"gd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._weight_v += np.square(self._weight_gradient)\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._weight_v) + eps)\n",
    "\n",
    "            self.w -= learning_rate_weight * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v += np.square(self._bias_gradient)\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._bias_v) + eps)\n",
    "\n",
    "                self.bias -= learning_rate_bias * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == 'adam':\n",
    "            self._iter += 1\n",
    "\n",
    "            self._weight_m = self._b1 * self._weight_m + (1- self._b1) * self._weight_gradient\n",
    "            self._weight_v = self._b2 * self._weight_v + (1- self._b2) * np.square(self._weight_gradient)\n",
    "\n",
    "            weight_m = self._weight_m / (1 - np.power(self._b1, self._iter))\n",
    "            weight_v = self._weight_v / (1 - np.power(self._b2, self._iter))\n",
    "\n",
    "            self.w -= learning_rate * weight_m / (np.sqrt(weight_v) + eps) # Updating\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = self._b1 * self._bias_m + (1- self._b1) * self._bias_gradient\n",
    "                self._bias_v = self._b2 * self._bias_v + (1- self._b2) * np.square(self._bias_gradient)\n",
    "\n",
    "                bias_m = self._bias_m / (1 - np.power(self._b1, self._iter)) \n",
    "                bias_v = self._bias_v / (1 - np.power(self._b2, self._iter))\n",
    "\n",
    "\n",
    "                self.bias -= learning_rate * bias_m / (np.sqrt(bias_v) + eps) # Updating\n",
    "\n",
    "        \n",
    "        if self.optimizer == 'rms_prop':\n",
    "            self._weight_v = self._b2 * self._weight_v + (1- self._b2) * np.square(self._weight_gradient)\n",
    "\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._weight_v) + eps)\n",
    "\n",
    "            self.w -= learning_rate_weight * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_v = self._b2 * self._bias_v + (1- self._b2) * np.square(self._bias_gradient)\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._bias_v) + eps)\n",
    "\n",
    "                self.bias -= learning_rate_bias * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == 'gdm':\n",
    "            self._weight_m = self._b2 * self._weight_m + (1- self._b2) * self._weight_gradient\n",
    "\n",
    "            self.w -= learning_rate_weight * self._weight_m\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._bias_m = self._b2 * self._bias_m + (1 - self._b2) * self._bias_gradient\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._bias_m) + eps)\n",
    "\n",
    "                self.bias -= learning_rate_bias * self._bias_gradient\n",
    "\n",
    "\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input to the layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output of the layer after applying the activation function.\n",
    "        \"\"\"\n",
    "        if self.input_layer:\n",
    "            return X\n",
    "        \n",
    "        self._input = X\n",
    "        self._output = self.activationFunction(X @ self.w + self.bias)\n",
    "\n",
    "        return self._output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeauralNetwork:\n",
    "    def __init__(\n",
    "            self, \n",
    "            layers: list, \n",
    "            loss_function: str = \"mse\", \n",
    "            learning_rate = 0.01,\n",
    "            verbose: bool = False,\n",
    "            optimizer: str = \"gd\",\n",
    "            epochs: int = 1, \n",
    "            batch_size: int = 32,\n",
    "            beta_1: float = 0.9,\n",
    "            beta_2: float = 0.999\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network.\n",
    "\n",
    "        Args:\n",
    "            layers (list): List of Layer objects defining the network architecture. \n",
    "            loss_function (str, optional): Loss function to use. Defaults to \"mse\".\n",
    "            optimizer (str, optional): Optimization algorithm to use for updating weights during training.\n",
    "                Options include:\n",
    "                - \"gd\" (Gradient Descent): Standard gradient descent.\n",
    "                - \"sgd\" (Stochastic Gradient Descent): Update weights using a single sample at a time.\n",
    "                - \"adagrad\" (Adaptive Gradient): Adjust the learning rate based on the frequency of feature occurrences.\n",
    "                - \"adam\" (Adam): Adaptive Moment Estimation algorithm.\n",
    "                - \"rms_prop\" (Root Mean Square Propagation): Adapt the learning rate based on the moving average of squared gradients.\n",
    "                - \"gdm\" (Gradient Descent with Momentum): Add momentum to the gradient descent algorithm.\n",
    "                Defaults to \"gd\".\n",
    "\n",
    "            learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n",
    "            epochs (int, optional): Number of epochs for training. Defaults to 1.\n",
    "            batch_size (int, optional): Batch size for training. Defaults to 32.\n",
    "            verbose (bool, optional): Whether to display training progress. Defaults to False.\n",
    "\n",
    "            beta_1 (float, optional): Parameter for the optimizer. Defaults to 0.9.\n",
    "            beta_2 (float, optional): Parameter for the optimizer. Defaults to 0.999.\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.optimizer = optimizer  # Optimizer for all layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.beta_1 = beta_1  # Optimizer parameters\n",
    "        self.beta_2 = beta_2  # Optimizer parameters\n",
    "\n",
    "        # Weights initializing:\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i]._weightInit(self.layers[i - 1].units)\n",
    "            self.layers[i]._setOptimizer(self.optimizer, self.beta_1, self.beta_2)\n",
    "            # Initialize weights for each layer and set the optimizer\n",
    "\n",
    "\n",
    "    def lossFunction(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the loss between the true values and predicted values.\n",
    "\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True values.\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 0.5 * np.mean(np.linalg.norm(y_pred - y_true, axis=1)**2)\n",
    "\n",
    "        # Can be added\n",
    "\n",
    "    def _lossFunctionDerivative(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 1 / len(y_pred) * (y_pred - y_true)\n",
    "\n",
    "        # Can be added\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the neural network on the given input-output pairs.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "            y (numpy.ndarray): Output data.\n",
    "\n",
    "        Notes:\n",
    "            - Reshape y to a column vector (shape: (n_samples, output_size)).\n",
    "        \"\"\"\n",
    "        batch_separation = [(i, i + self.batch_size) for i in range(0, len(X), self.batch_size)] # Get batch indices\n",
    "        epoch_len = len(batch_separation)\n",
    "\n",
    "        indeces = np.arange(len(X))\n",
    "\n",
    "        for _ in range(self.epochs):    \n",
    "            np.random.shuffle(indeces) # Shuffle the training data\n",
    "\n",
    "            for iter, (i, j) in enumerate(batch_separation):\n",
    "                X_ = X[indeces[i:j]] # Get current batch\n",
    "                y_ = y[indeces[i:j]] # Get current batch\n",
    "\n",
    "                pred = self.forward(X_)\n",
    "\n",
    "                if self.verbose:\n",
    "                    process_percent = int(iter / epoch_len * 10)\n",
    "                    print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter}/{epoch_len}: [{process_percent * '=' + '>' + (10 - process_percent) * '-'}] - loss: {self.lossFunction(y_, pred)}\",end='')\n",
    "                \n",
    "                self.backward(pred, y_)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter + 1}/{epoch_len}: [{11 * '='}] - loss: {self.lossFunction(y_, pred)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform predictions using the trained neural network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted output data.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        X_ = np.copy(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            X_ = layer.call(X_)\n",
    "        return X_\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to update the weights of the network.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = self._lossFunctionDerivative(y_pred, y_true)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer._setGrad(gradient)\n",
    "            layer._updateGrad(self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/1000; Batch 7/7: [===========] - loss: 13373.464840976092\n",
      " Epoch 2/1000; Batch 7/7: [===========] - loss: 16619.113060004704\n",
      " Epoch 3/1000; Batch 7/7: [===========] - loss: 13596.696355065718\n",
      " Epoch 4/1000; Batch 7/7: [===========] - loss: 17494.930310572377\n",
      " Epoch 5/1000; Batch 7/7: [===========] - loss: 14849.877757803102\n",
      " Epoch 6/1000; Batch 7/7: [===========] - loss: 13316.429436146967\n",
      " Epoch 7/1000; Batch 7/7: [===========] - loss: 13427.783900786219\n",
      " Epoch 8/1000; Batch 7/7: [===========] - loss: 16594.467508498507\n",
      " Epoch 9/1000; Batch 7/7: [===========] - loss: 16965.865920812645\n",
      " Epoch 10/1000; Batch 7/7: [===========] - loss: 13035.433604769989\n",
      " Epoch 11/1000; Batch 7/7: [===========] - loss: 15455.607941863932\n",
      " Epoch 12/1000; Batch 7/7: [===========] - loss: 13172.087901382014\n",
      " Epoch 13/1000; Batch 7/7: [===========] - loss: 16418.730907019867\n",
      " Epoch 14/1000; Batch 7/7: [===========] - loss: 15595.564813603944\n",
      " Epoch 15/1000; Batch 7/7: [===========] - loss: 15197.671029546067\n",
      " Epoch 16/1000; Batch 7/7: [===========] - loss: 14436.347753588825\n",
      " Epoch 17/1000; Batch 7/7: [===========] - loss: 14358.922340236966\n",
      " Epoch 18/1000; Batch 7/7: [===========] - loss: 12616.700776776603\n",
      " Epoch 19/1000; Batch 7/7: [===========] - loss: 14497.165154687369\n",
      " Epoch 20/1000; Batch 7/7: [===========] - loss: 14794.555391663884\n",
      " Epoch 21/1000; Batch 7/7: [===========] - loss: 14067.517100351553\n",
      " Epoch 22/1000; Batch 7/7: [===========] - loss: 15191.441093227455\n",
      " Epoch 23/1000; Batch 7/7: [===========] - loss: 12315.043104804876\n",
      " Epoch 24/1000; Batch 7/7: [===========] - loss: 13303.698522997081\n",
      " Epoch 25/1000; Batch 7/7: [===========] - loss: 12762.081192221727\n",
      " Epoch 26/1000; Batch 7/7: [===========] - loss: 13677.393388082095\n",
      " Epoch 27/1000; Batch 7/7: [===========] - loss: 11942.465263123737\n",
      " Epoch 28/1000; Batch 7/7: [===========] - loss: 12962.710928757015\n",
      " Epoch 29/1000; Batch 7/7: [===========] - loss: 16404.132202452412\n",
      " Epoch 30/1000; Batch 7/7: [===========] - loss: 14000.758752823767\n",
      " Epoch 31/1000; Batch 7/7: [===========] - loss: 11632.720586095318\n",
      " Epoch 32/1000; Batch 7/7: [===========] - loss: 10170.479187030288\n",
      " Epoch 33/1000; Batch 7/7: [===========] - loss: 11441.169632208863\n",
      " Epoch 34/1000; Batch 7/7: [===========] - loss: 11044.647521407425\n",
      " Epoch 35/1000; Batch 7/7: [===========] - loss: 10313.881563753535\n",
      " Epoch 36/1000; Batch 7/7: [===========] - loss: 11562.472911976049\n",
      " Epoch 37/1000; Batch 7/7: [===========] - loss: 12561.232665444868\n",
      " Epoch 38/1000; Batch 7/7: [===========] - loss: 12938.461885219676\n",
      " Epoch 39/1000; Batch 7/7: [===========] - loss: 11075.298345953796\n",
      " Epoch 40/1000; Batch 7/7: [===========] - loss: 11933.348437084776\n",
      " Epoch 41/1000; Batch 7/7: [===========] - loss: 10659.034908221123\n",
      " Epoch 42/1000; Batch 7/7: [===========] - loss: 9204.8206938657825\n",
      " Epoch 43/1000; Batch 7/7: [===========] - loss: 9961.8611315704996\n",
      " Epoch 44/1000; Batch 7/7: [===========] - loss: 9995.479672566476\n",
      " Epoch 45/1000; Batch 7/7: [===========] - loss: 9062.5948897240831\n",
      " Epoch 46/1000; Batch 7/7: [===========] - loss: 10532.271347093241\n",
      " Epoch 47/1000; Batch 7/7: [===========] - loss: 6380.881163216282\n",
      " Epoch 48/1000; Batch 7/7: [===========] - loss: 6293.8312738753649\n",
      " Epoch 49/1000; Batch 7/7: [===========] - loss: 7841.655523494505\n",
      " Epoch 50/1000; Batch 7/7: [===========] - loss: 8593.378584240158\n",
      " Epoch 51/1000; Batch 7/7: [===========] - loss: 6344.3886545097255\n",
      " Epoch 52/1000; Batch 7/7: [===========] - loss: 7086.927272102625\n",
      " Epoch 53/1000; Batch 7/7: [===========] - loss: 5696.566951833529\n",
      " Epoch 54/1000; Batch 7/7: [===========] - loss: 6318.8353655124865\n",
      " Epoch 55/1000; Batch 7/7: [===========] - loss: 5717.982172035444\n",
      " Epoch 56/1000; Batch 7/7: [===========] - loss: 6060.9883664904765\n",
      " Epoch 57/1000; Batch 7/7: [===========] - loss: 4827.9310074603455\n",
      " Epoch 58/1000; Batch 7/7: [===========] - loss: 2961.9242419909545\n",
      " Epoch 59/1000; Batch 7/7: [===========] - loss: 3113.4112913634176\n",
      " Epoch 60/1000; Batch 7/7: [===========] - loss: 4063.9451142567846\n",
      " Epoch 61/1000; Batch 7/7: [===========] - loss: 4317.1793605824235\n",
      " Epoch 62/1000; Batch 7/7: [===========] - loss: 4899.5628999129685\n",
      " Epoch 63/1000; Batch 7/7: [===========] - loss: 2961.1151709550963\n",
      " Epoch 64/1000; Batch 7/7: [===========] - loss: 3088.1709518696944\n",
      " Epoch 65/1000; Batch 7/7: [===========] - loss: 2787.8289950274136\n",
      " Epoch 66/1000; Batch 7/7: [===========] - loss: 2937.7965124671957\n",
      " Epoch 67/1000; Batch 7/7: [===========] - loss: 2931.3202836037666\n",
      " Epoch 68/1000; Batch 7/7: [===========] - loss: 2509.3226263547062\n",
      " Epoch 69/1000; Batch 7/7: [===========] - loss: 2118.1135992243794\n",
      " Epoch 70/1000; Batch 7/7: [===========] - loss: 2439.7720640897146\n",
      " Epoch 71/1000; Batch 7/7: [===========] - loss: 1998.1574552365478\n",
      " Epoch 72/1000; Batch 7/7: [===========] - loss: 1933.5369857796234\n",
      " Epoch 73/1000; Batch 7/7: [===========] - loss: 1683.7799575146314\n",
      " Epoch 74/1000; Batch 7/7: [===========] - loss: 2146.6892523215224\n",
      " Epoch 75/1000; Batch 7/7: [===========] - loss: 1773.1598139043585\n",
      " Epoch 76/1000; Batch 7/7: [===========] - loss: 1944.3246283310243\n",
      " Epoch 77/1000; Batch 7/7: [===========] - loss: 1954.0700221392128\n",
      " Epoch 78/1000; Batch 7/7: [===========] - loss: 2272.0066608970337\n",
      " Epoch 79/1000; Batch 7/7: [===========] - loss: 1813.1812248549327\n",
      " Epoch 80/1000; Batch 7/7: [===========] - loss: 1391.9656625406699\n",
      " Epoch 81/1000; Batch 7/7: [===========] - loss: 1732.2541564029789\n",
      " Epoch 82/1000; Batch 7/7: [===========] - loss: 1822.7825620321216\n",
      " Epoch 83/1000; Batch 7/7: [===========] - loss: 1484.7792778634107\n",
      " Epoch 84/1000; Batch 7/7: [===========] - loss: 1798.9415299024445\n",
      " Epoch 85/1000; Batch 7/7: [===========] - loss: 1964.0535988179267\n",
      " Epoch 86/1000; Batch 7/7: [===========] - loss: 1930.8431190843933\n",
      " Epoch 87/1000; Batch 7/7: [===========] - loss: 1940.8943797160884\n",
      " Epoch 88/1000; Batch 7/7: [===========] - loss: 1525.0860496590171\n",
      " Epoch 89/1000; Batch 7/7: [===========] - loss: 1727.0589858234353\n",
      " Epoch 90/1000; Batch 7/7: [===========] - loss: 1611.0573937424401\n",
      " Epoch 91/1000; Batch 7/7: [===========] - loss: 1692.9919968332504\n",
      " Epoch 92/1000; Batch 7/7: [===========] - loss: 1667.5421242115185\n",
      " Epoch 93/1000; Batch 7/7: [===========] - loss: 1870.5763074967101\n",
      " Epoch 94/1000; Batch 7/7: [===========] - loss: 1451.9023998127573\n",
      " Epoch 95/1000; Batch 7/7: [===========] - loss: 2018.5262955896822\n",
      " Epoch 96/1000; Batch 7/7: [===========] - loss: 1689.4558541651836\n",
      " Epoch 97/1000; Batch 7/7: [===========] - loss: 1427.9591536048572\n",
      " Epoch 98/1000; Batch 7/7: [===========] - loss: 1376.2260388533437\n",
      " Epoch 99/1000; Batch 7/7: [===========] - loss: 1809.0245401651732\n",
      " Epoch 100/1000; Batch 7/7: [===========] - loss: 1586.2522467603003\n",
      " Epoch 101/1000; Batch 7/7: [===========] - loss: 1898.3613754928856\n",
      " Epoch 102/1000; Batch 7/7: [===========] - loss: 1505.2571315748412\n",
      " Epoch 103/1000; Batch 7/7: [===========] - loss: 1515.5384613820047\n",
      " Epoch 104/1000; Batch 7/7: [===========] - loss: 1980.5829637381312\n",
      " Epoch 105/1000; Batch 7/7: [===========] - loss: 1921.6279542367392\n",
      " Epoch 106/1000; Batch 7/7: [===========] - loss: 1759.4069890570493\n",
      " Epoch 107/1000; Batch 7/7: [===========] - loss: 1697.4442177041349\n",
      " Epoch 108/1000; Batch 7/7: [===========] - loss: 2121.8256955046263\n",
      " Epoch 109/1000; Batch 7/7: [===========] - loss: 1926.3455204942898\n",
      " Epoch 110/1000; Batch 7/7: [===========] - loss: 2037.4312381948298\n",
      " Epoch 111/1000; Batch 7/7: [===========] - loss: 1660.5714971336824\n",
      " Epoch 112/1000; Batch 7/7: [===========] - loss: 1534.1879088411342\n",
      " Epoch 113/1000; Batch 7/7: [===========] - loss: 1703.2285042740582\n",
      " Epoch 114/1000; Batch 7/7: [===========] - loss: 1398.8219705458564\n",
      " Epoch 115/1000; Batch 7/7: [===========] - loss: 1704.9726223964099\n",
      " Epoch 116/1000; Batch 7/7: [===========] - loss: 1541.3431171609548\n",
      " Epoch 117/1000; Batch 7/7: [===========] - loss: 2173.2092905020145\n",
      " Epoch 118/1000; Batch 7/7: [===========] - loss: 1643.0178789158049\n",
      " Epoch 119/1000; Batch 7/7: [===========] - loss: 1593.2549497350658\n",
      " Epoch 120/1000; Batch 7/7: [===========] - loss: 1609.7343694585438\n",
      " Epoch 121/1000; Batch 7/7: [===========] - loss: 1588.7045152229905\n",
      " Epoch 122/1000; Batch 7/7: [===========] - loss: 1394.4074547943922\n",
      " Epoch 123/1000; Batch 7/7: [===========] - loss: 1384.5288200499572\n",
      " Epoch 124/1000; Batch 7/7: [===========] - loss: 1897.6337329480373\n",
      " Epoch 125/1000; Batch 7/7: [===========] - loss: 1835.0512287157837\n",
      " Epoch 126/1000; Batch 7/7: [===========] - loss: 1769.6203859506868\n",
      " Epoch 127/1000; Batch 7/7: [===========] - loss: 2256.1590387859515\n",
      " Epoch 128/1000; Batch 7/7: [===========] - loss: 1395.1045839373223\n",
      " Epoch 129/1000; Batch 7/7: [===========] - loss: 1746.0371082362026\n",
      " Epoch 130/1000; Batch 7/7: [===========] - loss: 1652.6036793505957\n",
      " Epoch 131/1000; Batch 7/7: [===========] - loss: 1870.8023456687224\n",
      " Epoch 132/1000; Batch 7/7: [===========] - loss: 1459.8977519874393\n",
      " Epoch 133/1000; Batch 7/7: [===========] - loss: 1716.9932068515497\n",
      " Epoch 134/1000; Batch 7/7: [===========] - loss: 2008.9629507101306\n",
      " Epoch 135/1000; Batch 7/7: [===========] - loss: 1722.3829037618866\n",
      " Epoch 136/1000; Batch 7/7: [===========] - loss: 1427.5344972088565\n",
      " Epoch 137/1000; Batch 7/7: [===========] - loss: 1387.9368924486253\n",
      " Epoch 138/1000; Batch 7/7: [===========] - loss: 1900.4776318662412\n",
      " Epoch 139/1000; Batch 7/7: [===========] - loss: 1737.2348751197894\n",
      " Epoch 140/1000; Batch 7/7: [===========] - loss: 1541.0706137170532\n",
      " Epoch 141/1000; Batch 7/7: [===========] - loss: 1526.1745438950826\n",
      " Epoch 142/1000; Batch 7/7: [===========] - loss: 1775.6079782468692\n",
      " Epoch 143/1000; Batch 7/7: [===========] - loss: 1545.8760125790516\n",
      " Epoch 144/1000; Batch 7/7: [===========] - loss: 1509.7055584395234\n",
      " Epoch 145/1000; Batch 7/7: [===========] - loss: 1366.9433441609684\n",
      " Epoch 146/1000; Batch 7/7: [===========] - loss: 1572.4410520921344\n",
      " Epoch 147/1000; Batch 7/7: [===========] - loss: 1774.6400235230133\n",
      " Epoch 148/1000; Batch 7/7: [===========] - loss: 1469.8907411968425\n",
      " Epoch 149/1000; Batch 7/7: [===========] - loss: 1256.8915631127165\n",
      " Epoch 150/1000; Batch 7/7: [===========] - loss: 1149.3341246578284\n",
      " Epoch 151/1000; Batch 7/7: [===========] - loss: 1781.2984268169841\n",
      " Epoch 152/1000; Batch 7/7: [===========] - loss: 1712.0167419276333\n",
      " Epoch 153/1000; Batch 7/7: [===========] - loss: 1216.6422205261986\n",
      " Epoch 154/1000; Batch 7/7: [===========] - loss: 1394.3849999837491\n",
      " Epoch 155/1000; Batch 7/7: [===========] - loss: 1888.7539686035258\n",
      " Epoch 156/1000; Batch 7/7: [===========] - loss: 2072.8900450135815\n",
      " Epoch 157/1000; Batch 7/7: [===========] - loss: 1294.1541813972524\n",
      " Epoch 158/1000; Batch 7/7: [===========] - loss: 1905.9256000593657\n",
      " Epoch 159/1000; Batch 7/7: [===========] - loss: 1426.6060995435078\n",
      " Epoch 160/1000; Batch 7/7: [===========] - loss: 1513.5477177920818\n",
      " Epoch 161/1000; Batch 7/7: [===========] - loss: 1486.4465088116035\n",
      " Epoch 162/1000; Batch 7/7: [===========] - loss: 1251.5145489821327\n",
      " Epoch 163/1000; Batch 7/7: [===========] - loss: 1593.1331626455417\n",
      " Epoch 164/1000; Batch 7/7: [===========] - loss: 1316.7702797421064\n",
      " Epoch 165/1000; Batch 7/7: [===========] - loss: 1228.6551478757498\n",
      " Epoch 166/1000; Batch 7/7: [===========] - loss: 1714.7379319009453\n",
      " Epoch 167/1000; Batch 7/7: [===========] - loss: 1537.3990454943372\n",
      " Epoch 168/1000; Batch 7/7: [===========] - loss: 1545.2669590857493\n",
      " Epoch 169/1000; Batch 7/7: [===========] - loss: 1234.8018981774167\n",
      " Epoch 170/1000; Batch 7/7: [===========] - loss: 1427.3574978790373\n",
      " Epoch 171/1000; Batch 7/7: [===========] - loss: 1356.6521627490467\n",
      " Epoch 172/1000; Batch 7/7: [===========] - loss: 1996.8458144643769\n",
      " Epoch 173/1000; Batch 7/7: [===========] - loss: 1390.4201755834133\n",
      " Epoch 174/1000; Batch 7/7: [===========] - loss: 1435.4423364959825\n",
      " Epoch 175/1000; Batch 7/7: [===========] - loss: 1478.3071936665615\n",
      " Epoch 176/1000; Batch 7/7: [===========] - loss: 1489.0363877786267\n",
      " Epoch 177/1000; Batch 7/7: [===========] - loss: 1651.3780395230488\n",
      " Epoch 178/1000; Batch 7/7: [===========] - loss: 1503.8386557557676\n",
      " Epoch 179/1000; Batch 7/7: [===========] - loss: 1570.4335867331924\n",
      " Epoch 180/1000; Batch 7/7: [===========] - loss: 1703.7796131436248\n",
      " Epoch 181/1000; Batch 7/7: [===========] - loss: 1300.6070833951342\n",
      " Epoch 182/1000; Batch 7/7: [===========] - loss: 1363.3084960586382\n",
      " Epoch 183/1000; Batch 7/7: [===========] - loss: 1598.6488828336348\n",
      " Epoch 184/1000; Batch 7/7: [===========] - loss: 1127.5270533021248\n",
      " Epoch 185/1000; Batch 7/7: [===========] - loss: 1123.7317294640666\n",
      " Epoch 186/1000; Batch 7/7: [===========] - loss: 1421.4100974057567\n",
      " Epoch 187/1000; Batch 7/7: [===========] - loss: 1735.2410644361587\n",
      " Epoch 188/1000; Batch 7/7: [===========] - loss: 1434.8384671222111\n",
      " Epoch 189/1000; Batch 7/7: [===========] - loss: 1363.3152502984417\n",
      " Epoch 190/1000; Batch 7/7: [===========] - loss: 1275.2900131002239\n",
      " Epoch 191/1000; Batch 7/7: [===========] - loss: 1439.5852374967646\n",
      " Epoch 192/1000; Batch 7/7: [===========] - loss: 1682.1278610322827\n",
      " Epoch 193/1000; Batch 7/7: [===========] - loss: 1353.6278071423556\n",
      " Epoch 194/1000; Batch 7/7: [===========] - loss: 1776.7601816718789\n",
      " Epoch 195/1000; Batch 7/7: [===========] - loss: 1269.3463105315611\n",
      " Epoch 196/1000; Batch 7/7: [===========] - loss: 1663.4762533061898\n",
      " Epoch 197/1000; Batch 7/7: [===========] - loss: 1607.5577595494017\n",
      " Epoch 198/1000; Batch 7/7: [===========] - loss: 1510.8856075735378\n",
      " Epoch 199/1000; Batch 7/7: [===========] - loss: 1531.9711946812008\n",
      " Epoch 200/1000; Batch 7/7: [===========] - loss: 1596.6079353879973\n",
      " Epoch 201/1000; Batch 7/7: [===========] - loss: 1816.4204614023538\n",
      " Epoch 202/1000; Batch 7/7: [===========] - loss: 1257.5500775967777\n",
      " Epoch 203/1000; Batch 7/7: [===========] - loss: 1701.7008360654531\n",
      " Epoch 204/1000; Batch 7/7: [===========] - loss: 1475.5082425713474\n",
      " Epoch 205/1000; Batch 7/7: [===========] - loss: 993.93789408115965\n",
      " Epoch 206/1000; Batch 7/7: [===========] - loss: 1672.8855060215858\n",
      " Epoch 207/1000; Batch 7/7: [===========] - loss: 1422.5077370169342\n",
      " Epoch 208/1000; Batch 7/7: [===========] - loss: 1138.5634873073896\n",
      " Epoch 209/1000; Batch 7/7: [===========] - loss: 1396.6316277011342\n",
      " Epoch 210/1000; Batch 7/7: [===========] - loss: 1352.3348182592089\n",
      " Epoch 211/1000; Batch 7/7: [===========] - loss: 1628.7241537071125\n",
      " Epoch 212/1000; Batch 7/7: [===========] - loss: 1326.0144847288382\n",
      " Epoch 213/1000; Batch 7/7: [===========] - loss: 1204.7188451591805\n",
      " Epoch 214/1000; Batch 7/7: [===========] - loss: 1429.8956358722176\n",
      " Epoch 215/1000; Batch 7/7: [===========] - loss: 1102.8048701548434\n",
      " Epoch 216/1000; Batch 7/7: [===========] - loss: 1216.0118723632388\n",
      " Epoch 217/1000; Batch 7/7: [===========] - loss: 1274.9787375279986\n",
      " Epoch 218/1000; Batch 7/7: [===========] - loss: 1148.1681188673243\n",
      " Epoch 219/1000; Batch 7/7: [===========] - loss: 1073.6093517511777\n",
      " Epoch 220/1000; Batch 7/7: [===========] - loss: 1719.7493939577193\n",
      " Epoch 221/1000; Batch 7/7: [===========] - loss: 1680.6609703805271\n",
      " Epoch 222/1000; Batch 7/7: [===========] - loss: 1227.8992364573035\n",
      " Epoch 223/1000; Batch 7/7: [===========] - loss: 1265.6325947752937\n",
      " Epoch 224/1000; Batch 7/7: [===========] - loss: 1561.7486469944674\n",
      " Epoch 225/1000; Batch 7/7: [===========] - loss: 1710.1808254421753\n",
      " Epoch 226/1000; Batch 7/7: [===========] - loss: 1421.3231364702958\n",
      " Epoch 227/1000; Batch 7/7: [===========] - loss: 1364.3148323327318\n",
      " Epoch 228/1000; Batch 7/7: [===========] - loss: 1619.9580162489149\n",
      " Epoch 229/1000; Batch 7/7: [===========] - loss: 1866.9887494946934\n",
      " Epoch 230/1000; Batch 7/7: [===========] - loss: 1178.1638546835845\n",
      " Epoch 231/1000; Batch 7/7: [===========] - loss: 1401.3435285631344\n",
      " Epoch 232/1000; Batch 7/7: [===========] - loss: 1649.0742242522174\n",
      " Epoch 233/1000; Batch 7/7: [===========] - loss: 1522.7979693198617\n",
      " Epoch 234/1000; Batch 7/7: [===========] - loss: 1373.5002463920596\n",
      " Epoch 235/1000; Batch 7/7: [===========] - loss: 1582.2629338077804\n",
      " Epoch 236/1000; Batch 7/7: [===========] - loss: 1514.8226105104543\n",
      " Epoch 237/1000; Batch 7/7: [===========] - loss: 1178.8069665659152\n",
      " Epoch 238/1000; Batch 7/7: [===========] - loss: 1449.5289665843316\n",
      " Epoch 239/1000; Batch 7/7: [===========] - loss: 1549.1972857736769\n",
      " Epoch 240/1000; Batch 7/7: [===========] - loss: 1455.1256100429955\n",
      " Epoch 241/1000; Batch 7/7: [===========] - loss: 1654.9668161737625\n",
      " Epoch 242/1000; Batch 7/7: [===========] - loss: 1308.8029988109479\n",
      " Epoch 243/1000; Batch 7/7: [===========] - loss: 1537.4377804266883\n",
      " Epoch 244/1000; Batch 7/7: [===========] - loss: 1718.1353359860673\n",
      " Epoch 245/1000; Batch 7/7: [===========] - loss: 1584.6941150981818\n",
      " Epoch 246/1000; Batch 7/7: [===========] - loss: 1431.5729902703094\n",
      " Epoch 247/1000; Batch 7/7: [===========] - loss: 1434.9081777884298\n",
      " Epoch 248/1000; Batch 7/7: [===========] - loss: 1632.3342721255235\n",
      " Epoch 249/1000; Batch 7/7: [===========] - loss: 1354.6360663355645\n",
      " Epoch 250/1000; Batch 7/7: [===========] - loss: 1509.1988401285052\n",
      " Epoch 251/1000; Batch 7/7: [===========] - loss: 2014.7527669385402\n",
      " Epoch 252/1000; Batch 7/7: [===========] - loss: 1541.1399742238966\n",
      " Epoch 253/1000; Batch 7/7: [===========] - loss: 1698.2854234415277\n",
      " Epoch 254/1000; Batch 7/7: [===========] - loss: 1684.2067247173017\n",
      " Epoch 255/1000; Batch 7/7: [===========] - loss: 1262.0940186169487\n",
      " Epoch 256/1000; Batch 7/7: [===========] - loss: 1305.7333061941506\n",
      " Epoch 257/1000; Batch 7/7: [===========] - loss: 1434.4680678747172\n",
      " Epoch 258/1000; Batch 7/7: [===========] - loss: 1161.6699908188361\n",
      " Epoch 259/1000; Batch 7/7: [===========] - loss: 1362.6885990490934\n",
      " Epoch 260/1000; Batch 7/7: [===========] - loss: 1351.9411945037327\n",
      " Epoch 261/1000; Batch 7/7: [===========] - loss: 1400.9699405801903\n",
      " Epoch 262/1000; Batch 7/7: [===========] - loss: 1370.2649370824558\n",
      " Epoch 263/1000; Batch 7/7: [===========] - loss: 1617.6098115659383\n",
      " Epoch 264/1000; Batch 7/7: [===========] - loss: 873.32083587002736\n",
      " Epoch 265/1000; Batch 7/7: [===========] - loss: 1386.7038638002637\n",
      " Epoch 266/1000; Batch 7/7: [===========] - loss: 1147.1439396415214\n",
      " Epoch 267/1000; Batch 7/7: [===========] - loss: 1305.6447349545488\n",
      " Epoch 268/1000; Batch 7/7: [===========] - loss: 1365.4107656544702\n",
      " Epoch 269/1000; Batch 7/7: [===========] - loss: 1292.5343995218288\n",
      " Epoch 270/1000; Batch 7/7: [===========] - loss: 971.13108215197298\n",
      " Epoch 271/1000; Batch 7/7: [===========] - loss: 1563.5465925634038\n",
      " Epoch 272/1000; Batch 7/7: [===========] - loss: 1477.7539104004363\n",
      " Epoch 273/1000; Batch 7/7: [===========] - loss: 1871.2483067965472\n",
      " Epoch 274/1000; Batch 7/7: [===========] - loss: 1340.9751960762688\n",
      " Epoch 275/1000; Batch 7/7: [===========] - loss: 1606.3411192258297\n",
      " Epoch 276/1000; Batch 7/7: [===========] - loss: 1573.1468755882843\n",
      " Epoch 277/1000; Batch 7/7: [===========] - loss: 1288.2626026372347\n",
      " Epoch 278/1000; Batch 7/7: [===========] - loss: 1141.6933040890732\n",
      " Epoch 279/1000; Batch 7/7: [===========] - loss: 1572.9861254254663\n",
      " Epoch 280/1000; Batch 7/7: [===========] - loss: 1827.9121555723732\n",
      " Epoch 281/1000; Batch 7/7: [===========] - loss: 1352.0104116851414\n",
      " Epoch 282/1000; Batch 7/7: [===========] - loss: 1538.6705448853409\n",
      " Epoch 283/1000; Batch 7/7: [===========] - loss: 1824.7465359521038\n",
      " Epoch 284/1000; Batch 7/7: [===========] - loss: 1283.0569708511546\n",
      " Epoch 285/1000; Batch 7/7: [===========] - loss: 1064.5111164715015\n",
      " Epoch 286/1000; Batch 7/7: [===========] - loss: 1613.2205226099475\n",
      " Epoch 287/1000; Batch 7/7: [===========] - loss: 1401.3488562115508\n",
      " Epoch 288/1000; Batch 7/7: [===========] - loss: 1250.8521132548544\n",
      " Epoch 289/1000; Batch 7/7: [===========] - loss: 1549.2344626835502\n",
      " Epoch 290/1000; Batch 7/7: [===========] - loss: 1382.5707118899988\n",
      " Epoch 291/1000; Batch 7/7: [===========] - loss: 1461.5400014832219\n",
      " Epoch 292/1000; Batch 7/7: [===========] - loss: 1530.9219834295445\n",
      " Epoch 293/1000; Batch 7/7: [===========] - loss: 1668.0817185915093\n",
      " Epoch 294/1000; Batch 7/7: [===========] - loss: 1406.4064129229226\n",
      " Epoch 295/1000; Batch 7/7: [===========] - loss: 1268.0893787907316\n",
      " Epoch 296/1000; Batch 7/7: [===========] - loss: 1506.3488937195411\n",
      " Epoch 297/1000; Batch 7/7: [===========] - loss: 1594.0231631510376\n",
      " Epoch 298/1000; Batch 7/7: [===========] - loss: 1185.3728292091942\n",
      " Epoch 299/1000; Batch 7/7: [===========] - loss: 1446.4745027247848\n",
      " Epoch 300/1000; Batch 7/7: [===========] - loss: 1205.0788251294782\n",
      " Epoch 301/1000; Batch 7/7: [===========] - loss: 1721.1708240531012\n",
      " Epoch 302/1000; Batch 7/7: [===========] - loss: 1566.0466863594943\n",
      " Epoch 303/1000; Batch 7/7: [===========] - loss: 1579.5465728392744\n",
      " Epoch 304/1000; Batch 7/7: [===========] - loss: 1836.4380468610987\n",
      " Epoch 305/1000; Batch 7/7: [===========] - loss: 1162.5290985704848\n",
      " Epoch 306/1000; Batch 7/7: [===========] - loss: 1316.7923380606098\n",
      " Epoch 307/1000; Batch 7/7: [===========] - loss: 1361.9928852110327\n",
      " Epoch 308/1000; Batch 7/7: [===========] - loss: 1054.5461996139245\n",
      " Epoch 309/1000; Batch 7/7: [===========] - loss: 1798.0861869270143\n",
      " Epoch 310/1000; Batch 7/7: [===========] - loss: 1444.4808978804875\n",
      " Epoch 311/1000; Batch 7/7: [===========] - loss: 1547.0380975766564\n",
      " Epoch 312/1000; Batch 7/7: [===========] - loss: 1310.2315702634703\n",
      " Epoch 313/1000; Batch 7/7: [===========] - loss: 1532.5350854650296\n",
      " Epoch 314/1000; Batch 7/7: [===========] - loss: 1438.9320545652166\n",
      " Epoch 315/1000; Batch 7/7: [===========] - loss: 1587.7219565020625\n",
      " Epoch 316/1000; Batch 7/7: [===========] - loss: 1381.5794877177373\n",
      " Epoch 317/1000; Batch 7/7: [===========] - loss: 1360.4408019385946\n",
      " Epoch 318/1000; Batch 7/7: [===========] - loss: 1124.1246182511327\n",
      " Epoch 319/1000; Batch 7/7: [===========] - loss: 2126.9295461167717\n",
      " Epoch 320/1000; Batch 7/7: [===========] - loss: 1654.2704385594197\n",
      " Epoch 321/1000; Batch 7/7: [===========] - loss: 1758.6174299060754\n",
      " Epoch 322/1000; Batch 7/7: [===========] - loss: 1825.8806538086275\n",
      " Epoch 323/1000; Batch 7/7: [===========] - loss: 1132.0646187535735\n",
      " Epoch 324/1000; Batch 7/7: [===========] - loss: 1445.4791016782433\n",
      " Epoch 325/1000; Batch 7/7: [===========] - loss: 1932.1043525009804\n",
      " Epoch 326/1000; Batch 7/7: [===========] - loss: 1026.4953463056631\n",
      " Epoch 327/1000; Batch 7/7: [===========] - loss: 1206.1630382494366\n",
      " Epoch 328/1000; Batch 7/7: [===========] - loss: 1325.4125934093279\n",
      " Epoch 329/1000; Batch 7/7: [===========] - loss: 1723.0122964448058\n",
      " Epoch 330/1000; Batch 7/7: [===========] - loss: 1355.2875960199475\n",
      " Epoch 331/1000; Batch 7/7: [===========] - loss: 1262.0026360159352\n",
      " Epoch 332/1000; Batch 7/7: [===========] - loss: 1683.6982364282746\n",
      " Epoch 333/1000; Batch 7/7: [===========] - loss: 1725.5377627960161\n",
      " Epoch 334/1000; Batch 7/7: [===========] - loss: 1565.0403336437863\n",
      " Epoch 335/1000; Batch 7/7: [===========] - loss: 1406.0201131031429\n",
      " Epoch 336/1000; Batch 7/7: [===========] - loss: 1540.1017777827064\n",
      " Epoch 337/1000; Batch 7/7: [===========] - loss: 1404.0326308186495\n",
      " Epoch 338/1000; Batch 7/7: [===========] - loss: 1215.7458831234152\n",
      " Epoch 339/1000; Batch 7/7: [===========] - loss: 937.59455143368757\n",
      " Epoch 340/1000; Batch 7/7: [===========] - loss: 1322.7954753614877\n",
      " Epoch 341/1000; Batch 7/7: [===========] - loss: 1150.8771501832238\n",
      " Epoch 342/1000; Batch 7/7: [===========] - loss: 1614.6109228085772\n",
      " Epoch 343/1000; Batch 7/7: [===========] - loss: 1648.8028431658627\n",
      " Epoch 344/1000; Batch 7/7: [===========] - loss: 1289.3514727933596\n",
      " Epoch 345/1000; Batch 7/7: [===========] - loss: 1841.3262975572811\n",
      " Epoch 346/1000; Batch 7/7: [===========] - loss: 1425.1817644291423\n",
      " Epoch 347/1000; Batch 7/7: [===========] - loss: 1256.4431159873955\n",
      " Epoch 348/1000; Batch 7/7: [===========] - loss: 1263.5725985346264\n",
      " Epoch 349/1000; Batch 7/7: [===========] - loss: 1289.0798596498887\n",
      " Epoch 350/1000; Batch 7/7: [===========] - loss: 1316.6980037890478\n",
      " Epoch 351/1000; Batch 7/7: [===========] - loss: 1394.1964943189257\n",
      " Epoch 352/1000; Batch 7/7: [===========] - loss: 1542.9414047773778\n",
      " Epoch 353/1000; Batch 7/7: [===========] - loss: 1171.2139855081635\n",
      " Epoch 354/1000; Batch 7/7: [===========] - loss: 1148.4783759548274\n",
      " Epoch 355/1000; Batch 7/7: [===========] - loss: 1406.4145382707736\n",
      " Epoch 356/1000; Batch 7/7: [===========] - loss: 1087.8447200674166\n",
      " Epoch 357/1000; Batch 7/7: [===========] - loss: 1310.6218534689633\n",
      " Epoch 358/1000; Batch 7/7: [===========] - loss: 1407.2947483802204\n",
      " Epoch 359/1000; Batch 7/7: [===========] - loss: 1070.6623195615844\n",
      " Epoch 360/1000; Batch 7/7: [===========] - loss: 1210.2271741611873\n",
      " Epoch 361/1000; Batch 7/7: [===========] - loss: 1076.2982380701426\n",
      " Epoch 362/1000; Batch 7/7: [===========] - loss: 1327.5388513404243\n",
      " Epoch 363/1000; Batch 7/7: [===========] - loss: 1377.4199999869081\n",
      " Epoch 364/1000; Batch 7/7: [===========] - loss: 1473.9008904116708\n",
      " Epoch 365/1000; Batch 7/7: [===========] - loss: 1656.2806329960163\n",
      " Epoch 366/1000; Batch 7/7: [===========] - loss: 1784.1893501834252\n",
      " Epoch 367/1000; Batch 7/7: [===========] - loss: 1581.3540107650474\n",
      " Epoch 368/1000; Batch 7/7: [===========] - loss: 1442.2945146150842\n",
      " Epoch 369/1000; Batch 7/7: [===========] - loss: 1192.1660803151503\n",
      " Epoch 370/1000; Batch 7/7: [===========] - loss: 1386.1209876231874\n",
      " Epoch 371/1000; Batch 7/7: [===========] - loss: 1083.1738852748615\n",
      " Epoch 372/1000; Batch 7/7: [===========] - loss: 1103.9676168652804\n",
      " Epoch 373/1000; Batch 7/7: [===========] - loss: 1626.7634575170882\n",
      " Epoch 374/1000; Batch 7/7: [===========] - loss: 1497.3215202924816\n",
      " Epoch 375/1000; Batch 7/7: [===========] - loss: 1309.4589062663786\n",
      " Epoch 376/1000; Batch 7/7: [===========] - loss: 1471.8510781752493\n",
      " Epoch 377/1000; Batch 7/7: [===========] - loss: 1740.3263436097789\n",
      " Epoch 378/1000; Batch 7/7: [===========] - loss: 1812.5415996028978\n",
      " Epoch 379/1000; Batch 7/7: [===========] - loss: 1639.0209526043911\n",
      " Epoch 380/1000; Batch 7/7: [===========] - loss: 1792.2467126007507\n",
      " Epoch 381/1000; Batch 7/7: [===========] - loss: 1461.0515767611598\n",
      " Epoch 382/1000; Batch 7/7: [===========] - loss: 1229.8793234133489\n",
      " Epoch 383/1000; Batch 7/7: [===========] - loss: 1872.3547381268365\n",
      " Epoch 384/1000; Batch 7/7: [===========] - loss: 1587.3259751307655\n",
      " Epoch 385/1000; Batch 7/7: [===========] - loss: 1418.8841621677684\n",
      " Epoch 386/1000; Batch 7/7: [===========] - loss: 1582.2995693826122\n",
      " Epoch 387/1000; Batch 7/7: [===========] - loss: 1365.3783271551549\n",
      " Epoch 388/1000; Batch 7/7: [===========] - loss: 1350.9250268014484\n",
      " Epoch 389/1000; Batch 7/7: [===========] - loss: 1300.3058950898394\n",
      " Epoch 390/1000; Batch 7/7: [===========] - loss: 1376.6044341733618\n",
      " Epoch 391/1000; Batch 7/7: [===========] - loss: 1284.4575096747799\n",
      " Epoch 392/1000; Batch 7/7: [===========] - loss: 1218.2496250110385\n",
      " Epoch 393/1000; Batch 7/7: [===========] - loss: 1455.2022492100336\n",
      " Epoch 394/1000; Batch 7/7: [===========] - loss: 1236.3294940623967\n",
      " Epoch 395/1000; Batch 7/7: [===========] - loss: 1462.3452493779944\n",
      " Epoch 396/1000; Batch 7/7: [===========] - loss: 1507.5781321561274\n",
      " Epoch 397/1000; Batch 7/7: [===========] - loss: 1247.1727871047874\n",
      " Epoch 398/1000; Batch 7/7: [===========] - loss: 1192.5950452222175\n",
      " Epoch 399/1000; Batch 7/7: [===========] - loss: 1365.8445945246026\n",
      " Epoch 400/1000; Batch 7/7: [===========] - loss: 1767.4976299091434\n",
      " Epoch 401/1000; Batch 7/7: [===========] - loss: 1495.0200664995532\n",
      " Epoch 402/1000; Batch 7/7: [===========] - loss: 1165.2372416286598\n",
      " Epoch 403/1000; Batch 7/7: [===========] - loss: 1476.8974499590193\n",
      " Epoch 404/1000; Batch 7/7: [===========] - loss: 1438.4285090987137\n",
      " Epoch 405/1000; Batch 7/7: [===========] - loss: 1206.2310945099524\n",
      " Epoch 406/1000; Batch 7/7: [===========] - loss: 1724.0508307970829\n",
      " Epoch 407/1000; Batch 7/7: [===========] - loss: 2013.0942999803966\n",
      " Epoch 408/1000; Batch 7/7: [===========] - loss: 1284.0354019426454\n",
      " Epoch 409/1000; Batch 7/7: [===========] - loss: 1309.0267640908166\n",
      " Epoch 410/1000; Batch 7/7: [===========] - loss: 1533.4442393272668\n",
      " Epoch 411/1000; Batch 7/7: [===========] - loss: 1243.1595863024943\n",
      " Epoch 412/1000; Batch 7/7: [===========] - loss: 1575.0863267107122\n",
      " Epoch 413/1000; Batch 7/7: [===========] - loss: 1319.4045240121313\n",
      " Epoch 414/1000; Batch 7/7: [===========] - loss: 1669.3883852819245\n",
      " Epoch 415/1000; Batch 7/7: [===========] - loss: 1395.7567661945616\n",
      " Epoch 416/1000; Batch 7/7: [===========] - loss: 1491.9752990095032\n",
      " Epoch 417/1000; Batch 7/7: [===========] - loss: 1788.3448268137947\n",
      " Epoch 418/1000; Batch 7/7: [===========] - loss: 1612.0515426331516\n",
      " Epoch 419/1000; Batch 7/7: [===========] - loss: 1740.3856847521224\n",
      " Epoch 420/1000; Batch 7/7: [===========] - loss: 1423.7779589679583\n",
      " Epoch 421/1000; Batch 7/7: [===========] - loss: 1489.2147178606135\n",
      " Epoch 422/1000; Batch 7/7: [===========] - loss: 1321.3812870590448\n",
      " Epoch 423/1000; Batch 7/7: [===========] - loss: 1407.6869914489143\n",
      " Epoch 424/1000; Batch 7/7: [===========] - loss: 1298.0936529383828\n",
      " Epoch 425/1000; Batch 7/7: [===========] - loss: 1509.3800643359675\n",
      " Epoch 426/1000; Batch 7/7: [===========] - loss: 1716.8446716994638\n",
      " Epoch 427/1000; Batch 7/7: [===========] - loss: 1130.2119369318586\n",
      " Epoch 428/1000; Batch 7/7: [===========] - loss: 1323.7910125950061\n",
      " Epoch 429/1000; Batch 7/7: [===========] - loss: 1671.0414185001032\n",
      " Epoch 430/1000; Batch 7/7: [===========] - loss: 1320.3020238460383\n",
      " Epoch 431/1000; Batch 7/7: [===========] - loss: 1805.9754706360459\n",
      " Epoch 432/1000; Batch 7/7: [===========] - loss: 1827.1340987027459\n",
      " Epoch 433/1000; Batch 7/7: [===========] - loss: 1859.6607705269396\n",
      " Epoch 434/1000; Batch 7/7: [===========] - loss: 1226.8089320967458\n",
      " Epoch 435/1000; Batch 7/7: [===========] - loss: 1097.6832018805399\n",
      " Epoch 436/1000; Batch 7/7: [===========] - loss: 938.51179518563657\n",
      " Epoch 437/1000; Batch 7/7: [===========] - loss: 1556.6120341216392\n",
      " Epoch 438/1000; Batch 7/7: [===========] - loss: 1374.4816215259866\n",
      " Epoch 439/1000; Batch 7/7: [===========] - loss: 1441.4606777093331\n",
      " Epoch 440/1000; Batch 7/7: [===========] - loss: 858.22734924230925\n",
      " Epoch 441/1000; Batch 7/7: [===========] - loss: 1020.7903867703341\n",
      " Epoch 442/1000; Batch 7/7: [===========] - loss: 1441.5033120236656\n",
      " Epoch 443/1000; Batch 7/7: [===========] - loss: 1556.0620451514037\n",
      " Epoch 444/1000; Batch 7/7: [===========] - loss: 1454.2044863689368\n",
      " Epoch 445/1000; Batch 7/7: [===========] - loss: 999.00790923795777\n",
      " Epoch 446/1000; Batch 7/7: [===========] - loss: 1307.6222502642584\n",
      " Epoch 447/1000; Batch 7/7: [===========] - loss: 1245.4215859930207\n",
      " Epoch 448/1000; Batch 7/7: [===========] - loss: 1658.0760241671885\n",
      " Epoch 449/1000; Batch 7/7: [===========] - loss: 1817.2162755748944\n",
      " Epoch 450/1000; Batch 7/7: [===========] - loss: 1199.4585895501853\n",
      " Epoch 451/1000; Batch 7/7: [===========] - loss: 1673.1515755697155\n",
      " Epoch 452/1000; Batch 7/7: [===========] - loss: 2055.9830421648135\n",
      " Epoch 453/1000; Batch 7/7: [===========] - loss: 1653.4473313469307\n",
      " Epoch 454/1000; Batch 7/7: [===========] - loss: 1286.4666597950538\n",
      " Epoch 455/1000; Batch 7/7: [===========] - loss: 1709.1738425014946\n",
      " Epoch 456/1000; Batch 7/7: [===========] - loss: 1141.4738594175403\n",
      " Epoch 457/1000; Batch 7/7: [===========] - loss: 1247.8347783852828\n",
      " Epoch 458/1000; Batch 7/7: [===========] - loss: 1425.7935373498224\n",
      " Epoch 459/1000; Batch 7/7: [===========] - loss: 1426.0489499327134\n",
      " Epoch 460/1000; Batch 7/7: [===========] - loss: 1104.2158364878085\n",
      " Epoch 461/1000; Batch 7/7: [===========] - loss: 1332.3612945559273\n",
      " Epoch 462/1000; Batch 7/7: [===========] - loss: 1328.0906031326974\n",
      " Epoch 463/1000; Batch 7/7: [===========] - loss: 1296.5731079432325\n",
      " Epoch 464/1000; Batch 7/7: [===========] - loss: 1218.8298396832036\n",
      " Epoch 465/1000; Batch 7/7: [===========] - loss: 1574.4628986047314\n",
      " Epoch 466/1000; Batch 7/7: [===========] - loss: 1346.7019014754084\n",
      " Epoch 467/1000; Batch 7/7: [===========] - loss: 1688.5611703052605\n",
      " Epoch 468/1000; Batch 7/7: [===========] - loss: 1204.6598559989927\n",
      " Epoch 469/1000; Batch 7/7: [===========] - loss: 1234.2114680346858\n",
      " Epoch 470/1000; Batch 7/7: [===========] - loss: 1319.8541491670137\n",
      " Epoch 471/1000; Batch 7/7: [===========] - loss: 1531.9134577547933\n",
      " Epoch 472/1000; Batch 7/7: [===========] - loss: 1166.0030672004664\n",
      " Epoch 473/1000; Batch 7/7: [===========] - loss: 1325.2036252427554\n",
      " Epoch 474/1000; Batch 7/7: [===========] - loss: 1315.6580459684576\n",
      " Epoch 475/1000; Batch 7/7: [===========] - loss: 1576.1278820162102\n",
      " Epoch 476/1000; Batch 7/7: [===========] - loss: 1280.9854734994717\n",
      " Epoch 477/1000; Batch 7/7: [===========] - loss: 1876.1629614293854\n",
      " Epoch 478/1000; Batch 7/7: [===========] - loss: 1417.2519952331343\n",
      " Epoch 479/1000; Batch 7/7: [===========] - loss: 1081.0398196101944\n",
      " Epoch 480/1000; Batch 7/7: [===========] - loss: 1152.3512058567944\n",
      " Epoch 481/1000; Batch 7/7: [===========] - loss: 1546.9568162217066\n",
      " Epoch 482/1000; Batch 7/7: [===========] - loss: 1708.6732642025145\n",
      " Epoch 483/1000; Batch 7/7: [===========] - loss: 1467.7484786542514\n",
      " Epoch 484/1000; Batch 7/7: [===========] - loss: 1436.3525570328662\n",
      " Epoch 485/1000; Batch 7/7: [===========] - loss: 1376.5154693264353\n",
      " Epoch 486/1000; Batch 7/7: [===========] - loss: 1746.9564530937478\n",
      " Epoch 487/1000; Batch 7/7: [===========] - loss: 1430.3065693040112\n",
      " Epoch 488/1000; Batch 7/7: [===========] - loss: 1374.8569543261792\n",
      " Epoch 489/1000; Batch 7/7: [===========] - loss: 1402.3738071388934\n",
      " Epoch 490/1000; Batch 7/7: [===========] - loss: 1267.6258140393716\n",
      " Epoch 491/1000; Batch 7/7: [===========] - loss: 1479.9279659078905\n",
      " Epoch 492/1000; Batch 7/7: [===========] - loss: 1587.1244690976885\n",
      " Epoch 493/1000; Batch 7/7: [===========] - loss: 1472.0067005906787\n",
      " Epoch 494/1000; Batch 7/7: [===========] - loss: 1343.9067702285156\n",
      " Epoch 495/1000; Batch 7/7: [===========] - loss: 1546.3101759429564\n",
      " Epoch 496/1000; Batch 7/7: [===========] - loss: 1418.5588693525015\n",
      " Epoch 497/1000; Batch 7/7: [===========] - loss: 1183.2737408842424\n",
      " Epoch 498/1000; Batch 7/7: [===========] - loss: 1379.8952197156747\n",
      " Epoch 499/1000; Batch 7/7: [===========] - loss: 1773.6581426896587\n",
      " Epoch 500/1000; Batch 7/7: [===========] - loss: 1276.3619966936585\n",
      " Epoch 501/1000; Batch 7/7: [===========] - loss: 1186.5866986505882\n",
      " Epoch 502/1000; Batch 7/7: [===========] - loss: 1451.2637824769479\n",
      " Epoch 503/1000; Batch 7/7: [===========] - loss: 1308.9884747252745\n",
      " Epoch 504/1000; Batch 7/7: [===========] - loss: 1742.7001952774197\n",
      " Epoch 505/1000; Batch 7/7: [===========] - loss: 1788.6007142281596\n",
      " Epoch 506/1000; Batch 7/7: [===========] - loss: 1626.4896350086854\n",
      " Epoch 507/1000; Batch 7/7: [===========] - loss: 1497.3377803668545\n",
      " Epoch 508/1000; Batch 7/7: [===========] - loss: 1336.2753966651396\n",
      " Epoch 509/1000; Batch 7/7: [===========] - loss: 1310.0198200535312\n",
      " Epoch 510/1000; Batch 7/7: [===========] - loss: 939.33200934633895\n",
      " Epoch 511/1000; Batch 7/7: [===========] - loss: 1157.9446922663935\n",
      " Epoch 512/1000; Batch 7/7: [===========] - loss: 1278.2970806540104\n",
      " Epoch 513/1000; Batch 7/7: [===========] - loss: 1285.9024874926574\n",
      " Epoch 514/1000; Batch 7/7: [===========] - loss: 1256.0306630027913\n",
      " Epoch 515/1000; Batch 7/7: [===========] - loss: 1468.0186200850228\n",
      " Epoch 516/1000; Batch 7/7: [===========] - loss: 1257.7767822751755\n",
      " Epoch 517/1000; Batch 7/7: [===========] - loss: 1350.0722797420003\n",
      " Epoch 518/1000; Batch 7/7: [===========] - loss: 1677.5308575254721\n",
      " Epoch 519/1000; Batch 7/7: [===========] - loss: 1649.4784864592048\n",
      " Epoch 520/1000; Batch 7/7: [===========] - loss: 1797.8583925444896\n",
      " Epoch 521/1000; Batch 7/7: [===========] - loss: 1615.2333635505217\n",
      " Epoch 522/1000; Batch 7/7: [===========] - loss: 1333.2970688453473\n",
      " Epoch 523/1000; Batch 7/7: [===========] - loss: 1749.8354461915796\n",
      " Epoch 524/1000; Batch 7/7: [===========] - loss: 1536.1010699481267\n",
      " Epoch 525/1000; Batch 7/7: [===========] - loss: 1681.0198838770468\n",
      " Epoch 526/1000; Batch 7/7: [===========] - loss: 1561.5117962371223\n",
      " Epoch 527/1000; Batch 7/7: [===========] - loss: 1362.0643883783216\n",
      " Epoch 528/1000; Batch 7/7: [===========] - loss: 1311.3755036807843\n",
      " Epoch 529/1000; Batch 7/7: [===========] - loss: 1617.2425405023653\n",
      " Epoch 530/1000; Batch 7/7: [===========] - loss: 1204.4581506577782\n",
      " Epoch 531/1000; Batch 7/7: [===========] - loss: 1202.8030704676642\n",
      " Epoch 532/1000; Batch 7/7: [===========] - loss: 1723.6277392544678\n",
      " Epoch 533/1000; Batch 7/7: [===========] - loss: 1653.0989446701785\n",
      " Epoch 534/1000; Batch 7/7: [===========] - loss: 1200.4893415698175\n",
      " Epoch 535/1000; Batch 7/7: [===========] - loss: 1358.0523932065673\n",
      " Epoch 536/1000; Batch 7/7: [===========] - loss: 1048.8527408350133\n",
      " Epoch 537/1000; Batch 7/7: [===========] - loss: 1471.7153058295792\n",
      " Epoch 538/1000; Batch 7/7: [===========] - loss: 1316.5289109938572\n",
      " Epoch 539/1000; Batch 7/7: [===========] - loss: 1155.1273439697359\n",
      " Epoch 540/1000; Batch 7/7: [===========] - loss: 924.50968594748665\n",
      " Epoch 541/1000; Batch 7/7: [===========] - loss: 1133.6655864598933\n",
      " Epoch 542/1000; Batch 7/7: [===========] - loss: 1489.9930677082352\n",
      " Epoch 543/1000; Batch 7/7: [===========] - loss: 1388.2378355893254\n",
      " Epoch 544/1000; Batch 7/7: [===========] - loss: 1468.2021218206487\n",
      " Epoch 545/1000; Batch 7/7: [===========] - loss: 1045.7950529688706\n",
      " Epoch 546/1000; Batch 7/7: [===========] - loss: 1365.0977663584586\n",
      " Epoch 547/1000; Batch 7/7: [===========] - loss: 1342.2151555361097\n",
      " Epoch 548/1000; Batch 7/7: [===========] - loss: 1493.5651053224408\n",
      " Epoch 549/1000; Batch 7/7: [===========] - loss: 1306.4455900717091\n",
      " Epoch 550/1000; Batch 7/7: [===========] - loss: 1497.0757079125633\n",
      " Epoch 551/1000; Batch 7/7: [===========] - loss: 1627.5598359106928\n",
      " Epoch 552/1000; Batch 7/7: [===========] - loss: 1515.8304778018385\n",
      " Epoch 553/1000; Batch 7/7: [===========] - loss: 1585.0631424948447\n",
      " Epoch 554/1000; Batch 7/7: [===========] - loss: 1344.1067821814542\n",
      " Epoch 555/1000; Batch 7/7: [===========] - loss: 1005.6663519534985\n",
      " Epoch 556/1000; Batch 7/7: [===========] - loss: 1378.9446927412394\n",
      " Epoch 557/1000; Batch 7/7: [===========] - loss: 1452.5175484859903\n",
      " Epoch 558/1000; Batch 7/7: [===========] - loss: 1595.5516826817247\n",
      " Epoch 559/1000; Batch 7/7: [===========] - loss: 931.43373959099742\n",
      " Epoch 560/1000; Batch 7/7: [===========] - loss: 1677.5826032965574\n",
      " Epoch 561/1000; Batch 7/7: [===========] - loss: 1315.7829160658525\n",
      " Epoch 562/1000; Batch 7/7: [===========] - loss: 1483.9834557086378\n",
      " Epoch 563/1000; Batch 7/7: [===========] - loss: 1471.6544468507618\n",
      " Epoch 564/1000; Batch 7/7: [===========] - loss: 1300.4929885834624\n",
      " Epoch 565/1000; Batch 7/7: [===========] - loss: 1809.1730052664716\n",
      " Epoch 566/1000; Batch 7/7: [===========] - loss: 1447.9231210697437\n",
      " Epoch 567/1000; Batch 7/7: [===========] - loss: 1313.2922570398523\n",
      " Epoch 568/1000; Batch 7/7: [===========] - loss: 1311.8660662735383\n",
      " Epoch 569/1000; Batch 7/7: [===========] - loss: 1285.3238020722712\n",
      " Epoch 570/1000; Batch 7/7: [===========] - loss: 1479.7632346426399\n",
      " Epoch 571/1000; Batch 7/7: [===========] - loss: 1354.5635289978072\n",
      " Epoch 572/1000; Batch 7/7: [===========] - loss: 1603.5607159522503\n",
      " Epoch 573/1000; Batch 7/7: [===========] - loss: 1585.6645452303885\n",
      " Epoch 574/1000; Batch 7/7: [===========] - loss: 1531.2444592328045\n",
      " Epoch 575/1000; Batch 7/7: [===========] - loss: 1323.8474135172341\n",
      " Epoch 576/1000; Batch 7/7: [===========] - loss: 1421.9669347975653\n",
      " Epoch 577/1000; Batch 7/7: [===========] - loss: 1512.7345410500288\n",
      " Epoch 578/1000; Batch 7/7: [===========] - loss: 1352.9781175095254\n",
      " Epoch 579/1000; Batch 7/7: [===========] - loss: 1508.3685307753387\n",
      " Epoch 580/1000; Batch 7/7: [===========] - loss: 1030.0335433364876\n",
      " Epoch 581/1000; Batch 7/7: [===========] - loss: 1597.2552833048271\n",
      " Epoch 582/1000; Batch 7/7: [===========] - loss: 1589.5955041393938\n",
      " Epoch 583/1000; Batch 7/7: [===========] - loss: 1551.3501187286106\n",
      " Epoch 584/1000; Batch 7/7: [===========] - loss: 1380.9422786185269\n",
      " Epoch 585/1000; Batch 7/7: [===========] - loss: 1444.4633677897195\n",
      " Epoch 586/1000; Batch 7/7: [===========] - loss: 1901.1490969224665\n",
      " Epoch 587/1000; Batch 7/7: [===========] - loss: 1444.0309318982615\n",
      " Epoch 588/1000; Batch 7/7: [===========] - loss: 1383.5325496197431\n",
      " Epoch 589/1000; Batch 7/7: [===========] - loss: 1835.5373593510545\n",
      " Epoch 590/1000; Batch 7/7: [===========] - loss: 1592.4600612865054\n",
      " Epoch 591/1000; Batch 7/7: [===========] - loss: 1336.4974752364394\n",
      " Epoch 592/1000; Batch 7/7: [===========] - loss: 1541.5250476609135\n",
      " Epoch 593/1000; Batch 7/7: [===========] - loss: 1823.0758040287621\n",
      " Epoch 594/1000; Batch 7/7: [===========] - loss: 1208.1839903378893\n",
      " Epoch 595/1000; Batch 7/7: [===========] - loss: 1226.3791105290425\n",
      " Epoch 596/1000; Batch 7/7: [===========] - loss: 1755.6598839831213\n",
      " Epoch 597/1000; Batch 7/7: [===========] - loss: 1232.3408724245876\n",
      " Epoch 598/1000; Batch 7/7: [===========] - loss: 1526.7797721551874\n",
      " Epoch 599/1000; Batch 7/7: [===========] - loss: 1669.3412012170877\n",
      " Epoch 600/1000; Batch 7/7: [===========] - loss: 1420.4804890350078\n",
      " Epoch 601/1000; Batch 7/7: [===========] - loss: 1819.3514811549166\n",
      " Epoch 602/1000; Batch 7/7: [===========] - loss: 1189.5527132613447\n",
      " Epoch 603/1000; Batch 7/7: [===========] - loss: 1428.6217771406234\n",
      " Epoch 604/1000; Batch 7/7: [===========] - loss: 976.66657680242764\n",
      " Epoch 605/1000; Batch 7/7: [===========] - loss: 1434.3037213826408\n",
      " Epoch 606/1000; Batch 7/7: [===========] - loss: 1092.9254529255929\n",
      " Epoch 607/1000; Batch 7/7: [===========] - loss: 1587.5765680609716\n",
      " Epoch 608/1000; Batch 7/7: [===========] - loss: 1556.2430979970523\n",
      " Epoch 609/1000; Batch 7/7: [===========] - loss: 1313.2053703445456\n",
      " Epoch 610/1000; Batch 7/7: [===========] - loss: 1896.4817871293476\n",
      " Epoch 611/1000; Batch 7/7: [===========] - loss: 1113.5553515524375\n",
      " Epoch 612/1000; Batch 7/7: [===========] - loss: 1706.7950145861232\n",
      " Epoch 613/1000; Batch 7/7: [===========] - loss: 1757.3947910459558\n",
      " Epoch 614/1000; Batch 7/7: [===========] - loss: 1340.7983612255478\n",
      " Epoch 615/1000; Batch 7/7: [===========] - loss: 1539.9938263294744\n",
      " Epoch 616/1000; Batch 7/7: [===========] - loss: 1624.1521085687332\n",
      " Epoch 617/1000; Batch 7/7: [===========] - loss: 1922.5345834374898\n",
      " Epoch 618/1000; Batch 7/7: [===========] - loss: 1223.1769983614688\n",
      " Epoch 619/1000; Batch 7/7: [===========] - loss: 1897.2489227942979\n",
      " Epoch 620/1000; Batch 7/7: [===========] - loss: 1341.9386116280032\n",
      " Epoch 621/1000; Batch 7/7: [===========] - loss: 1414.1854819748614\n",
      " Epoch 622/1000; Batch 7/7: [===========] - loss: 1565.4063585936337\n",
      " Epoch 623/1000; Batch 7/7: [===========] - loss: 1400.5889752113217\n",
      " Epoch 624/1000; Batch 7/7: [===========] - loss: 1093.1245855011025\n",
      " Epoch 625/1000; Batch 7/7: [===========] - loss: 1074.6237136851646\n",
      " Epoch 626/1000; Batch 7/7: [===========] - loss: 1154.3640319555636\n",
      " Epoch 627/1000; Batch 7/7: [===========] - loss: 1101.6077883088208\n",
      " Epoch 628/1000; Batch 7/7: [===========] - loss: 1532.6794322142762\n",
      " Epoch 629/1000; Batch 7/7: [===========] - loss: 1667.8982135554859\n",
      " Epoch 630/1000; Batch 7/7: [===========] - loss: 1687.2918063119523\n",
      " Epoch 631/1000; Batch 7/7: [===========] - loss: 1635.5957796604432\n",
      " Epoch 632/1000; Batch 7/7: [===========] - loss: 1456.1360266772158\n",
      " Epoch 633/1000; Batch 7/7: [===========] - loss: 1489.0841523365991\n",
      " Epoch 634/1000; Batch 7/7: [===========] - loss: 1798.3945016386356\n",
      " Epoch 635/1000; Batch 7/7: [===========] - loss: 1521.3636955964698\n",
      " Epoch 636/1000; Batch 7/7: [===========] - loss: 1728.7038719371526\n",
      " Epoch 637/1000; Batch 7/7: [===========] - loss: 1101.0737442891157\n",
      " Epoch 638/1000; Batch 7/7: [===========] - loss: 1786.0475542799993\n",
      " Epoch 639/1000; Batch 7/7: [===========] - loss: 1544.1269512971375\n",
      " Epoch 640/1000; Batch 7/7: [===========] - loss: 1470.9534762765657\n",
      " Epoch 641/1000; Batch 7/7: [===========] - loss: 1667.0151541778623\n",
      " Epoch 642/1000; Batch 7/7: [===========] - loss: 1604.7745071617346\n",
      " Epoch 643/1000; Batch 7/7: [===========] - loss: 1381.6176265166218\n",
      " Epoch 644/1000; Batch 7/7: [===========] - loss: 1643.6073537282582\n",
      " Epoch 645/1000; Batch 7/7: [===========] - loss: 1670.4848024957236\n",
      " Epoch 646/1000; Batch 7/7: [===========] - loss: 1605.7694723959826\n",
      " Epoch 647/1000; Batch 7/7: [===========] - loss: 1021.9158401000882\n",
      " Epoch 648/1000; Batch 7/7: [===========] - loss: 1407.7610002230706\n",
      " Epoch 649/1000; Batch 7/7: [===========] - loss: 1567.5736450013626\n",
      " Epoch 650/1000; Batch 7/7: [===========] - loss: 1747.1344129973854\n",
      " Epoch 651/1000; Batch 7/7: [===========] - loss: 1242.3015535975533\n",
      " Epoch 652/1000; Batch 7/7: [===========] - loss: 1404.2756787660928\n",
      " Epoch 653/1000; Batch 7/7: [===========] - loss: 998.02002147761324\n",
      " Epoch 654/1000; Batch 7/7: [===========] - loss: 1914.5094792048918\n",
      " Epoch 655/1000; Batch 7/7: [===========] - loss: 1583.5439283111068\n",
      " Epoch 656/1000; Batch 7/7: [===========] - loss: 1542.6104612069764\n",
      " Epoch 657/1000; Batch 7/7: [===========] - loss: 1329.5933750499303\n",
      " Epoch 658/1000; Batch 7/7: [===========] - loss: 1202.2662734823823\n",
      " Epoch 659/1000; Batch 7/7: [===========] - loss: 1489.2028720023882\n",
      " Epoch 660/1000; Batch 7/7: [===========] - loss: 1338.9643182956293\n",
      " Epoch 661/1000; Batch 7/7: [===========] - loss: 1513.5257039522835\n",
      " Epoch 662/1000; Batch 7/7: [===========] - loss: 1496.8351999041484\n",
      " Epoch 663/1000; Batch 7/7: [===========] - loss: 1425.3362588733437\n",
      " Epoch 664/1000; Batch 7/7: [===========] - loss: 1550.0145032332462\n",
      " Epoch 665/1000; Batch 7/7: [===========] - loss: 1474.6003028755488\n",
      " Epoch 666/1000; Batch 7/7: [===========] - loss: 1582.1368637889886\n",
      " Epoch 667/1000; Batch 7/7: [===========] - loss: 1300.6799259556531\n",
      " Epoch 668/1000; Batch 7/7: [===========] - loss: 1533.3538516583464\n",
      " Epoch 669/1000; Batch 7/7: [===========] - loss: 1144.3343868574845\n",
      " Epoch 670/1000; Batch 7/7: [===========] - loss: 1618.3062861659664\n",
      " Epoch 671/1000; Batch 7/7: [===========] - loss: 1134.8349836770124\n",
      " Epoch 672/1000; Batch 7/7: [===========] - loss: 1371.9566743680293\n",
      " Epoch 673/1000; Batch 7/7: [===========] - loss: 1586.0731323925692\n",
      " Epoch 674/1000; Batch 7/7: [===========] - loss: 1689.2357781548144\n",
      " Epoch 675/1000; Batch 7/7: [===========] - loss: 1754.1812502723983\n",
      " Epoch 676/1000; Batch 7/7: [===========] - loss: 1476.1547776861883\n",
      " Epoch 677/1000; Batch 7/7: [===========] - loss: 1483.8290558837768\n",
      " Epoch 678/1000; Batch 7/7: [===========] - loss: 1458.4875373458815\n",
      " Epoch 679/1000; Batch 7/7: [===========] - loss: 1391.2187864626653\n",
      " Epoch 680/1000; Batch 7/7: [===========] - loss: 1780.9604946561622\n",
      " Epoch 681/1000; Batch 7/7: [===========] - loss: 1692.7668139582474\n",
      " Epoch 682/1000; Batch 7/7: [===========] - loss: 1244.6100926807221\n",
      " Epoch 683/1000; Batch 7/7: [===========] - loss: 1200.2949258717993\n",
      " Epoch 684/1000; Batch 7/7: [===========] - loss: 1626.0945365100301\n",
      " Epoch 685/1000; Batch 7/7: [===========] - loss: 1186.9497951631727\n",
      " Epoch 686/1000; Batch 7/7: [===========] - loss: 1236.0860927919355\n",
      " Epoch 687/1000; Batch 7/7: [===========] - loss: 1555.3629547872795\n",
      " Epoch 688/1000; Batch 7/7: [===========] - loss: 1491.7393869528653\n",
      " Epoch 689/1000; Batch 7/7: [===========] - loss: 1314.2970027893846\n",
      " Epoch 690/1000; Batch 7/7: [===========] - loss: 1492.1533050917126\n",
      " Epoch 691/1000; Batch 7/7: [===========] - loss: 1092.8879896213314\n",
      " Epoch 692/1000; Batch 7/7: [===========] - loss: 1647.2865531435455\n",
      " Epoch 693/1000; Batch 7/7: [===========] - loss: 1409.1412223746473\n",
      " Epoch 694/1000; Batch 7/7: [===========] - loss: 1191.6111692054858\n",
      " Epoch 695/1000; Batch 7/7: [===========] - loss: 1610.5097123680272\n",
      " Epoch 696/1000; Batch 7/7: [===========] - loss: 1519.2983208378446\n",
      " Epoch 697/1000; Batch 7/7: [===========] - loss: 1522.2252888881537\n",
      " Epoch 698/1000; Batch 7/7: [===========] - loss: 1611.7466694143627\n",
      " Epoch 699/1000; Batch 7/7: [===========] - loss: 1684.8825905801846\n",
      " Epoch 700/1000; Batch 7/7: [===========] - loss: 1263.3761495804176\n",
      " Epoch 701/1000; Batch 7/7: [===========] - loss: 1306.6701702992268\n",
      " Epoch 702/1000; Batch 7/7: [===========] - loss: 1471.1886666919113\n",
      " Epoch 703/1000; Batch 7/7: [===========] - loss: 1193.5465828528115\n",
      " Epoch 704/1000; Batch 7/7: [===========] - loss: 1103.9505813575832\n",
      " Epoch 705/1000; Batch 7/7: [===========] - loss: 903.07097759960495\n",
      " Epoch 706/1000; Batch 7/7: [===========] - loss: 1664.3971681801852\n",
      " Epoch 707/1000; Batch 7/7: [===========] - loss: 1093.5733394070444\n",
      " Epoch 708/1000; Batch 7/7: [===========] - loss: 2156.1966893585112\n",
      " Epoch 709/1000; Batch 7/7: [===========] - loss: 1361.7593793783594\n",
      " Epoch 710/1000; Batch 7/7: [===========] - loss: 1536.3757380148095\n",
      " Epoch 711/1000; Batch 7/7: [===========] - loss: 1611.3893304032003\n",
      " Epoch 712/1000; Batch 7/7: [===========] - loss: 1372.4915525620881\n",
      " Epoch 713/1000; Batch 7/7: [===========] - loss: 1435.9638596102063\n",
      " Epoch 714/1000; Batch 7/7: [===========] - loss: 1849.7664140294844\n",
      " Epoch 715/1000; Batch 7/7: [===========] - loss: 1212.6937239518522\n",
      " Epoch 716/1000; Batch 7/7: [===========] - loss: 1157.8459770405224\n",
      " Epoch 717/1000; Batch 7/7: [===========] - loss: 1421.7057314757291\n",
      " Epoch 718/1000; Batch 7/7: [===========] - loss: 1602.6332326443548\n",
      " Epoch 719/1000; Batch 7/7: [===========] - loss: 1041.6372026259414\n",
      " Epoch 720/1000; Batch 7/7: [===========] - loss: 1512.4182235006285\n",
      " Epoch 721/1000; Batch 7/7: [===========] - loss: 1527.5650744977127\n",
      " Epoch 722/1000; Batch 7/7: [===========] - loss: 1590.4475651774612\n",
      " Epoch 723/1000; Batch 7/7: [===========] - loss: 1580.7876199452995\n",
      " Epoch 724/1000; Batch 7/7: [===========] - loss: 1781.7256138885234\n",
      " Epoch 725/1000; Batch 7/7: [===========] - loss: 1791.8636080058086\n",
      " Epoch 726/1000; Batch 7/7: [===========] - loss: 1469.6414320226343\n",
      " Epoch 727/1000; Batch 7/7: [===========] - loss: 1211.0914352532955\n",
      " Epoch 728/1000; Batch 7/7: [===========] - loss: 1798.6058611457969\n",
      " Epoch 729/1000; Batch 7/7: [===========] - loss: 1349.2961992633623\n",
      " Epoch 730/1000; Batch 7/7: [===========] - loss: 1606.7086900175887\n",
      " Epoch 731/1000; Batch 7/7: [===========] - loss: 1267.7014162037942\n",
      " Epoch 732/1000; Batch 7/7: [===========] - loss: 1103.6479714767515\n",
      " Epoch 733/1000; Batch 7/7: [===========] - loss: 1817.8922323498832\n",
      " Epoch 734/1000; Batch 7/7: [===========] - loss: 1410.2922947270195\n",
      " Epoch 735/1000; Batch 7/7: [===========] - loss: 1497.2348084372295\n",
      " Epoch 736/1000; Batch 7/7: [===========] - loss: 1334.3594291544799\n",
      " Epoch 737/1000; Batch 7/7: [===========] - loss: 1463.3098461274517\n",
      " Epoch 738/1000; Batch 7/7: [===========] - loss: 1385.2627226246655\n",
      " Epoch 739/1000; Batch 7/7: [===========] - loss: 1663.2146830855083\n",
      " Epoch 740/1000; Batch 7/7: [===========] - loss: 1502.2694243000255\n",
      " Epoch 741/1000; Batch 7/7: [===========] - loss: 1333.8651181147364\n",
      " Epoch 742/1000; Batch 7/7: [===========] - loss: 1824.6229360354125\n",
      " Epoch 743/1000; Batch 7/7: [===========] - loss: 1560.0159836606238\n",
      " Epoch 744/1000; Batch 7/7: [===========] - loss: 1444.2735918389867\n",
      " Epoch 745/1000; Batch 7/7: [===========] - loss: 1604.9260278957247\n",
      " Epoch 746/1000; Batch 7/7: [===========] - loss: 1439.4704378529832\n",
      " Epoch 747/1000; Batch 7/7: [===========] - loss: 1209.8254931426432\n",
      " Epoch 748/1000; Batch 7/7: [===========] - loss: 1507.0379492138459\n",
      " Epoch 749/1000; Batch 7/7: [===========] - loss: 1334.4261046144482\n",
      " Epoch 750/1000; Batch 7/7: [===========] - loss: 1257.9703164710327\n",
      " Epoch 751/1000; Batch 7/7: [===========] - loss: 1582.5957300852456\n",
      " Epoch 752/1000; Batch 7/7: [===========] - loss: 1542.5972112071367\n",
      " Epoch 753/1000; Batch 7/7: [===========] - loss: 1438.0727833523613\n",
      " Epoch 754/1000; Batch 7/7: [===========] - loss: 1156.9316145647713\n",
      " Epoch 755/1000; Batch 7/7: [===========] - loss: 1595.0911058440172\n",
      " Epoch 756/1000; Batch 7/7: [===========] - loss: 1111.3650105489583\n",
      " Epoch 757/1000; Batch 7/7: [===========] - loss: 1395.8520435942796\n",
      " Epoch 758/1000; Batch 7/7: [===========] - loss: 1490.9697146353376\n",
      " Epoch 759/1000; Batch 7/7: [===========] - loss: 1367.2436038487726\n",
      " Epoch 760/1000; Batch 7/7: [===========] - loss: 1559.0351871161257\n",
      " Epoch 761/1000; Batch 7/7: [===========] - loss: 1217.2940423225805\n",
      " Epoch 762/1000; Batch 7/7: [===========] - loss: 1094.1249356811045\n",
      " Epoch 763/1000; Batch 7/7: [===========] - loss: 1305.4333160414503\n",
      " Epoch 764/1000; Batch 7/7: [===========] - loss: 1669.7028329221484\n",
      " Epoch 765/1000; Batch 7/7: [===========] - loss: 1516.5097153865277\n",
      " Epoch 766/1000; Batch 7/7: [===========] - loss: 1381.0751698609076\n",
      " Epoch 767/1000; Batch 7/7: [===========] - loss: 1506.4884481794536\n",
      " Epoch 768/1000; Batch 7/7: [===========] - loss: 1244.3796182567305\n",
      " Epoch 769/1000; Batch 7/7: [===========] - loss: 1709.0287645484799\n",
      " Epoch 770/1000; Batch 7/7: [===========] - loss: 1141.6747561687034\n",
      " Epoch 771/1000; Batch 7/7: [===========] - loss: 1468.8054825273623\n",
      " Epoch 772/1000; Batch 7/7: [===========] - loss: 767.24416988955622\n",
      " Epoch 773/1000; Batch 7/7: [===========] - loss: 1380.5147208649855\n",
      " Epoch 774/1000; Batch 7/7: [===========] - loss: 1242.4846889447797\n",
      " Epoch 775/1000; Batch 7/7: [===========] - loss: 1581.7146837107287\n",
      " Epoch 776/1000; Batch 7/7: [===========] - loss: 1325.0726869582081\n",
      " Epoch 777/1000; Batch 7/7: [===========] - loss: 1139.5452495905724\n",
      " Epoch 778/1000; Batch 7/7: [===========] - loss: 1265.8794502573783\n",
      " Epoch 779/1000; Batch 7/7: [===========] - loss: 1124.3108745525822\n",
      " Epoch 780/1000; Batch 7/7: [===========] - loss: 1324.2184865857027\n",
      " Epoch 781/1000; Batch 7/7: [===========] - loss: 2078.1612781444585\n",
      " Epoch 782/1000; Batch 7/7: [===========] - loss: 1162.4004531580933\n",
      " Epoch 783/1000; Batch 7/7: [===========] - loss: 1208.6610649793583\n",
      " Epoch 784/1000; Batch 7/7: [===========] - loss: 895.76172074090165\n",
      " Epoch 785/1000; Batch 7/7: [===========] - loss: 1715.8156250396398\n",
      " Epoch 786/1000; Batch 7/7: [===========] - loss: 1592.2668722970687\n",
      " Epoch 787/1000; Batch 7/7: [===========] - loss: 1305.5552372496534\n",
      " Epoch 788/1000; Batch 7/7: [===========] - loss: 1455.8395643153518\n",
      " Epoch 789/1000; Batch 7/7: [===========] - loss: 1587.2498498758198\n",
      " Epoch 790/1000; Batch 7/7: [===========] - loss: 1123.2538268814783\n",
      " Epoch 791/1000; Batch 7/7: [===========] - loss: 1343.2404195240063\n",
      " Epoch 792/1000; Batch 7/7: [===========] - loss: 1531.2453147428355\n",
      " Epoch 793/1000; Batch 7/7: [===========] - loss: 1512.9267125569372\n",
      " Epoch 794/1000; Batch 7/7: [===========] - loss: 1634.2861529110824\n",
      " Epoch 795/1000; Batch 7/7: [===========] - loss: 1378.0231576313637\n",
      " Epoch 796/1000; Batch 7/7: [===========] - loss: 1333.8256528527663\n",
      " Epoch 797/1000; Batch 7/7: [===========] - loss: 1309.9898875420276\n",
      " Epoch 798/1000; Batch 7/7: [===========] - loss: 1271.9954794313418\n",
      " Epoch 799/1000; Batch 7/7: [===========] - loss: 1528.4603291127974\n",
      " Epoch 800/1000; Batch 7/7: [===========] - loss: 1178.3010141649337\n",
      " Epoch 801/1000; Batch 7/7: [===========] - loss: 1299.8779347100118\n",
      " Epoch 802/1000; Batch 7/7: [===========] - loss: 1679.9425726807888\n",
      " Epoch 803/1000; Batch 7/7: [===========] - loss: 1800.4695351473351\n",
      " Epoch 804/1000; Batch 7/7: [===========] - loss: 1509.9728567890627\n",
      " Epoch 805/1000; Batch 7/7: [===========] - loss: 1428.0964221934887\n",
      " Epoch 806/1000; Batch 7/7: [===========] - loss: 1361.2414105906746\n",
      " Epoch 807/1000; Batch 7/7: [===========] - loss: 1614.6735120520593\n",
      " Epoch 808/1000; Batch 7/7: [===========] - loss: 1429.4148135470164\n",
      " Epoch 809/1000; Batch 7/7: [===========] - loss: 1425.1644164846951\n",
      " Epoch 810/1000; Batch 7/7: [===========] - loss: 1296.8923606169014\n",
      " Epoch 811/1000; Batch 7/7: [===========] - loss: 1792.7588168536035\n",
      " Epoch 812/1000; Batch 7/7: [===========] - loss: 1770.1678629761604\n",
      " Epoch 813/1000; Batch 7/7: [===========] - loss: 1369.8427489634123\n",
      " Epoch 814/1000; Batch 7/7: [===========] - loss: 1355.9709582686457\n",
      " Epoch 815/1000; Batch 7/7: [===========] - loss: 1359.0245962611607\n",
      " Epoch 816/1000; Batch 7/7: [===========] - loss: 1412.5144532898055\n",
      " Epoch 817/1000; Batch 7/7: [===========] - loss: 1413.6361845632755\n",
      " Epoch 818/1000; Batch 7/7: [===========] - loss: 1505.5780881534747\n",
      " Epoch 819/1000; Batch 7/7: [===========] - loss: 1677.3600032880228\n",
      " Epoch 820/1000; Batch 7/7: [===========] - loss: 1681.1699289452333\n",
      " Epoch 821/1000; Batch 7/7: [===========] - loss: 1486.4428435723885\n",
      " Epoch 822/1000; Batch 7/7: [===========] - loss: 1581.2727170242454\n",
      " Epoch 823/1000; Batch 7/7: [===========] - loss: 1359.3974788594137\n",
      " Epoch 824/1000; Batch 7/7: [===========] - loss: 1456.6261957199208\n",
      " Epoch 825/1000; Batch 7/7: [===========] - loss: 1566.3604371655788\n",
      " Epoch 826/1000; Batch 7/7: [===========] - loss: 1364.4897006875896\n",
      " Epoch 827/1000; Batch 7/7: [===========] - loss: 1128.1363121032628\n",
      " Epoch 828/1000; Batch 7/7: [===========] - loss: 1271.8936723281624\n",
      " Epoch 829/1000; Batch 7/7: [===========] - loss: 1398.6265750489965\n",
      " Epoch 830/1000; Batch 7/7: [===========] - loss: 1731.9193196121319\n",
      " Epoch 831/1000; Batch 7/7: [===========] - loss: 1471.1386090934783\n",
      " Epoch 832/1000; Batch 7/7: [===========] - loss: 1219.8847880706731\n",
      " Epoch 833/1000; Batch 7/7: [===========] - loss: 1365.5932765566522\n",
      " Epoch 834/1000; Batch 7/7: [===========] - loss: 1677.3834108530564\n",
      " Epoch 835/1000; Batch 7/7: [===========] - loss: 1222.4393682052448\n",
      " Epoch 836/1000; Batch 7/7: [===========] - loss: 1341.5458756522178\n",
      " Epoch 837/1000; Batch 7/7: [===========] - loss: 1172.7910538445826\n",
      " Epoch 838/1000; Batch 7/7: [===========] - loss: 1149.2559864141288\n",
      " Epoch 839/1000; Batch 7/7: [===========] - loss: 1806.1493334827987\n",
      " Epoch 840/1000; Batch 7/7: [===========] - loss: 1628.6735514350578\n",
      " Epoch 841/1000; Batch 7/7: [===========] - loss: 1427.0156474378518\n",
      " Epoch 842/1000; Batch 7/7: [===========] - loss: 1595.8499678684989\n",
      " Epoch 843/1000; Batch 7/7: [===========] - loss: 1645.5419128733574\n",
      " Epoch 844/1000; Batch 7/7: [===========] - loss: 1598.9090423782911\n",
      " Epoch 845/1000; Batch 7/7: [===========] - loss: 1853.5391413698289\n",
      " Epoch 846/1000; Batch 7/7: [===========] - loss: 1453.6300663537622\n",
      " Epoch 847/1000; Batch 7/7: [===========] - loss: 1261.3080978579587\n",
      " Epoch 848/1000; Batch 7/7: [===========] - loss: 1774.3386864440356\n",
      " Epoch 849/1000; Batch 7/7: [===========] - loss: 1546.7300805377163\n",
      " Epoch 850/1000; Batch 7/7: [===========] - loss: 1731.1424572728615\n",
      " Epoch 851/1000; Batch 7/7: [===========] - loss: 1769.9394663787969\n",
      " Epoch 852/1000; Batch 7/7: [===========] - loss: 1725.2478411752916\n",
      " Epoch 853/1000; Batch 7/7: [===========] - loss: 1652.7759489504333\n",
      " Epoch 854/1000; Batch 7/7: [===========] - loss: 1187.0218327318494\n",
      " Epoch 855/1000; Batch 7/7: [===========] - loss: 1271.5486635253349\n",
      " Epoch 856/1000; Batch 7/7: [===========] - loss: 1283.2783176392584\n",
      " Epoch 857/1000; Batch 7/7: [===========] - loss: 1707.3571775965318\n",
      " Epoch 858/1000; Batch 7/7: [===========] - loss: 949.40209697718235\n",
      " Epoch 859/1000; Batch 7/7: [===========] - loss: 1617.2248954462278\n",
      " Epoch 860/1000; Batch 7/7: [===========] - loss: 1587.5411196476223\n",
      " Epoch 861/1000; Batch 7/7: [===========] - loss: 1479.6004550543041\n",
      " Epoch 862/1000; Batch 7/7: [===========] - loss: 1656.3931243569439\n",
      " Epoch 863/1000; Batch 7/7: [===========] - loss: 1361.9994698876592\n",
      " Epoch 864/1000; Batch 7/7: [===========] - loss: 1405.1342569962075\n",
      " Epoch 865/1000; Batch 7/7: [===========] - loss: 1682.1428787315663\n",
      " Epoch 866/1000; Batch 7/7: [===========] - loss: 1068.6744706961613\n",
      " Epoch 867/1000; Batch 7/7: [===========] - loss: 1435.3067256690263\n",
      " Epoch 868/1000; Batch 7/7: [===========] - loss: 1303.2121156088112\n",
      " Epoch 869/1000; Batch 7/7: [===========] - loss: 1256.4253102290177\n",
      " Epoch 870/1000; Batch 7/7: [===========] - loss: 1638.5187228077114\n",
      " Epoch 871/1000; Batch 7/7: [===========] - loss: 1336.2972411756957\n",
      " Epoch 872/1000; Batch 7/7: [===========] - loss: 1375.1865812684282\n",
      " Epoch 873/1000; Batch 7/7: [===========] - loss: 1783.9416034900962\n",
      " Epoch 874/1000; Batch 7/7: [===========] - loss: 1528.6473643644856\n",
      " Epoch 875/1000; Batch 7/7: [===========] - loss: 1796.6989052222361\n",
      " Epoch 876/1000; Batch 7/7: [===========] - loss: 2008.6848087756946\n",
      " Epoch 877/1000; Batch 7/7: [===========] - loss: 1530.9034139667462\n",
      " Epoch 878/1000; Batch 7/7: [===========] - loss: 1539.9442593738136\n",
      " Epoch 879/1000; Batch 7/7: [===========] - loss: 1267.5162874884238\n",
      " Epoch 880/1000; Batch 7/7: [===========] - loss: 972.07246420983364\n",
      " Epoch 881/1000; Batch 7/7: [===========] - loss: 1827.1540798391409\n",
      " Epoch 882/1000; Batch 7/7: [===========] - loss: 1435.5371641139718\n",
      " Epoch 883/1000; Batch 7/7: [===========] - loss: 1706.8111391431544\n",
      " Epoch 884/1000; Batch 7/7: [===========] - loss: 1765.4121215533119\n",
      " Epoch 885/1000; Batch 7/7: [===========] - loss: 1277.1610965241327\n",
      " Epoch 886/1000; Batch 7/7: [===========] - loss: 1572.2375212464879\n",
      " Epoch 887/1000; Batch 7/7: [===========] - loss: 1198.9140447196594\n",
      " Epoch 888/1000; Batch 7/7: [===========] - loss: 1546.6508767151004\n",
      " Epoch 889/1000; Batch 7/7: [===========] - loss: 1483.3852439345503\n",
      " Epoch 890/1000; Batch 7/7: [===========] - loss: 1249.6142288074457\n",
      " Epoch 891/1000; Batch 7/7: [===========] - loss: 1466.9421001500484\n",
      " Epoch 892/1000; Batch 7/7: [===========] - loss: 1258.8317700230925\n",
      " Epoch 893/1000; Batch 7/7: [===========] - loss: 1514.7748643087577\n",
      " Epoch 894/1000; Batch 7/7: [===========] - loss: 1530.5544137977176\n",
      " Epoch 895/1000; Batch 7/7: [===========] - loss: 1764.6025309490115\n",
      " Epoch 896/1000; Batch 7/7: [===========] - loss: 1845.8678325951146\n",
      " Epoch 897/1000; Batch 7/7: [===========] - loss: 1786.9010782279368\n",
      " Epoch 898/1000; Batch 7/7: [===========] - loss: 1121.4831703744953\n",
      " Epoch 899/1000; Batch 7/7: [===========] - loss: 1567.0966680981508\n",
      " Epoch 900/1000; Batch 7/7: [===========] - loss: 1645.7108281460185\n",
      " Epoch 901/1000; Batch 7/7: [===========] - loss: 1256.4330405488517\n",
      " Epoch 902/1000; Batch 7/7: [===========] - loss: 1437.4309342248869\n",
      " Epoch 903/1000; Batch 7/7: [===========] - loss: 1454.3824568237667\n",
      " Epoch 904/1000; Batch 7/7: [===========] - loss: 1743.3444689698326\n",
      " Epoch 905/1000; Batch 7/7: [===========] - loss: 932.46583667755585\n",
      " Epoch 906/1000; Batch 7/7: [===========] - loss: 1312.3737242612751\n",
      " Epoch 907/1000; Batch 7/7: [===========] - loss: 1063.3239699080457\n",
      " Epoch 908/1000; Batch 7/7: [===========] - loss: 1732.2542517579573\n",
      " Epoch 909/1000; Batch 7/7: [===========] - loss: 1505.2626129877292\n",
      " Epoch 910/1000; Batch 7/7: [===========] - loss: 1502.5634450976534\n",
      " Epoch 911/1000; Batch 7/7: [===========] - loss: 1063.1454901756174\n",
      " Epoch 912/1000; Batch 7/7: [===========] - loss: 1797.1964906580888\n",
      " Epoch 913/1000; Batch 7/7: [===========] - loss: 1505.9410362913095\n",
      " Epoch 914/1000; Batch 7/7: [===========] - loss: 1306.3666556834696\n",
      " Epoch 915/1000; Batch 7/7: [===========] - loss: 1374.2008901860167\n",
      " Epoch 916/1000; Batch 7/7: [===========] - loss: 1769.9177454671549\n",
      " Epoch 917/1000; Batch 7/7: [===========] - loss: 1437.3858238826994\n",
      " Epoch 918/1000; Batch 7/7: [===========] - loss: 1218.9898239768618\n",
      " Epoch 919/1000; Batch 7/7: [===========] - loss: 1951.7424609973857\n",
      " Epoch 920/1000; Batch 7/7: [===========] - loss: 1513.6440380743397\n",
      " Epoch 921/1000; Batch 7/7: [===========] - loss: 1441.1351583276517\n",
      " Epoch 922/1000; Batch 7/7: [===========] - loss: 1402.8865672232162\n",
      " Epoch 923/1000; Batch 7/7: [===========] - loss: 1264.5172821201081\n",
      " Epoch 924/1000; Batch 7/7: [===========] - loss: 1227.6016953420274\n",
      " Epoch 925/1000; Batch 7/7: [===========] - loss: 1396.8214469011748\n",
      " Epoch 926/1000; Batch 7/7: [===========] - loss: 1094.5665807891172\n",
      " Epoch 927/1000; Batch 7/7: [===========] - loss: 1469.1449801877359\n",
      " Epoch 928/1000; Batch 7/7: [===========] - loss: 1178.5648233656011\n",
      " Epoch 929/1000; Batch 7/7: [===========] - loss: 1438.8887405264818\n",
      " Epoch 930/1000; Batch 7/7: [===========] - loss: 1437.1346057958726\n",
      " Epoch 931/1000; Batch 7/7: [===========] - loss: 1692.2267925223869\n",
      " Epoch 932/1000; Batch 7/7: [===========] - loss: 1179.9148327040878\n",
      " Epoch 933/1000; Batch 7/7: [===========] - loss: 1430.8014483290083\n",
      " Epoch 934/1000; Batch 7/7: [===========] - loss: 1649.5536335049715\n",
      " Epoch 935/1000; Batch 7/7: [===========] - loss: 1513.6151181273146\n",
      " Epoch 936/1000; Batch 7/7: [===========] - loss: 1464.8176871781984\n",
      " Epoch 937/1000; Batch 7/7: [===========] - loss: 1482.5695359631127\n",
      " Epoch 938/1000; Batch 7/7: [===========] - loss: 1568.1018536795629\n",
      " Epoch 939/1000; Batch 7/7: [===========] - loss: 1550.4740424410995\n",
      " Epoch 940/1000; Batch 7/7: [===========] - loss: 1283.0055444630852\n",
      " Epoch 941/1000; Batch 7/7: [===========] - loss: 1121.9048931293548\n",
      " Epoch 942/1000; Batch 7/7: [===========] - loss: 1241.9287218669126\n",
      " Epoch 943/1000; Batch 7/7: [===========] - loss: 1171.7534651448054\n",
      " Epoch 944/1000; Batch 7/7: [===========] - loss: 1648.6090286634203\n",
      " Epoch 945/1000; Batch 7/7: [===========] - loss: 1623.4574454768856\n",
      " Epoch 946/1000; Batch 7/7: [===========] - loss: 1242.3782362337768\n",
      " Epoch 947/1000; Batch 7/7: [===========] - loss: 1654.2917129639732\n",
      " Epoch 948/1000; Batch 7/7: [===========] - loss: 1221.4812758668088\n",
      " Epoch 949/1000; Batch 7/7: [===========] - loss: 1227.2194356764696\n",
      " Epoch 950/1000; Batch 7/7: [===========] - loss: 1316.2740497861726\n",
      " Epoch 951/1000; Batch 7/7: [===========] - loss: 1302.1348618897814\n",
      " Epoch 952/1000; Batch 7/7: [===========] - loss: 1555.8976429900968\n",
      " Epoch 953/1000; Batch 7/7: [===========] - loss: 1989.4018130234278\n",
      " Epoch 954/1000; Batch 7/7: [===========] - loss: 1253.1943140544213\n",
      " Epoch 955/1000; Batch 7/7: [===========] - loss: 1261.6530963463686\n",
      " Epoch 956/1000; Batch 7/7: [===========] - loss: 1270.6197664029507\n",
      " Epoch 957/1000; Batch 7/7: [===========] - loss: 1475.4454292484755\n",
      " Epoch 958/1000; Batch 7/7: [===========] - loss: 1632.5898473838233\n",
      " Epoch 959/1000; Batch 7/7: [===========] - loss: 1173.4648749358564\n",
      " Epoch 960/1000; Batch 7/7: [===========] - loss: 1613.3531196604782\n",
      " Epoch 961/1000; Batch 7/7: [===========] - loss: 1989.0142282910756\n",
      " Epoch 962/1000; Batch 7/7: [===========] - loss: 1510.4519685454686\n",
      " Epoch 963/1000; Batch 7/7: [===========] - loss: 1302.2632738940902\n",
      " Epoch 964/1000; Batch 7/7: [===========] - loss: 1736.9208947321121\n",
      " Epoch 965/1000; Batch 7/7: [===========] - loss: 1509.8680906999084\n",
      " Epoch 966/1000; Batch 7/7: [===========] - loss: 1138.9052410515637\n",
      " Epoch 967/1000; Batch 7/7: [===========] - loss: 1262.9480321453286\n",
      " Epoch 968/1000; Batch 7/7: [===========] - loss: 1332.0461669410727\n",
      " Epoch 969/1000; Batch 7/7: [===========] - loss: 2036.5845951940482\n",
      " Epoch 970/1000; Batch 7/7: [===========] - loss: 1321.0536672216393\n",
      " Epoch 971/1000; Batch 7/7: [===========] - loss: 1774.8646854934892\n",
      " Epoch 972/1000; Batch 7/7: [===========] - loss: 1456.8672143407862\n",
      " Epoch 973/1000; Batch 7/7: [===========] - loss: 1228.2787707551508\n",
      " Epoch 974/1000; Batch 7/7: [===========] - loss: 1249.1783846219018\n",
      " Epoch 975/1000; Batch 7/7: [===========] - loss: 1911.3810645762714\n",
      " Epoch 976/1000; Batch 7/7: [===========] - loss: 1495.2880775429046\n",
      " Epoch 977/1000; Batch 7/7: [===========] - loss: 1487.4214858632574\n",
      " Epoch 978/1000; Batch 7/7: [===========] - loss: 1464.1223092918822\n",
      " Epoch 979/1000; Batch 7/7: [===========] - loss: 1382.0718564942634\n",
      " Epoch 980/1000; Batch 7/7: [===========] - loss: 1598.3532358406142\n",
      " Epoch 981/1000; Batch 7/7: [===========] - loss: 1949.2125753176276\n",
      " Epoch 982/1000; Batch 7/7: [===========] - loss: 1624.6366839441234\n",
      " Epoch 983/1000; Batch 7/7: [===========] - loss: 1246.5485400733347\n",
      " Epoch 984/1000; Batch 7/7: [===========] - loss: 1319.6832481054155\n",
      " Epoch 985/1000; Batch 7/7: [===========] - loss: 1212.8254424490956\n",
      " Epoch 986/1000; Batch 7/7: [===========] - loss: 1680.0235788934025\n",
      " Epoch 987/1000; Batch 7/7: [===========] - loss: 1520.1388107886266\n",
      " Epoch 988/1000; Batch 7/7: [===========] - loss: 1494.3194845697772\n",
      " Epoch 989/1000; Batch 7/7: [===========] - loss: 1290.8437588407903\n",
      " Epoch 990/1000; Batch 7/7: [===========] - loss: 1631.0650480913814\n",
      " Epoch 991/1000; Batch 7/7: [===========] - loss: 1923.0796262193091\n",
      " Epoch 992/1000; Batch 7/7: [===========] - loss: 1450.9855360847087\n",
      " Epoch 993/1000; Batch 7/7: [===========] - loss: 1674.8904574428013\n",
      " Epoch 994/1000; Batch 7/7: [===========] - loss: 1042.1344901292387\n",
      " Epoch 995/1000; Batch 7/7: [===========] - loss: 1551.3138104733143\n",
      " Epoch 996/1000; Batch 7/7: [===========] - loss: 1818.3444160665215\n",
      " Epoch 997/1000; Batch 7/7: [===========] - loss: 956.67753936637988\n",
      " Epoch 998/1000; Batch 7/7: [===========] - loss: 1837.3673779002604\n",
      " Epoch 999/1000; Batch 7/7: [===========] - loss: 1626.8953539278455\n",
      " Epoch 1000/1000; Batch 7/7: [===========] - loss: 1181.2765481836414\n"
     ]
    }
   ],
   "source": [
    "nn = NeauralNetwork(layers=[\n",
    "        Layer(units=10, input_layer=True),\n",
    "        # Layer(units=40, activation=\"sigmoid\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=1),\n",
    "    ],\n",
    "    loss_function = \"mse\",\n",
    "    learning_rate=0.001, \n",
    "    verbose=True,\n",
    "    optimizer=\"gdm\",\n",
    "    batch_size = 64,\n",
    "    epochs=1000\n",
    ")\n",
    "\n",
    "y_diab = y_diab.reshape(-1, 1) # Network requirement\n",
    "\n",
    "nn.fit(X_diab, y_diab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
