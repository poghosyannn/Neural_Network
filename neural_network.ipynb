{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X_diab, y_diab = load_diabetes(return_X_y=True) # returns diabetes data shapes: (442, 10) and (442,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=60, n_features=10, noise=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(\n",
    "            self, \n",
    "            units, \n",
    "            *, \n",
    "            input_layer: bool = False,\n",
    "            activation: str = \"linear\",\n",
    "            use_bias: bool = True,\n",
    "            # optimizer: str = \"gd\" # Testing\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer.\n",
    "\n",
    "        Args:\n",
    "            units (int): Count of neurons in the layer.\n",
    "            input_layer (bool, optional): Whether the layer is an input layer. Defaults to False.\n",
    "            activation (str, optional): Activation function for the layer. Can be \"linear\", \"relu\", or \"sigmoid\". Defaults to \"linear\".\n",
    "            use_bias (bool, optional): Whether to use bias in the layer. Defaults to True.\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        self.units = units\n",
    "        self.input_layer = input_layer\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.optimizer = None # optimizer # Different optimizers for layers\n",
    "\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "\n",
    "        self.w = None # Weights matrix\n",
    "        self._weight_gradient = None # Weight derivative matrix\n",
    "        self._bias_gradient = None # Bias derivative vector\n",
    "\n",
    "        # self._adagrad_weight = None # AdaGrad optimizer's r for weight\n",
    "        # self._adagrad_bias = None # AdaGrad optimizer's r for bias\n",
    "\n",
    "    def activationFunction(self, z):\n",
    "        \"\"\"\n",
    "        Apply the activation function to the given input.\n",
    "\n",
    "        Args:\n",
    "            z (numpy.ndarray): Input to the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output after applying the activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return z\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(z, np.zeros(z.shape))\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _weightInit(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the weights matrix based on the input size.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of the input.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return # input_layer doesn't need weights\n",
    "\n",
    "        self.w = np.random.normal(loc = 0, scale = 1 / input_size, size=(input_size, self.units)) # loc -> mean, scale -> variance\n",
    "        self.bias = np.zeros((1, self.units))\n",
    "\n",
    "    def _setOptimizer(self, optimizer):\n",
    "\n",
    "        # TODO:\n",
    "        # check valid\n",
    "        #####\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            self.batch_size = 1 # SGD is the same as mini-batch gradient descent when batch-size = 1\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._adagrad_weight = np.zeros(self.w.shape)\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._adagrad_bias = np.zeros(self.bias.shape)\n",
    "\n",
    "    def _activationDerivative(self):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the activation function.\n",
    "\n",
    "        Notes:\n",
    "            Only supports the \"linear\", \"relu\", and \"sigmoid\" activation functions.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return 1\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return (self._output > 0) * 1\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self._output * (1 - self._output)\n",
    "\n",
    "    def _setGrad(self, grad):\n",
    "        \"\"\"\n",
    "        Calculate the gradients of weights and bias for backpropagation.\n",
    "\n",
    "        Args:\n",
    "            grad (numpy.ndarray): Gradient from the previous layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient to be passed to the previous layer.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "        \n",
    "        grad = grad * self._activationDerivative()\n",
    "        self._weight_gradient = self._input.T @ grad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self._bias_gradient = grad.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return grad @ self.w.T\n",
    "    \n",
    "    def _updateGrad(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and bias based on the computed gradients.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "\n",
    "        eps = 10e-8\n",
    "\n",
    "        if self.optimizer == \"gd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._adagrad_weight += self._weight_gradient ** 2\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._adagrad_weight) + eps)\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._adagrad_bias += self._bias_gradient ** 2\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._adagrad_bias) + eps)\n",
    "\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input to the layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output of the layer after applying the activation function.\n",
    "        \"\"\"\n",
    "        if self.input_layer:\n",
    "            return X\n",
    "        \n",
    "        self._input = X\n",
    "        self._output = self.activationFunction(X @ self.w + self.bias)\n",
    "\n",
    "        return self._output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeauralNetwork:\n",
    "    def __init__(\n",
    "            self, \n",
    "            layers: list, \n",
    "            loss_function: str = \"mse\", \n",
    "            learning_rate = 0.01,\n",
    "            verbose: bool = False,\n",
    "            optimizer: str = \"gd\",\n",
    "            epochs: int = 1, \n",
    "            batch_size: int = 32):\n",
    "        \"\"\"\n",
    "        Initialize a neural network.\n",
    "\n",
    "        Args:\n",
    "            layers (list): List of Layer objects defining the network architecture. \n",
    "            loss_function (str, optional): Loss function to use. Defaults to \"mse\".\n",
    "            optimizer (str, optional): Optimization algorithm to use for updating weights during training.\n",
    "                Options include:\n",
    "                - \"gd\" (Gradient Descent): Standard gradient descent.\n",
    "                // - \"sgd\" (Stochastic Gradient Descent): Update weights using a single sample at a time.\n",
    "                // - \"adam\" (Adam): Adaptive Moment Estimation algorithm.\n",
    "                Defaults to \"gd\".\n",
    "\n",
    "            learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n",
    "            max_iter (int, optional): Maximum number of iterations for training. Defaults to 1000.\n",
    "            verbose (bool, optional): Whether to display training progress. Defaults to False.\n",
    "\n",
    "            // TODO: Documentation\n",
    "            epochs:\n",
    "            batch_size:\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.verbose = verbose\n",
    "        self.optimizer = optimizer # Optimizer for all layers\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Weights initializing:\n",
    "        for i in range(len(self.layers)):\n",
    "            self.layers[i]._weightInit(self.layers[i - 1].units)\n",
    "            self.layers[i]._setOptimizer(self.optimizer)\n",
    "\n",
    "\n",
    "\n",
    "    def lossFunction(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the loss between the true values and predicted values.\n",
    "\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True values.\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 0.5 * np.mean(np.linalg.norm(y_pred - y_true, axis=1)**2)\n",
    "\n",
    "        # Can be add\n",
    "\n",
    "    def _lossFunctionDerivative(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 1 / len(y_pred) * (y_pred - y_true)\n",
    "\n",
    "        # Can be add\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the neural network on the given input-output pairs.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "            y (numpy.ndarray): Output data.\n",
    "\n",
    "        Notes:\n",
    "            Reshapes X and y to match the expected input shapes of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        batch_separation = [(X[i:i + self.batch_size], y[i:i + self.batch_size]) for i in range(0, 442, self.batch_size)]\n",
    "        epoch_len = len(batch_separation)\n",
    "\n",
    "        for _ in range(self.epochs):       \n",
    "\n",
    "            for iter, (X_, y_) in enumerate(batch_separation):\n",
    "\n",
    "                pred = self.forward(X_)\n",
    "\n",
    "                if self.verbose:\n",
    "                    process_percent = int(iter / epoch_len * 10)\n",
    "                    print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter}/{epoch_len}: [{process_percent * '=' + '>' + (10 - process_percent) * '-'}] - loss: {self.lossFunction(y_, pred)}\",end='')\n",
    "                \n",
    "                self.backward(pred, y_)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f\"\\r Epoch {_ + 1}/{self.epochs}; Batch {iter + 1}/{epoch_len}: [{11 * '='}] - loss: {self.lossFunction(y_, pred)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform predictions using the trained neural network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted output data.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        X_ = np.copy(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            X_ = layer.call(X_)\n",
    "        return X_\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to update the weights of the network.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = self._lossFunctionDerivative(y_pred, y_true)\n",
    "\n",
    "        for layer in reversed(self.layers):\n",
    "            gradient = layer._setGrad(gradient)\n",
    "            layer._updateGrad(self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch 1/1000; Batch 14/14: [===========] - loss: 10481.479467102554\n",
      " Epoch 2/1000; Batch 14/14: [===========] - loss: 9012.0816387569567\n",
      " Epoch 3/1000; Batch 14/14: [===========] - loss: 5358.0644728852344\n",
      " Epoch 4/1000; Batch 14/14: [===========] - loss: 5100.1589742471954\n",
      " Epoch 5/1000; Batch 14/14: [===========] - loss: 4673.3088579786547\n",
      " Epoch 6/1000; Batch 14/14: [===========] - loss: 4055.1930844295853\n",
      " Epoch 7/1000; Batch 14/14: [===========] - loss: 3164.0825884961264\n",
      " Epoch 8/1000; Batch 14/14: [===========] - loss: 2346.0512125749497\n",
      " Epoch 9/1000; Batch 14/14: [===========] - loss: 1779.6442340043047\n",
      " Epoch 10/1000; Batch 14/14: [===========] - loss: 1399.7322654347777\n",
      " Epoch 11/1000; Batch 14/14: [===========] - loss: 5118.1629465739817\n",
      " Epoch 12/1000; Batch 14/14: [===========] - loss: 4149.0101599395123\n",
      " Epoch 13/1000; Batch 14/14: [===========] - loss: 3578.0501467242514\n",
      " Epoch 14/1000; Batch 14/14: [===========] - loss: 3008.4908585052933\n",
      " Epoch 15/1000; Batch 14/14: [===========] - loss: 2488.1223724893143\n",
      " Epoch 16/1000; Batch 14/14: [===========] - loss: 1410.3040364750216\n",
      " Epoch 17/1000; Batch 14/14: [===========] - loss: 1247.8212262651891\n",
      " Epoch 18/1000; Batch 14/14: [===========] - loss: 1149.4128549233117\n",
      " Epoch 19/1000; Batch 14/14: [===========] - loss: 1078.8146264523097\n",
      " Epoch 20/1000; Batch 14/14: [===========] - loss: 5663.9327716350054\n",
      " Epoch 21/1000; Batch 14/14: [===========] - loss: 3090.3801902405614\n",
      " Epoch 22/1000; Batch 14/14: [===========] - loss: 3153.8320309578994\n",
      " Epoch 23/1000; Batch 14/14: [===========] - loss: 3594.4111309272503\n",
      " Epoch 24/1000; Batch 14/14: [===========] - loss: 3922.1184672755523\n",
      " Epoch 25/1000; Batch 14/14: [===========] - loss: 3121.762137709096\n",
      " Epoch 26/1000; Batch 14/14: [===========] - loss: 6646.3434443554392\n",
      " Epoch 27/1000; Batch 14/14: [===========] - loss: 2974.0959071775815\n",
      " Epoch 28/1000; Batch 14/14: [===========] - loss: 3188.8319196912936\n",
      " Epoch 29/1000; Batch 14/14: [===========] - loss: 3204.5750424577923\n",
      " Epoch 30/1000; Batch 14/14: [===========] - loss: 3192.8600397243927\n",
      " Epoch 31/1000; Batch 14/14: [===========] - loss: 3172.5182761307133\n",
      " Epoch 32/1000; Batch 14/14: [===========] - loss: 3172.2924184674913\n",
      " Epoch 33/1000; Batch 14/14: [===========] - loss: 3172.635995616015\n",
      " Epoch 34/1000; Batch 14/14: [===========] - loss: 3173.3071844248457\n",
      " Epoch 35/1000; Batch 14/14: [===========] - loss: 3175.5695865469597\n",
      " Epoch 36/1000; Batch 14/14: [===========] - loss: 3183.6673869849724\n",
      " Epoch 37/1000; Batch 14/14: [===========] - loss: 3201.4347676648676\n",
      " Epoch 38/1000; Batch 14/14: [===========] - loss: 3225.3713353737662\n",
      " Epoch 39/1000; Batch 14/14: [===========] - loss: 3182.5758976202866\n",
      " Epoch 40/1000; Batch 14/14: [===========] - loss: 3390.3286867388043\n",
      " Epoch 41/1000; Batch 14/14: [===========] - loss: 3931.3826303978476\n",
      " Epoch 42/1000; Batch 14/14: [===========] - loss: 3165.3609780402216\n",
      " Epoch 43/1000; Batch 14/14: [===========] - loss: 3207.3333861996657\n",
      " Epoch 44/1000; Batch 14/14: [===========] - loss: 3233.4082302381767\n",
      " Epoch 45/1000; Batch 14/14: [===========] - loss: 3174.2781564555835\n",
      " Epoch 46/1000; Batch 14/14: [===========] - loss: 3854.4599491688487\n",
      " Epoch 47/1000; Batch 14/14: [===========] - loss: 3767.9684420423728\n",
      " Epoch 48/1000; Batch 14/14: [===========] - loss: 3488.3144792987296\n",
      " Epoch 49/1000; Batch 14/14: [===========] - loss: 4241.3354217077213\n",
      " Epoch 50/1000; Batch 14/14: [===========] - loss: 3151.4440683148796\n",
      " Epoch 51/1000; Batch 14/14: [===========] - loss: 3159.5402483928106\n",
      " Epoch 52/1000; Batch 14/14: [===========] - loss: 4086.9540234139563\n",
      " Epoch 53/1000; Batch 14/14: [===========] - loss: 3122.7731237426156\n",
      " Epoch 54/1000; Batch 14/14: [===========] - loss: 4650.9368142624194\n",
      " Epoch 55/1000; Batch 14/14: [===========] - loss: 3077.0777561560294\n",
      " Epoch 56/1000; Batch 14/14: [===========] - loss: 3076.5380015713577\n",
      " Epoch 57/1000; Batch 14/14: [===========] - loss: 3475.8729368016753\n",
      " Epoch 58/1000; Batch 14/14: [===========] - loss: 2624.1002533575775\n",
      " Epoch 59/1000; Batch 14/14: [===========] - loss: 4022.5941497956521\n",
      " Epoch 60/1000; Batch 14/14: [===========] - loss: 2955.7056356846498\n",
      " Epoch 61/1000; Batch 14/14: [===========] - loss: 3753.2640409431273\n",
      " Epoch 62/1000; Batch 14/14: [===========] - loss: 3510.8961498974018\n",
      " Epoch 63/1000; Batch 14/14: [===========] - loss: 3094.2963359268943\n",
      " Epoch 64/1000; Batch 14/14: [===========] - loss: 2723.6986677367863\n",
      " Epoch 65/1000; Batch 14/14: [===========] - loss: 1825.8754806854606\n",
      " Epoch 66/1000; Batch 14/14: [===========] - loss: 1518.8844024745164\n",
      " Epoch 67/1000; Batch 14/14: [===========] - loss: 1345.5601135248107\n",
      " Epoch 68/1000; Batch 14/14: [===========] - loss: 1156.1465855050342\n",
      " Epoch 69/1000; Batch 14/14: [===========] - loss: 1135.4014979003033\n",
      " Epoch 70/1000; Batch 14/14: [===========] - loss: 3022.4388950746913\n",
      " Epoch 71/1000; Batch 14/14: [===========] - loss: 1434.8327511342857\n",
      " Epoch 72/1000; Batch 14/14: [===========] - loss: 1519.6019330189924\n",
      " Epoch 73/1000; Batch 14/14: [===========] - loss: 1217.9232266258493\n",
      " Epoch 74/1000; Batch 14/14: [===========] - loss: 1319.3193824414474\n",
      " Epoch 75/1000; Batch 14/14: [===========] - loss: 3840.9574204919672\n",
      " Epoch 76/1000; Batch 14/14: [===========] - loss: 2099.8961293858074\n",
      " Epoch 77/1000; Batch 14/14: [===========] - loss: 1967.1958600349217\n",
      " Epoch 78/1000; Batch 14/14: [===========] - loss: 2455.1261345386797\n",
      " Epoch 79/1000; Batch 14/14: [===========] - loss: 2028.0157594688785\n",
      " Epoch 80/1000; Batch 14/14: [===========] - loss: 2112.7160673552016\n",
      " Epoch 81/1000; Batch 14/14: [===========] - loss: 2020.3615174123166\n",
      " Epoch 82/1000; Batch 14/14: [===========] - loss: 2118.9710846115998\n",
      " Epoch 83/1000; Batch 14/14: [===========] - loss: 2262.7038294959452\n",
      " Epoch 84/1000; Batch 14/14: [===========] - loss: 2645.2901237746823\n",
      " Epoch 85/1000; Batch 14/14: [===========] - loss: 2429.5271561724414\n",
      " Epoch 86/1000; Batch 14/14: [===========] - loss: 2037.0688189196646\n",
      " Epoch 87/1000; Batch 14/14: [===========] - loss: 1889.1582655067418\n",
      " Epoch 88/1000; Batch 14/14: [===========] - loss: 2227.1488164917673\n",
      " Epoch 89/1000; Batch 14/14: [===========] - loss: 2338.9719024564492\n",
      " Epoch 90/1000; Batch 14/14: [===========] - loss: 2062.4540875248516\n",
      " Epoch 91/1000; Batch 14/14: [===========] - loss: 1983.5318803988014\n",
      " Epoch 92/1000; Batch 14/14: [===========] - loss: 1932.9806404292249\n",
      " Epoch 93/1000; Batch 14/14: [===========] - loss: 1929.7621950199043\n",
      " Epoch 94/1000; Batch 14/14: [===========] - loss: 1935.1475063920964\n",
      " Epoch 95/1000; Batch 14/14: [===========] - loss: 1949.9571852769218\n",
      " Epoch 96/1000; Batch 14/14: [===========] - loss: 1888.1534045379163\n",
      " Epoch 97/1000; Batch 14/14: [===========] - loss: 1897.6125826957314\n",
      " Epoch 98/1000; Batch 14/14: [===========] - loss: 1905.8614677357266\n",
      " Epoch 99/1000; Batch 14/14: [===========] - loss: 1738.8518457807588\n",
      " Epoch 100/1000; Batch 14/14: [===========] - loss: 1977.7345956947997\n",
      " Epoch 101/1000; Batch 14/14: [===========] - loss: 2033.0013709804966\n",
      " Epoch 102/1000; Batch 14/14: [===========] - loss: 2046.3269890809158\n",
      " Epoch 103/1000; Batch 14/14: [===========] - loss: 2058.4320127472756\n",
      " Epoch 104/1000; Batch 14/14: [===========] - loss: 2068.1208521857847\n",
      " Epoch 105/1000; Batch 14/14: [===========] - loss: 2068.1541446445735\n",
      " Epoch 106/1000; Batch 14/14: [===========] - loss: 2123.2229667157346\n",
      " Epoch 107/1000; Batch 14/14: [===========] - loss: 1770.7077399073705\n",
      " Epoch 108/1000; Batch 14/14: [===========] - loss: 2051.7124197244893\n",
      " Epoch 109/1000; Batch 14/14: [===========] - loss: 2088.1770884622198\n",
      " Epoch 110/1000; Batch 14/14: [===========] - loss: 1934.8323429723705\n",
      " Epoch 111/1000; Batch 14/14: [===========] - loss: 1925.9492768096713\n",
      " Epoch 112/1000; Batch 14/14: [===========] - loss: 1918.7613140845547\n",
      " Epoch 113/1000; Batch 14/14: [===========] - loss: 1956.4505918826076\n",
      " Epoch 114/1000; Batch 14/14: [===========] - loss: 1968.9625121939035\n",
      " Epoch 115/1000; Batch 14/14: [===========] - loss: 1963.1593673580345\n",
      " Epoch 116/1000; Batch 14/14: [===========] - loss: 1956.2689282309084\n",
      " Epoch 117/1000; Batch 14/14: [===========] - loss: 1947.6985333725559\n",
      " Epoch 118/1000; Batch 14/14: [===========] - loss: 1940.4074153563026\n",
      " Epoch 119/1000; Batch 14/14: [===========] - loss: 1932.6452394967969\n",
      " Epoch 120/1000; Batch 14/14: [===========] - loss: 1826.8556419827744\n",
      " Epoch 121/1000; Batch 14/14: [===========] - loss: 1737.6757887079066\n",
      " Epoch 122/1000; Batch 14/14: [===========] - loss: 1725.3303880493359\n",
      " Epoch 123/1000; Batch 14/14: [===========] - loss: 1721.5369456804526\n",
      " Epoch 124/1000; Batch 14/14: [===========] - loss: 1717.8782003954836\n",
      " Epoch 125/1000; Batch 14/14: [===========] - loss: 1714.2221178584514\n",
      " Epoch 126/1000; Batch 14/14: [===========] - loss: 1710.6846132892815\n",
      " Epoch 127/1000; Batch 14/14: [===========] - loss: 1707.0498650685722\n",
      " Epoch 128/1000; Batch 14/14: [===========] - loss: 1703.2526825569742\n",
      " Epoch 129/1000; Batch 14/14: [===========] - loss: 1699.6529069239435\n",
      " Epoch 130/1000; Batch 14/14: [===========] - loss: 1861.4547244979192\n",
      " Epoch 131/1000; Batch 14/14: [===========] - loss: 1757.6189811047678\n",
      " Epoch 132/1000; Batch 14/14: [===========] - loss: 1843.3074738580278\n",
      " Epoch 133/1000; Batch 14/14: [===========] - loss: 1741.3390648580319\n",
      " Epoch 134/1000; Batch 14/14: [===========] - loss: 1672.8108899554522\n",
      " Epoch 135/1000; Batch 14/14: [===========] - loss: 1829.2754467138707\n",
      " Epoch 136/1000; Batch 14/14: [===========] - loss: 1723.2507554166639\n",
      " Epoch 137/1000; Batch 14/14: [===========] - loss: 1814.0210437555704\n",
      " Epoch 138/1000; Batch 14/14: [===========] - loss: 1804.3364659040444\n",
      " Epoch 139/1000; Batch 14/14: [===========] - loss: 1796.9027964412558\n",
      " Epoch 140/1000; Batch 14/14: [===========] - loss: 1789.7965998376892\n",
      " Epoch 141/1000; Batch 14/14: [===========] - loss: 1783.0309291395872\n",
      " Epoch 142/1000; Batch 14/14: [===========] - loss: 1775.8937354564036\n",
      " Epoch 143/1000; Batch 14/14: [===========] - loss: 1769.4225411004527\n",
      " Epoch 144/1000; Batch 14/14: [===========] - loss: 1762.7076248590712\n",
      " Epoch 145/1000; Batch 14/14: [===========] - loss: 1756.3619965820027\n",
      " Epoch 146/1000; Batch 14/14: [===========] - loss: 1750.2195820555883\n",
      " Epoch 147/1000; Batch 14/14: [===========] - loss: 1743.8366601923456\n",
      " Epoch 148/1000; Batch 14/14: [===========] - loss: 1738.0186076218868\n",
      " Epoch 149/1000; Batch 14/14: [===========] - loss: 1732.3932575867236\n",
      " Epoch 150/1000; Batch 14/14: [===========] - loss: 1726.3480998048278\n",
      " Epoch 151/1000; Batch 14/14: [===========] - loss: 1720.2562669582908\n",
      " Epoch 152/1000; Batch 14/14: [===========] - loss: 1673.2564473426282\n",
      " Epoch 153/1000; Batch 14/14: [===========] - loss: 1669.8924891298848\n",
      " Epoch 154/1000; Batch 14/14: [===========] - loss: 1712.4782833317902\n",
      " Epoch 155/1000; Batch 14/14: [===========] - loss: 1660.6248295985679\n",
      " Epoch 156/1000; Batch 14/14: [===========] - loss: 1661.1636539287256\n",
      " Epoch 157/1000; Batch 14/14: [===========] - loss: 1657.5096801285274\n",
      " Epoch 158/1000; Batch 14/14: [===========] - loss: 1612.1309245279494\n",
      " Epoch 159/1000; Batch 14/14: [===========] - loss: 1641.1831784758194\n",
      " Epoch 160/1000; Batch 14/14: [===========] - loss: 1645.7640231826755\n",
      " Epoch 161/1000; Batch 14/14: [===========] - loss: 1640.8071502323905\n",
      " Epoch 162/1000; Batch 14/14: [===========] - loss: 1637.5768973288093\n",
      " Epoch 163/1000; Batch 14/14: [===========] - loss: 1595.3187241486075\n",
      " Epoch 164/1000; Batch 14/14: [===========] - loss: 1622.0585657881047\n",
      " Epoch 165/1000; Batch 14/14: [===========] - loss: 1552.0074905369843\n",
      " Epoch 166/1000; Batch 14/14: [===========] - loss: 1617.6653741132884\n",
      " Epoch 167/1000; Batch 14/14: [===========] - loss: 1545.0727601706265\n",
      " Epoch 168/1000; Batch 14/14: [===========] - loss: 1610.2736435566171\n",
      " Epoch 169/1000; Batch 14/14: [===========] - loss: 1621.3723002390689\n",
      " Epoch 170/1000; Batch 14/14: [===========] - loss: 1616.7596167581437\n",
      " Epoch 171/1000; Batch 14/14: [===========] - loss: 1612.4375016653138\n",
      " Epoch 172/1000; Batch 14/14: [===========] - loss: 1597.8250649678348\n",
      " Epoch 173/1000; Batch 14/14: [===========] - loss: 1606.3231976613786\n",
      " Epoch 174/1000; Batch 14/14: [===========] - loss: 1603.6628939011873\n",
      " Epoch 175/1000; Batch 14/14: [===========] - loss: 1586.8095937046494\n",
      " Epoch 176/1000; Batch 14/14: [===========] - loss: 1596.3985156758908\n",
      " Epoch 177/1000; Batch 14/14: [===========] - loss: 1583.6156752654022\n",
      " Epoch 178/1000; Batch 14/14: [===========] - loss: 1589.9545203131539\n",
      " Epoch 179/1000; Batch 14/14: [===========] - loss: 1577.7186663147425\n",
      " Epoch 180/1000; Batch 14/14: [===========] - loss: 1581.0962195269337\n",
      " Epoch 181/1000; Batch 14/14: [===========] - loss: 1577.0069266043045\n",
      " Epoch 182/1000; Batch 14/14: [===========] - loss: 1568.0534952889402\n",
      " Epoch 183/1000; Batch 14/14: [===========] - loss: 1572.0972119810235\n",
      " Epoch 184/1000; Batch 14/14: [===========] - loss: 1563.0008117008283\n",
      " Epoch 185/1000; Batch 14/14: [===========] - loss: 1567.0840300537843\n",
      " Epoch 186/1000; Batch 14/14: [===========] - loss: 1558.5029430329714\n",
      " Epoch 187/1000; Batch 14/14: [===========] - loss: 1554.9326912198672\n",
      " Epoch 188/1000; Batch 14/14: [===========] - loss: 1559.2831364853857\n",
      " Epoch 189/1000; Batch 14/14: [===========] - loss: 1551.2626325606111\n",
      " Epoch 190/1000; Batch 14/14: [===========] - loss: 1549.4297807821388\n",
      " Epoch 191/1000; Batch 14/14: [===========] - loss: 1550.4173809496315\n",
      " Epoch 192/1000; Batch 14/14: [===========] - loss: 1542.7735828551786\n",
      " Epoch 193/1000; Batch 14/14: [===========] - loss: 1542.5913162207364\n",
      " Epoch 194/1000; Batch 14/14: [===========] - loss: 1539.0559980778238\n",
      " Epoch 195/1000; Batch 14/14: [===========] - loss: 1542.0318481483218\n",
      " Epoch 196/1000; Batch 14/14: [===========] - loss: 1534.2027215802211\n",
      " Epoch 197/1000; Batch 14/14: [===========] - loss: 1532.7733423432053\n",
      " Epoch 198/1000; Batch 14/14: [===========] - loss: 1534.4418733004874\n",
      " Epoch 199/1000; Batch 14/14: [===========] - loss: 1527.7078258094825\n",
      " Epoch 200/1000; Batch 14/14: [===========] - loss: 1525.7018610196772\n",
      " Epoch 201/1000; Batch 14/14: [===========] - loss: 1524.3904736926636\n",
      " Epoch 202/1000; Batch 14/14: [===========] - loss: 1521.9683265116052\n",
      " Epoch 203/1000; Batch 14/14: [===========] - loss: 1520.2853662996083\n",
      " Epoch 204/1000; Batch 14/14: [===========] - loss: 1518.3074368461141\n",
      " Epoch 205/1000; Batch 14/14: [===========] - loss: 1516.1747808444632\n",
      " Epoch 206/1000; Batch 14/14: [===========] - loss: 1514.2215638091825\n",
      " Epoch 207/1000; Batch 14/14: [===========] - loss: 1512.4641205951261\n",
      " Epoch 208/1000; Batch 14/14: [===========] - loss: 1529.8886230201342\n",
      " Epoch 209/1000; Batch 14/14: [===========] - loss: 1525.2953286091101\n",
      " Epoch 210/1000; Batch 14/14: [===========] - loss: 1506.9285876085623\n",
      " Epoch 211/1000; Batch 14/14: [===========] - loss: 1525.1733976641326\n",
      " Epoch 212/1000; Batch 14/14: [===========] - loss: 1521.7416521020239\n",
      " Epoch 213/1000; Batch 14/14: [===========] - loss: 1503.3470597320763\n",
      " Epoch 214/1000; Batch 14/14: [===========] - loss: 1506.8817186256126\n",
      " Epoch 215/1000; Batch 14/14: [===========] - loss: 1504.7347030963126\n",
      " Epoch 216/1000; Batch 14/14: [===========] - loss: 1502.7083943267066\n",
      " Epoch 217/1000; Batch 14/14: [===========] - loss: 1488.1756177526666\n",
      " Epoch 218/1000; Batch 14/14: [===========] - loss: 1498.2556489396409\n",
      " Epoch 219/1000; Batch 14/14: [===========] - loss: 1485.6375491266656\n",
      " Epoch 220/1000; Batch 14/14: [===========] - loss: 1497.8851113528667\n",
      " Epoch 221/1000; Batch 14/14: [===========] - loss: 1500.4359304856616\n",
      " Epoch 222/1000; Batch 14/14: [===========] - loss: 1496.3215603314607\n",
      " Epoch 223/1000; Batch 14/14: [===========] - loss: 1498.2298342344645\n",
      " Epoch 224/1000; Batch 14/14: [===========] - loss: 1492.9854509868197\n",
      " Epoch 225/1000; Batch 14/14: [===========] - loss: 1491.8960939808005\n",
      " Epoch 226/1000; Batch 14/14: [===========] - loss: 1490.0807216085782\n",
      " Epoch 227/1000; Batch 14/14: [===========] - loss: 1490.4836146154003\n",
      " Epoch 228/1000; Batch 14/14: [===========] - loss: 1487.2093237564372\n",
      " Epoch 229/1000; Batch 14/14: [===========] - loss: 1486.1747902081884\n",
      " Epoch 230/1000; Batch 14/14: [===========] - loss: 1484.1727148089472\n",
      " Epoch 231/1000; Batch 14/14: [===========] - loss: 1482.9282012117787\n",
      " Epoch 232/1000; Batch 14/14: [===========] - loss: 1481.4730035055915\n",
      " Epoch 233/1000; Batch 14/14: [===========] - loss: 1480.0543767432298\n",
      " Epoch 234/1000; Batch 14/14: [===========] - loss: 1480.5523674057285\n",
      " Epoch 235/1000; Batch 14/14: [===========] - loss: 1479.3257804044595\n",
      " Epoch 236/1000; Batch 14/14: [===========] - loss: 1581.7681087775185\n",
      " Epoch 237/1000; Batch 14/14: [===========] - loss: 1487.5786379816766\n",
      " Epoch 238/1000; Batch 14/14: [===========] - loss: 1475.6200967547265\n",
      " Epoch 239/1000; Batch 14/14: [===========] - loss: 1577.0162191222505\n",
      " Epoch 240/1000; Batch 14/14: [===========] - loss: 1483.4269528430186\n",
      " Epoch 241/1000; Batch 14/14: [===========] - loss: 1574.3419819270043\n",
      " Epoch 242/1000; Batch 14/14: [===========] - loss: 1480.8541293456578\n",
      " Epoch 243/1000; Batch 14/14: [===========] - loss: 1575.0683384147596\n",
      " Epoch 244/1000; Batch 14/14: [===========] - loss: 1478.3435479696354\n",
      " Epoch 245/1000; Batch 14/14: [===========] - loss: 1467.5926391036564\n",
      " Epoch 246/1000; Batch 14/14: [===========] - loss: 1565.2923666323227\n",
      " Epoch 247/1000; Batch 14/14: [===========] - loss: 1472.5954297430317\n",
      " Epoch 248/1000; Batch 14/14: [===========] - loss: 1569.4346065497798\n",
      " Epoch 249/1000; Batch 14/14: [===========] - loss: 1470.6326026793527\n",
      " Epoch 250/1000; Batch 14/14: [===========] - loss: 1460.0357028468186\n",
      " Epoch 251/1000; Batch 14/14: [===========] - loss: 1562.3840504247298\n",
      " Epoch 252/1000; Batch 14/14: [===========] - loss: 1463.9683401545071\n",
      " Epoch 253/1000; Batch 14/14: [===========] - loss: 1560.1164013401904\n",
      " Epoch 254/1000; Batch 14/14: [===========] - loss: 1465.5881784980143\n",
      " Epoch 255/1000; Batch 14/14: [===========] - loss: 1551.6462348464797\n",
      " Epoch 256/1000; Batch 14/14: [===========] - loss: 1460.3230890474638\n",
      " Epoch 257/1000; Batch 14/14: [===========] - loss: 1554.6827962375573\n",
      " Epoch 258/1000; Batch 14/14: [===========] - loss: 1457.0047484693548\n",
      " Epoch 259/1000; Batch 14/14: [===========] - loss: 1552.2768075725785\n",
      " Epoch 260/1000; Batch 14/14: [===========] - loss: 1454.9762810921309\n",
      " Epoch 261/1000; Batch 14/14: [===========] - loss: 1547.4385439638268\n",
      " Epoch 262/1000; Batch 14/14: [===========] - loss: 1553.9831201420068\n",
      " Epoch 263/1000; Batch 14/14: [===========] - loss: 1563.1646684539496\n",
      " Epoch 264/1000; Batch 14/14: [===========] - loss: 1550.7256746269072\n",
      " Epoch 265/1000; Batch 14/14: [===========] - loss: 1560.2878723605927\n",
      " Epoch 266/1000; Batch 14/14: [===========] - loss: 1547.5001688512534\n",
      " Epoch 267/1000; Batch 14/14: [===========] - loss: 1554.0523877706016\n",
      " Epoch 268/1000; Batch 14/14: [===========] - loss: 1554.2481180630834\n",
      " Epoch 269/1000; Batch 14/14: [===========] - loss: 1544.6959224221357\n",
      " Epoch 270/1000; Batch 14/14: [===========] - loss: 1542.0883755016582\n",
      " Epoch 271/1000; Batch 14/14: [===========] - loss: 1541.6784405896033\n",
      " Epoch 272/1000; Batch 14/14: [===========] - loss: 1541.0945265178425\n",
      " Epoch 273/1000; Batch 14/14: [===========] - loss: 1540.1586937014633\n",
      " Epoch 274/1000; Batch 14/14: [===========] - loss: 1538.5509402122066\n",
      " Epoch 275/1000; Batch 14/14: [===========] - loss: 1548.8421980433777\n",
      " Epoch 276/1000; Batch 14/14: [===========] - loss: 1536.2101889618239\n",
      " Epoch 277/1000; Batch 14/14: [===========] - loss: 1535.6634646314199\n",
      " Epoch 278/1000; Batch 14/14: [===========] - loss: 1522.6832313043328\n",
      " Epoch 279/1000; Batch 14/14: [===========] - loss: 1544.7610967235198\n",
      " Epoch 280/1000; Batch 14/14: [===========] - loss: 1555.4337880355533\n",
      " Epoch 281/1000; Batch 14/14: [===========] - loss: 1529.1646527677917\n",
      " Epoch 282/1000; Batch 14/14: [===========] - loss: 1550.1536525446866\n",
      " Epoch 283/1000; Batch 14/14: [===========] - loss: 1550.4210428871384\n",
      " Epoch 284/1000; Batch 14/14: [===========] - loss: 1518.4795864182133\n",
      " Epoch 285/1000; Batch 14/14: [===========] - loss: 1544.6687087238681\n",
      " Epoch 286/1000; Batch 14/14: [===========] - loss: 1544.7166368576557\n",
      " Epoch 287/1000; Batch 14/14: [===========] - loss: 1530.2732504277567\n",
      " Epoch 288/1000; Batch 14/14: [===========] - loss: 1538.8936313976592\n",
      " Epoch 289/1000; Batch 14/14: [===========] - loss: 1518.1466612257877\n",
      " Epoch 290/1000; Batch 14/14: [===========] - loss: 1536.4003673877592\n",
      " Epoch 291/1000; Batch 14/14: [===========] - loss: 1531.7334003000979\n",
      " Epoch 292/1000; Batch 14/14: [===========] - loss: 1527.6190089297629\n",
      " Epoch 293/1000; Batch 14/14: [===========] - loss: 1533.5626884741157\n",
      " Epoch 294/1000; Batch 14/14: [===========] - loss: 1522.5152343542397\n",
      " Epoch 295/1000; Batch 14/14: [===========] - loss: 1530.9535155713806\n",
      " Epoch 296/1000; Batch 14/14: [===========] - loss: 1516.2444723342521\n",
      " Epoch 297/1000; Batch 14/14: [===========] - loss: 1517.1181799290794\n",
      " Epoch 298/1000; Batch 14/14: [===========] - loss: 1509.9785339966625\n",
      " Epoch 299/1000; Batch 14/14: [===========] - loss: 1522.1044100396578\n",
      " Epoch 300/1000; Batch 14/14: [===========] - loss: 1520.1723330010193\n",
      " Epoch 301/1000; Batch 14/14: [===========] - loss: 1505.9206459262084\n",
      " Epoch 302/1000; Batch 14/14: [===========] - loss: 1501.8746567814017\n",
      " Epoch 303/1000; Batch 14/14: [===========] - loss: 1506.816760791369\n",
      " Epoch 304/1000; Batch 14/14: [===========] - loss: 1516.0778353119329\n",
      " Epoch 305/1000; Batch 14/14: [===========] - loss: 1514.0575446474465\n",
      " Epoch 306/1000; Batch 14/14: [===========] - loss: 1497.1367136496026\n",
      " Epoch 307/1000; Batch 14/14: [===========] - loss: 1493.7108045279834\n",
      " Epoch 308/1000; Batch 14/14: [===========] - loss: 1496.8812247986534\n",
      " Epoch 309/1000; Batch 14/14: [===========] - loss: 1494.0344596890313\n",
      " Epoch 310/1000; Batch 14/14: [===========] - loss: 1494.5076229783224\n",
      " Epoch 311/1000; Batch 14/14: [===========] - loss: 1490.9036643167308\n",
      " Epoch 312/1000; Batch 14/14: [===========] - loss: 1497.1060396700245\n",
      " Epoch 313/1000; Batch 14/14: [===========] - loss: 1489.6676794201023\n",
      " Epoch 314/1000; Batch 14/14: [===========] - loss: 1490.2561025917491\n",
      " Epoch 315/1000; Batch 14/14: [===========] - loss: 1489.0448052604153\n",
      " Epoch 316/1000; Batch 14/14: [===========] - loss: 1488.2278665544277\n",
      " Epoch 317/1000; Batch 14/14: [===========] - loss: 1487.1808391278626\n",
      " Epoch 318/1000; Batch 14/14: [===========] - loss: 1485.2349804409337\n",
      " Epoch 319/1000; Batch 14/14: [===========] - loss: 1484.3376510331664\n",
      " Epoch 320/1000; Batch 14/14: [===========] - loss: 1480.2716665931696\n",
      " Epoch 321/1000; Batch 14/14: [===========] - loss: 1480.9718899725974\n",
      " Epoch 322/1000; Batch 14/14: [===========] - loss: 1479.8986800354423\n",
      " Epoch 323/1000; Batch 14/14: [===========] - loss: 1475.8251312734685\n",
      " Epoch 324/1000; Batch 14/14: [===========] - loss: 1476.6858845405627\n",
      " Epoch 325/1000; Batch 14/14: [===========] - loss: 1473.2790818546675\n",
      " Epoch 326/1000; Batch 14/14: [===========] - loss: 1468.9263094772084\n",
      " Epoch 327/1000; Batch 14/14: [===========] - loss: 1467.7806717129142\n",
      " Epoch 328/1000; Batch 14/14: [===========] - loss: 1466.6385630904755\n",
      " Epoch 329/1000; Batch 14/14: [===========] - loss: 1465.4360161222362\n",
      " Epoch 330/1000; Batch 14/14: [===========] - loss: 1463.8917894227375\n",
      " Epoch 331/1000; Batch 14/14: [===========] - loss: 1462.4391725224437\n",
      " Epoch 332/1000; Batch 14/14: [===========] - loss: 1460.8935845624972\n",
      " Epoch 333/1000; Batch 14/14: [===========] - loss: 1459.5145921056824\n",
      " Epoch 334/1000; Batch 14/14: [===========] - loss: 1458.0295019632108\n",
      " Epoch 335/1000; Batch 14/14: [===========] - loss: 1456.7705846379333\n",
      " Epoch 336/1000; Batch 14/14: [===========] - loss: 1456.8201619992919\n",
      " Epoch 337/1000; Batch 14/14: [===========] - loss: 1453.7667470489755\n",
      " Epoch 338/1000; Batch 14/14: [===========] - loss: 1445.2407244561932\n",
      " Epoch 339/1000; Batch 14/14: [===========] - loss: 1445.1155193451655\n",
      " Epoch 340/1000; Batch 14/14: [===========] - loss: 1453.1033262878825\n",
      " Epoch 341/1000; Batch 14/14: [===========] - loss: 1443.2397658084542\n",
      " Epoch 342/1000; Batch 14/14: [===========] - loss: 1451.2507073702816\n",
      " Epoch 343/1000; Batch 14/14: [===========] - loss: 1441.0731395885345\n",
      " Epoch 344/1000; Batch 14/14: [===========] - loss: 1448.7003333426974\n",
      " Epoch 345/1000; Batch 14/14: [===========] - loss: 1438.7326474531328\n",
      " Epoch 346/1000; Batch 14/14: [===========] - loss: 1450.7195504269605\n",
      " Epoch 347/1000; Batch 14/14: [===========] - loss: 1445.7345005870147\n",
      " Epoch 348/1000; Batch 14/14: [===========] - loss: 1434.9006778851544\n",
      " Epoch 349/1000; Batch 14/14: [===========] - loss: 1447.0065523350122\n",
      " Epoch 350/1000; Batch 14/14: [===========] - loss: 1442.0741029561776\n",
      " Epoch 351/1000; Batch 14/14: [===========] - loss: 1440.7196070669536\n",
      " Epoch 352/1000; Batch 14/14: [===========] - loss: 1439.4630861346627\n",
      " Epoch 353/1000; Batch 14/14: [===========] - loss: 1438.9417144619883\n",
      " Epoch 354/1000; Batch 14/14: [===========] - loss: 1437.2943015036012\n",
      " Epoch 355/1000; Batch 14/14: [===========] - loss: 1436.5882368982311\n",
      " Epoch 356/1000; Batch 14/14: [===========] - loss: 1434.7697710741113\n",
      " Epoch 357/1000; Batch 14/14: [===========] - loss: 1434.0899347045706\n",
      " Epoch 358/1000; Batch 14/14: [===========] - loss: 1422.5271182260933\n",
      " Epoch 359/1000; Batch 14/14: [===========] - loss: 1430.1713765734012\n",
      " Epoch 360/1000; Batch 14/14: [===========] - loss: 1429.7733846847357\n",
      " Epoch 361/1000; Batch 14/14: [===========] - loss: 1419.3017492586054\n",
      " Epoch 362/1000; Batch 14/14: [===========] - loss: 1421.9670089934816\n",
      " Epoch 363/1000; Batch 14/14: [===========] - loss: 1417.1721355584705\n",
      " Epoch 364/1000; Batch 14/14: [===========] - loss: 1420.1967868209867\n",
      " Epoch 365/1000; Batch 14/14: [===========] - loss: 1422.0338812824557\n",
      " Epoch 366/1000; Batch 14/14: [===========] - loss: 1421.6450884469812\n",
      " Epoch 367/1000; Batch 14/14: [===========] - loss: 1419.9536262721306\n",
      " Epoch 368/1000; Batch 14/14: [===========] - loss: 1419.2143699977993\n",
      " Epoch 369/1000; Batch 14/14: [===========] - loss: 1417.9363828912844\n",
      " Epoch 370/1000; Batch 14/14: [===========] - loss: 1417.0778391624806\n",
      " Epoch 371/1000; Batch 14/14: [===========] - loss: 1415.9993646412145\n",
      " Epoch 372/1000; Batch 14/14: [===========] - loss: 1415.3119979617802\n",
      " Epoch 373/1000; Batch 14/14: [===========] - loss: 1413.8925282629723\n",
      " Epoch 374/1000; Batch 14/14: [===========] - loss: 1412.8871796157905\n",
      " Epoch 375/1000; Batch 14/14: [===========] - loss: 1408.8875098893343\n",
      " Epoch 376/1000; Batch 14/14: [===========] - loss: 1402.4858891825302\n",
      " Epoch 377/1000; Batch 14/14: [===========] - loss: 1417.7675129707047\n",
      " Epoch 378/1000; Batch 14/14: [===========] - loss: 1406.4463006785497\n",
      " Epoch 379/1000; Batch 14/14: [===========] - loss: 1399.2621602844165\n",
      " Epoch 380/1000; Batch 14/14: [===========] - loss: 1414.7873138756378\n",
      " Epoch 381/1000; Batch 14/14: [===========] - loss: 1403.3203465006144\n",
      " Epoch 382/1000; Batch 14/14: [===========] - loss: 1397.0819139276136\n",
      " Epoch 383/1000; Batch 14/14: [===========] - loss: 1412.6634000864071\n",
      " Epoch 384/1000; Batch 14/14: [===========] - loss: 1411.9614089136446\n",
      " Epoch 385/1000; Batch 14/14: [===========] - loss: 1400.3328489640562\n",
      " Epoch 386/1000; Batch 14/14: [===========] - loss: 1393.2027884010584\n",
      " Epoch 387/1000; Batch 14/14: [===========] - loss: 1397.0652559923533\n",
      " Epoch 388/1000; Batch 14/14: [===========] - loss: 1394.0405102353002\n",
      " Epoch 389/1000; Batch 14/14: [===========] - loss: 1390.4074935875842\n",
      " Epoch 390/1000; Batch 14/14: [===========] - loss: 1393.3950927915125\n",
      " Epoch 391/1000; Batch 14/14: [===========] - loss: 1388.9206017835988\n",
      " Epoch 392/1000; Batch 14/14: [===========] - loss: 1390.9996404827032\n",
      " Epoch 393/1000; Batch 14/14: [===========] - loss: 1387.1547052712623\n",
      " Epoch 394/1000; Batch 14/14: [===========] - loss: 1389.9130429773493\n",
      " Epoch 395/1000; Batch 14/14: [===========] - loss: 1384.6088400144677\n",
      " Epoch 396/1000; Batch 14/14: [===========] - loss: 1387.3464520747852\n",
      " Epoch 397/1000; Batch 14/14: [===========] - loss: 1383.3927163435778\n",
      " Epoch 398/1000; Batch 14/14: [===========] - loss: 1385.6425131803326\n",
      " Epoch 399/1000; Batch 14/14: [===========] - loss: 1391.4745933192859\n",
      " Epoch 400/1000; Batch 14/14: [===========] - loss: 1392.0816300634897\n",
      " Epoch 401/1000; Batch 14/14: [===========] - loss: 1390.9595736678239\n",
      " Epoch 402/1000; Batch 14/14: [===========] - loss: 1381.9056386243867\n",
      " Epoch 403/1000; Batch 14/14: [===========] - loss: 1381.6271144985176\n",
      " Epoch 404/1000; Batch 14/14: [===========] - loss: 1377.3429106673252\n",
      " Epoch 405/1000; Batch 14/14: [===========] - loss: 1380.4936077320808\n",
      " Epoch 406/1000; Batch 14/14: [===========] - loss: 1385.5984733010096\n",
      " Epoch 407/1000; Batch 14/14: [===========] - loss: 1378.2023084012783\n",
      " Epoch 408/1000; Batch 14/14: [===========] - loss: 1374.2599925346758\n",
      " Epoch 409/1000; Batch 14/14: [===========] - loss: 1376.381599364561\n",
      " Epoch 410/1000; Batch 14/14: [===========] - loss: 1308.2839315276688\n",
      " Epoch 411/1000; Batch 14/14: [===========] - loss: 1296.1759078722944\n",
      " Epoch 412/1000; Batch 14/14: [===========] - loss: 1299.6639603964882\n",
      " Epoch 413/1000; Batch 14/14: [===========] - loss: 1289.9789731422804\n",
      " Epoch 414/1000; Batch 14/14: [===========] - loss: 1295.7375312671816\n",
      " Epoch 415/1000; Batch 14/14: [===========] - loss: 1295.0901045648347\n",
      " Epoch 416/1000; Batch 14/14: [===========] - loss: 1295.2793893950168\n",
      " Epoch 417/1000; Batch 14/14: [===========] - loss: 1294.6245752774682\n",
      " Epoch 418/1000; Batch 14/14: [===========] - loss: 1284.4392312629766\n",
      " Epoch 419/1000; Batch 14/14: [===========] - loss: 1283.4733825661598\n",
      " Epoch 420/1000; Batch 14/14: [===========] - loss: 1292.6741183939691\n",
      " Epoch 421/1000; Batch 14/14: [===========] - loss: 1292.0715983381378\n",
      " Epoch 422/1000; Batch 14/14: [===========] - loss: 1291.5051721364678\n",
      " Epoch 423/1000; Batch 14/14: [===========] - loss: 1286.1732247645845\n",
      " Epoch 424/1000; Batch 14/14: [===========] - loss: 1290.3532941795556\n",
      " Epoch 425/1000; Batch 14/14: [===========] - loss: 1284.4845306970358\n",
      " Epoch 426/1000; Batch 14/14: [===========] - loss: 1284.1314520078772\n",
      " Epoch 427/1000; Batch 14/14: [===========] - loss: 1277.8904517959538\n",
      " Epoch 428/1000; Batch 14/14: [===========] - loss: 1277.6085099567604\n",
      " Epoch 429/1000; Batch 14/14: [===========] - loss: 1278.0419774071067\n",
      " Epoch 430/1000; Batch 14/14: [===========] - loss: 1277.4021716602754\n",
      " Epoch 431/1000; Batch 14/14: [===========] - loss: 1275.0343317968252\n",
      " Epoch 432/1000; Batch 14/14: [===========] - loss: 1275.8641064022918\n",
      " Epoch 433/1000; Batch 14/14: [===========] - loss: 1275.1370105931642\n",
      " Epoch 434/1000; Batch 14/14: [===========] - loss: 1276.1489801737744\n",
      " Epoch 435/1000; Batch 14/14: [===========] - loss: 1271.6795378577674\n",
      " Epoch 436/1000; Batch 14/14: [===========] - loss: 1280.6191167558437\n",
      " Epoch 437/1000; Batch 14/14: [===========] - loss: 1280.4319207063725\n",
      " Epoch 438/1000; Batch 14/14: [===========] - loss: 1270.9385073893754\n",
      " Epoch 439/1000; Batch 14/14: [===========] - loss: 1270.1907989330051\n",
      " Epoch 440/1000; Batch 14/14: [===========] - loss: 1270.0831258616577\n",
      " Epoch 441/1000; Batch 14/14: [===========] - loss: 1278.1249485299045\n",
      " Epoch 442/1000; Batch 14/14: [===========] - loss: 1277.7757295616614\n",
      " Epoch 443/1000; Batch 14/14: [===========] - loss: 1270.4125171786031\n",
      " Epoch 444/1000; Batch 14/14: [===========] - loss: 1267.0812979959223\n",
      " Epoch 445/1000; Batch 14/14: [===========] - loss: 1266.4770808326735\n",
      " Epoch 446/1000; Batch 14/14: [===========] - loss: 1265.0752809267763\n",
      " Epoch 447/1000; Batch 14/14: [===========] - loss: 1265.4929816196293\n",
      " Epoch 448/1000; Batch 14/14: [===========] - loss: 1270.1671666064149\n",
      " Epoch 449/1000; Batch 14/14: [===========] - loss: 1264.6089389006588\n",
      " Epoch 450/1000; Batch 14/14: [===========] - loss: 1263.6631067682001\n",
      " Epoch 451/1000; Batch 14/14: [===========] - loss: 1265.6127322169718\n",
      " Epoch 452/1000; Batch 14/14: [===========] - loss: 1265.7070675502873\n",
      " Epoch 453/1000; Batch 14/14: [===========] - loss: 1258.8511847522188\n",
      " Epoch 454/1000; Batch 14/14: [===========] - loss: 1267.8128540466303\n",
      " Epoch 455/1000; Batch 14/14: [===========] - loss: 1260.5287746894392\n",
      " Epoch 456/1000; Batch 14/14: [===========] - loss: 1259.7062560757445\n",
      " Epoch 457/1000; Batch 14/14: [===========] - loss: 1267.4208533106172\n",
      " Epoch 458/1000; Batch 14/14: [===========] - loss: 1258.3308824332871\n",
      " Epoch 459/1000; Batch 14/14: [===========] - loss: 1256.8922929862113\n",
      " Epoch 460/1000; Batch 14/14: [===========] - loss: 1254.7586302476766\n",
      " Epoch 461/1000; Batch 14/14: [===========] - loss: 1257.2350435695332\n",
      " Epoch 462/1000; Batch 14/14: [===========] - loss: 1255.9445116504944\n",
      " Epoch 463/1000; Batch 14/14: [===========] - loss: 1258.4892678351196\n",
      " Epoch 464/1000; Batch 14/14: [===========] - loss: 1251.4662918975084\n",
      " Epoch 465/1000; Batch 14/14: [===========] - loss: 1263.6198814998863\n",
      " Epoch 466/1000; Batch 14/14: [===========] - loss: 1256.3813270902444\n",
      " Epoch 467/1000; Batch 14/14: [===========] - loss: 1249.2845811317325\n",
      " Epoch 468/1000; Batch 14/14: [===========] - loss: 1252.7477153669993\n",
      " Epoch 469/1000; Batch 14/14: [===========] - loss: 1254.6864588852394\n",
      " Epoch 470/1000; Batch 14/14: [===========] - loss: 1225.0344328714245\n",
      " Epoch 471/1000; Batch 14/14: [===========] - loss: 1224.4718446625536\n",
      " Epoch 472/1000; Batch 14/14: [===========] - loss: 1225.9338380801596\n",
      " Epoch 473/1000; Batch 14/14: [===========] - loss: 1220.8484672242798\n",
      " Epoch 474/1000; Batch 14/14: [===========] - loss: 1226.4769223717917\n",
      " Epoch 475/1000; Batch 14/14: [===========] - loss: 1219.5821341271499\n",
      " Epoch 476/1000; Batch 14/14: [===========] - loss: 1223.2898706504752\n",
      " Epoch 477/1000; Batch 14/14: [===========] - loss: 1232.1724034752149\n",
      " Epoch 478/1000; Batch 14/14: [===========] - loss: 1232.4334169735725\n",
      " Epoch 479/1000; Batch 14/14: [===========] - loss: 1215.6628583887284\n",
      " Epoch 480/1000; Batch 14/14: [===========] - loss: 1220.9907579900946\n",
      " Epoch 481/1000; Batch 14/14: [===========] - loss: 1204.5462853001548\n",
      " Epoch 482/1000; Batch 14/14: [===========] - loss: 1210.1402637645484\n",
      " Epoch 483/1000; Batch 14/14: [===========] - loss: 1208.5350327759963\n",
      " Epoch 484/1000; Batch 14/14: [===========] - loss: 1209.1082601885678\n",
      " Epoch 485/1000; Batch 14/14: [===========] - loss: 1209.2100677053696\n",
      " Epoch 486/1000; Batch 14/14: [===========] - loss: 1214.7071406077948\n",
      " Epoch 487/1000; Batch 14/14: [===========] - loss: 1204.6325761651724\n",
      " Epoch 488/1000; Batch 14/14: [===========] - loss: 1212.4638259173341\n",
      " Epoch 489/1000; Batch 14/14: [===========] - loss: 1208.2662083840814\n",
      " Epoch 490/1000; Batch 14/14: [===========] - loss: 1208.9674951366343\n",
      " Epoch 491/1000; Batch 14/14: [===========] - loss: 1211.3496776868747\n",
      " Epoch 492/1000; Batch 14/14: [===========] - loss: 1207.5564505989126\n",
      " Epoch 493/1000; Batch 14/14: [===========] - loss: 1197.2917660340022\n",
      " Epoch 494/1000; Batch 14/14: [===========] - loss: 1186.9556398738443\n",
      " Epoch 495/1000; Batch 14/14: [===========] - loss: 1189.6217558166607\n",
      " Epoch 496/1000; Batch 14/14: [===========] - loss: 1185.9516326729572\n",
      " Epoch 497/1000; Batch 14/14: [===========] - loss: 1183.8043739618267\n",
      " Epoch 498/1000; Batch 14/14: [===========] - loss: 1183.5871278079053\n",
      " Epoch 499/1000; Batch 14/14: [===========] - loss: 1179.7624134382826\n",
      " Epoch 500/1000; Batch 14/14: [===========] - loss: 1174.4680889061246\n",
      " Epoch 501/1000; Batch 14/14: [===========] - loss: 1182.1199396810723\n",
      " Epoch 502/1000; Batch 14/14: [===========] - loss: 1165.0494306028609\n",
      " Epoch 503/1000; Batch 14/14: [===========] - loss: 1180.5265991596978\n",
      " Epoch 504/1000; Batch 14/14: [===========] - loss: 1178.5306759013384\n",
      " Epoch 505/1000; Batch 14/14: [===========] - loss: 1178.2188122610946\n",
      " Epoch 506/1000; Batch 14/14: [===========] - loss: 1166.375272706583\n",
      " Epoch 507/1000; Batch 14/14: [===========] - loss: 1177.4270664006946\n",
      " Epoch 508/1000; Batch 14/14: [===========] - loss: 1184.4580040556066\n",
      " Epoch 509/1000; Batch 14/14: [===========] - loss: 1196.0735144635914\n",
      " Epoch 510/1000; Batch 14/14: [===========] - loss: 1201.8212399363897\n",
      " Epoch 511/1000; Batch 14/14: [===========] - loss: 1160.5844627977237\n",
      " Epoch 512/1000; Batch 14/14: [===========] - loss: 1176.8198225667752\n",
      " Epoch 513/1000; Batch 14/14: [===========] - loss: 1155.3597269650359\n",
      " Epoch 514/1000; Batch 14/14: [===========] - loss: 1136.0643516565436\n",
      " Epoch 515/1000; Batch 14/14: [===========] - loss: 1156.0347804322132\n",
      " Epoch 516/1000; Batch 14/14: [===========] - loss: 1158.9481799123719\n",
      " Epoch 517/1000; Batch 14/14: [===========] - loss: 1144.7193533506597\n",
      " Epoch 518/1000; Batch 14/14: [===========] - loss: 1143.4195189139173\n",
      " Epoch 519/1000; Batch 14/14: [===========] - loss: 1142.4462229934347\n",
      " Epoch 520/1000; Batch 14/14: [===========] - loss: 1138.2968830114141\n",
      " Epoch 521/1000; Batch 14/14: [===========] - loss: 1121.0493127984444\n",
      " Epoch 522/1000; Batch 14/14: [===========] - loss: 1133.9287717118955\n",
      " Epoch 523/1000; Batch 14/14: [===========] - loss: 1135.8464055745458\n",
      " Epoch 524/1000; Batch 14/14: [===========] - loss: 1103.4482175570208\n",
      " Epoch 525/1000; Batch 14/14: [===========] - loss: 1078.3166460962082\n",
      " Epoch 526/1000; Batch 14/14: [===========] - loss: 1088.4513120259295\n",
      " Epoch 527/1000; Batch 14/14: [===========] - loss: 1086.4510254746485\n",
      " Epoch 528/1000; Batch 14/14: [===========] - loss: 1087.0562082512456\n",
      " Epoch 529/1000; Batch 14/14: [===========] - loss: 1088.9028981857883\n",
      " Epoch 530/1000; Batch 14/14: [===========] - loss: 1072.5317003114341\n",
      " Epoch 531/1000; Batch 14/14: [===========] - loss: 1070.2301397964927\n",
      " Epoch 532/1000; Batch 14/14: [===========] - loss: 1079.1218902597043\n",
      " Epoch 533/1000; Batch 14/14: [===========] - loss: 1064.8232186889736\n",
      " Epoch 534/1000; Batch 14/14: [===========] - loss: 1065.8238134554874\n",
      " Epoch 535/1000; Batch 14/14: [===========] - loss: 1059.2015430860401\n",
      " Epoch 536/1000; Batch 14/14: [===========] - loss: 1057.4905776897531\n",
      " Epoch 537/1000; Batch 14/14: [===========] - loss: 1071.9098445580757\n",
      " Epoch 538/1000; Batch 14/14: [===========] - loss: 1053.7213103704075\n",
      " Epoch 539/1000; Batch 14/14: [===========] - loss: 1052.1386864486797\n",
      " Epoch 540/1000; Batch 14/14: [===========] - loss: 1050.9294027970645\n",
      " Epoch 541/1000; Batch 14/14: [===========] - loss: 1066.6204409257386\n",
      " Epoch 542/1000; Batch 14/14: [===========] - loss: 1047.8949591999256\n",
      " Epoch 543/1000; Batch 14/14: [===========] - loss: 1055.1744177147955\n",
      " Epoch 544/1000; Batch 14/14: [===========] - loss: 1050.2295132397082\n",
      " Epoch 545/1000; Batch 14/14: [===========] - loss: 1049.0263539918387\n",
      " Epoch 546/1000; Batch 14/14: [===========] - loss: 1046.5357421088272\n",
      " Epoch 547/1000; Batch 14/14: [===========] - loss: 1048.3526600394856\n",
      " Epoch 548/1000; Batch 14/14: [===========] - loss: 1042.0915553781235\n",
      " Epoch 549/1000; Batch 14/14: [===========] - loss: 1047.9390222199898\n",
      " Epoch 550/1000; Batch 14/14: [===========] - loss: 1039.9436438263797\n",
      " Epoch 551/1000; Batch 14/14: [===========] - loss: 1031.275502360208\n",
      " Epoch 552/1000; Batch 14/14: [===========] - loss: 1061.3632533528298\n",
      " Epoch 553/1000; Batch 14/14: [===========] - loss: 1035.3137388315147\n",
      " Epoch 554/1000; Batch 14/14: [===========] - loss: 1034.1027890769296\n",
      " Epoch 555/1000; Batch 14/14: [===========] - loss: 1020.8951967683364\n",
      " Epoch 556/1000; Batch 14/14: [===========] - loss: 1030.2910642621255\n",
      " Epoch 557/1000; Batch 14/14: [===========] - loss: 1015.1326298941392\n",
      " Epoch 558/1000; Batch 14/14: [===========] - loss: 1027.4544300001412\n",
      " Epoch 559/1000; Batch 14/14: [===========] - loss: 1012.0415879943866\n",
      " Epoch 560/1000; Batch 14/14: [===========] - loss: 1028.5785142993818\n",
      " Epoch 561/1000; Batch 14/14: [===========] - loss: 1017.2516406702433\n",
      " Epoch 562/1000; Batch 14/14: [===========] - loss: 1014.8022375651934\n",
      " Epoch 563/1000; Batch 14/14: [===========] - loss: 1014.9381673481683\n",
      " Epoch 564/1000; Batch 14/14: [===========] - loss: 1010.0136282599464\n",
      " Epoch 565/1000; Batch 14/14: [===========] - loss: 1008.4685222166954\n",
      " Epoch 566/1000; Batch 14/14: [===========] - loss: 1011.2821401730637\n",
      " Epoch 567/1000; Batch 14/14: [===========] - loss: 1028.5555479038792\n",
      " Epoch 568/1000; Batch 14/14: [===========] - loss: 1014.3861992256394\n",
      " Epoch 569/1000; Batch 14/14: [===========] - loss: 1012.8637930553188\n",
      " Epoch 570/1000; Batch 14/14: [===========] - loss: 942.47733436563264\n",
      " Epoch 571/1000; Batch 14/14: [===========] - loss: 1014.6396429791787\n",
      " Epoch 572/1000; Batch 14/14: [===========] - loss: 932.44907414761135\n",
      " Epoch 573/1000; Batch 14/14: [===========] - loss: 1010.6798228112298\n",
      " Epoch 574/1000; Batch 14/14: [===========] - loss: 936.81486480676846\n",
      " Epoch 575/1000; Batch 14/14: [===========] - loss: 1016.6508072902624\n",
      " Epoch 576/1000; Batch 14/14: [===========] - loss: 959.17068943219327\n",
      " Epoch 577/1000; Batch 14/14: [===========] - loss: 947.50933720982824\n",
      " Epoch 578/1000; Batch 14/14: [===========] - loss: 937.46750395444623\n",
      " Epoch 579/1000; Batch 14/14: [===========] - loss: 1018.2106206014099\n",
      " Epoch 580/1000; Batch 14/14: [===========] - loss: 946.80910565807182\n",
      " Epoch 581/1000; Batch 14/14: [===========] - loss: 946.48359428424624\n",
      " Epoch 582/1000; Batch 14/14: [===========] - loss: 946.70808130471059\n",
      " Epoch 583/1000; Batch 14/14: [===========] - loss: 945.56177744270368\n",
      " Epoch 584/1000; Batch 14/14: [===========] - loss: 945.15795376799997\n",
      " Epoch 585/1000; Batch 14/14: [===========] - loss: 942.47183587901623\n",
      " Epoch 586/1000; Batch 14/14: [===========] - loss: 940.83733852799469\n",
      " Epoch 587/1000; Batch 14/14: [===========] - loss: 939.97519863495174\n",
      " Epoch 588/1000; Batch 14/14: [===========] - loss: 940.11577770871682\n",
      " Epoch 589/1000; Batch 14/14: [===========] - loss: 925.47264382211325\n",
      " Epoch 590/1000; Batch 14/14: [===========] - loss: 922.69222099987466\n",
      " Epoch 591/1000; Batch 14/14: [===========] - loss: 933.58556327368294\n",
      " Epoch 592/1000; Batch 14/14: [===========] - loss: 933.71240448534479\n",
      " Epoch 593/1000; Batch 14/14: [===========] - loss: 935.95402213097791\n",
      " Epoch 594/1000; Batch 14/14: [===========] - loss: 933.11763229670284\n",
      " Epoch 595/1000; Batch 14/14: [===========] - loss: 931.50645495437456\n",
      " Epoch 596/1000; Batch 14/14: [===========] - loss: 932.18269849902265\n",
      " Epoch 597/1000; Batch 14/14: [===========] - loss: 1019.8837454147805\n",
      " Epoch 598/1000; Batch 14/14: [===========] - loss: 1018.8796145113155\n",
      " Epoch 599/1000; Batch 14/14: [===========] - loss: 1019.3983121135928\n",
      " Epoch 600/1000; Batch 14/14: [===========] - loss: 941.32650540336554\n",
      " Epoch 601/1000; Batch 14/14: [===========] - loss: 921.48446137637156\n",
      " Epoch 602/1000; Batch 14/14: [===========] - loss: 942.25066044137596\n",
      " Epoch 603/1000; Batch 14/14: [===========] - loss: 920.57874350908253\n",
      " Epoch 604/1000; Batch 14/14: [===========] - loss: 941.60367322221092\n",
      " Epoch 605/1000; Batch 14/14: [===========] - loss: 940.33675864493425\n",
      " Epoch 606/1000; Batch 14/14: [===========] - loss: 943.77574001009096\n",
      " Epoch 607/1000; Batch 14/14: [===========] - loss: 945.45399663021176\n",
      " Epoch 608/1000; Batch 14/14: [===========] - loss: 942.92815631228048\n",
      " Epoch 609/1000; Batch 14/14: [===========] - loss: 943.89112371788086\n",
      " Epoch 610/1000; Batch 14/14: [===========] - loss: 944.53051968926183\n",
      " Epoch 611/1000; Batch 14/14: [===========] - loss: 943.25577659554965\n",
      " Epoch 612/1000; Batch 14/14: [===========] - loss: 940.09277848599912\n",
      " Epoch 613/1000; Batch 14/14: [===========] - loss: 941.56323747762315\n",
      " Epoch 614/1000; Batch 14/14: [===========] - loss: 941.87787145278875\n",
      " Epoch 615/1000; Batch 14/14: [===========] - loss: 941.82234786204014\n",
      " Epoch 616/1000; Batch 14/14: [===========] - loss: 940.16447780919099\n",
      " Epoch 617/1000; Batch 14/14: [===========] - loss: 939.30885876312084\n",
      " Epoch 618/1000; Batch 14/14: [===========] - loss: 942.62576602968198\n",
      " Epoch 619/1000; Batch 14/14: [===========] - loss: 963.59113512693957\n",
      " Epoch 620/1000; Batch 14/14: [===========] - loss: 943.82644529614558\n",
      " Epoch 621/1000; Batch 14/14: [===========] - loss: 938.58244351053835\n",
      " Epoch 622/1000; Batch 14/14: [===========] - loss: 944.70577327468533\n",
      " Epoch 623/1000; Batch 14/14: [===========] - loss: 944.25712532662775\n",
      " Epoch 624/1000; Batch 14/14: [===========] - loss: 946.62600781172947\n",
      " Epoch 625/1000; Batch 14/14: [===========] - loss: 946.60972117744115\n",
      " Epoch 626/1000; Batch 14/14: [===========] - loss: 943.38176054810665\n",
      " Epoch 627/1000; Batch 14/14: [===========] - loss: 943.62441379459039\n",
      " Epoch 628/1000; Batch 14/14: [===========] - loss: 944.12897979948582\n",
      " Epoch 629/1000; Batch 14/14: [===========] - loss: 925.36633914518073\n",
      " Epoch 630/1000; Batch 14/14: [===========] - loss: 941.83372282166174\n",
      " Epoch 631/1000; Batch 14/14: [===========] - loss: 943.76588122425118\n",
      " Epoch 632/1000; Batch 14/14: [===========] - loss: 940.42783291929305\n",
      " Epoch 633/1000; Batch 14/14: [===========] - loss: 932.88995204270657\n",
      " Epoch 634/1000; Batch 14/14: [===========] - loss: 941.04595157395485\n",
      " Epoch 635/1000; Batch 14/14: [===========] - loss: 941.05758051901012\n",
      " Epoch 636/1000; Batch 14/14: [===========] - loss: 939.03487831976687\n",
      " Epoch 637/1000; Batch 14/14: [===========] - loss: 934.29057093990636\n",
      " Epoch 638/1000; Batch 14/14: [===========] - loss: 937.48868566953346\n",
      " Epoch 639/1000; Batch 14/14: [===========] - loss: 966.10362519221948\n",
      " Epoch 640/1000; Batch 14/14: [===========] - loss: 946.08885520517168\n",
      " Epoch 641/1000; Batch 14/14: [===========] - loss: 930.87069864965068\n",
      " Epoch 642/1000; Batch 14/14: [===========] - loss: 939.31913897248938\n",
      " Epoch 643/1000; Batch 14/14: [===========] - loss: 918.13773867598073\n",
      " Epoch 644/1000; Batch 14/14: [===========] - loss: 937.56938777631836\n",
      " Epoch 645/1000; Batch 14/14: [===========] - loss: 950.29007472675853\n",
      " Epoch 646/1000; Batch 14/14: [===========] - loss: 938.26090820266118\n",
      " Epoch 647/1000; Batch 14/14: [===========] - loss: 938.31381283496096\n",
      " Epoch 648/1000; Batch 14/14: [===========] - loss: 946.79211370652254\n",
      " Epoch 649/1000; Batch 14/14: [===========] - loss: 940.64483305055598\n",
      " Epoch 650/1000; Batch 14/14: [===========] - loss: 937.17021498675523\n",
      " Epoch 651/1000; Batch 14/14: [===========] - loss: 939.12732885832877\n",
      " Epoch 652/1000; Batch 14/14: [===========] - loss: 945.37201296581115\n",
      " Epoch 653/1000; Batch 14/14: [===========] - loss: 940.57319375460023\n",
      " Epoch 654/1000; Batch 14/14: [===========] - loss: 951.64586253734327\n",
      " Epoch 655/1000; Batch 14/14: [===========] - loss: 948.25640391850672\n",
      " Epoch 656/1000; Batch 14/14: [===========] - loss: 943.41823454682587\n",
      " Epoch 657/1000; Batch 14/14: [===========] - loss: 939.02882844211677\n",
      " Epoch 658/1000; Batch 14/14: [===========] - loss: 940.73997832723352\n",
      " Epoch 659/1000; Batch 14/14: [===========] - loss: 945.19206686970848\n",
      " Epoch 660/1000; Batch 14/14: [===========] - loss: 939.40560454950319\n",
      " Epoch 661/1000; Batch 14/14: [===========] - loss: 938.48339329808799\n",
      " Epoch 662/1000; Batch 14/14: [===========] - loss: 945.28377852664562\n",
      " Epoch 663/1000; Batch 14/14: [===========] - loss: 939.88462270429723\n",
      " Epoch 664/1000; Batch 14/14: [===========] - loss: 945.86018901353377\n",
      " Epoch 665/1000; Batch 14/14: [===========] - loss: 947.74191676550784\n",
      " Epoch 666/1000; Batch 14/14: [===========] - loss: 940.98426385295373\n",
      " Epoch 667/1000; Batch 14/14: [===========] - loss: 947.27045851729845\n",
      " Epoch 668/1000; Batch 14/14: [===========] - loss: 955.79823962451826\n",
      " Epoch 669/1000; Batch 14/14: [===========] - loss: 941.46663730041017\n",
      " Epoch 670/1000; Batch 14/14: [===========] - loss: 944.85076908990432\n",
      " Epoch 671/1000; Batch 14/14: [===========] - loss: 949.85654190989284\n",
      " Epoch 672/1000; Batch 14/14: [===========] - loss: 947.27130261536813\n",
      " Epoch 673/1000; Batch 14/14: [===========] - loss: 951.22494759511028\n",
      " Epoch 674/1000; Batch 14/14: [===========] - loss: 942.39974402289546\n",
      " Epoch 675/1000; Batch 14/14: [===========] - loss: 958.48783952340643\n",
      " Epoch 676/1000; Batch 14/14: [===========] - loss: 925.78581457710835\n",
      " Epoch 677/1000; Batch 14/14: [===========] - loss: 943.58706677843246\n",
      " Epoch 678/1000; Batch 14/14: [===========] - loss: 937.27449712586257\n",
      " Epoch 679/1000; Batch 14/14: [===========] - loss: 954.74190994157758\n",
      " Epoch 680/1000; Batch 14/14: [===========] - loss: 947.30585570458699\n",
      " Epoch 681/1000; Batch 14/14: [===========] - loss: 945.77600304094215\n",
      " Epoch 682/1000; Batch 14/14: [===========] - loss: 942.40911249267083\n",
      " Epoch 683/1000; Batch 14/14: [===========] - loss: 940.59355910513662\n",
      " Epoch 684/1000; Batch 14/14: [===========] - loss: 934.40306773504393\n",
      " Epoch 685/1000; Batch 14/14: [===========] - loss: 932.94962270021766\n",
      " Epoch 686/1000; Batch 14/14: [===========] - loss: 937.09919371734337\n",
      " Epoch 687/1000; Batch 14/14: [===========] - loss: 949.08845870528947\n",
      " Epoch 688/1000; Batch 14/14: [===========] - loss: 917.84433715177595\n",
      " Epoch 689/1000; Batch 14/14: [===========] - loss: 918.12412568011654\n",
      " Epoch 690/1000; Batch 14/14: [===========] - loss: 933.64980903482828\n",
      " Epoch 691/1000; Batch 14/14: [===========] - loss: 925.20637856549085\n",
      " Epoch 692/1000; Batch 14/14: [===========] - loss: 924.68018540925872\n",
      " Epoch 693/1000; Batch 14/14: [===========] - loss: 931.14493722256533\n",
      " Epoch 694/1000; Batch 14/14: [===========] - loss: 931.05945729761969\n",
      " Epoch 695/1000; Batch 14/14: [===========] - loss: 931.24708066988227\n",
      " Epoch 696/1000; Batch 14/14: [===========] - loss: 931.34346114265719\n",
      " Epoch 697/1000; Batch 14/14: [===========] - loss: 950.95714757391485\n",
      " Epoch 698/1000; Batch 14/14: [===========] - loss: 951.40606330281462\n",
      " Epoch 699/1000; Batch 14/14: [===========] - loss: 930.32616684633187\n",
      " Epoch 700/1000; Batch 14/14: [===========] - loss: 929.56081989726179\n",
      " Epoch 701/1000; Batch 14/14: [===========] - loss: 928.44975391882881\n",
      " Epoch 702/1000; Batch 14/14: [===========] - loss: 928.97088386005452\n",
      " Epoch 703/1000; Batch 14/14: [===========] - loss: 929.21644569242886\n",
      " Epoch 704/1000; Batch 14/14: [===========] - loss: 951.53136442163549\n",
      " Epoch 705/1000; Batch 14/14: [===========] - loss: 929.44568051503697\n",
      " Epoch 706/1000; Batch 14/14: [===========] - loss: 928.49480174994474\n",
      " Epoch 707/1000; Batch 14/14: [===========] - loss: 928.14635564881563\n",
      " Epoch 708/1000; Batch 14/14: [===========] - loss: 931.13200174954253\n",
      " Epoch 709/1000; Batch 14/14: [===========] - loss: 928.36683075612042\n",
      " Epoch 710/1000; Batch 14/14: [===========] - loss: 928.66327927802189\n",
      " Epoch 711/1000; Batch 14/14: [===========] - loss: 936.77464846553834\n",
      " Epoch 712/1000; Batch 14/14: [===========] - loss: 930.00008340565904\n",
      " Epoch 713/1000; Batch 14/14: [===========] - loss: 935.85288881846842\n",
      " Epoch 714/1000; Batch 14/14: [===========] - loss: 929.79036079218547\n",
      " Epoch 715/1000; Batch 14/14: [===========] - loss: 936.11786624581632\n",
      " Epoch 716/1000; Batch 14/14: [===========] - loss: 929.57058851712389\n",
      " Epoch 717/1000; Batch 14/14: [===========] - loss: 926.31568670338616\n",
      " Epoch 718/1000; Batch 14/14: [===========] - loss: 934.33436371444754\n",
      " Epoch 719/1000; Batch 14/14: [===========] - loss: 930.75118271087317\n",
      " Epoch 720/1000; Batch 14/14: [===========] - loss: 938.56945688825016\n",
      " Epoch 721/1000; Batch 14/14: [===========] - loss: 925.00565285257212\n",
      " Epoch 722/1000; Batch 14/14: [===========] - loss: 934.48591598159457\n",
      " Epoch 723/1000; Batch 14/14: [===========] - loss: 940.50375498828054\n",
      " Epoch 724/1000; Batch 14/14: [===========] - loss: 925.53995613386067\n",
      " Epoch 725/1000; Batch 14/14: [===========] - loss: 935.03355182734832\n",
      " Epoch 726/1000; Batch 14/14: [===========] - loss: 940.95453274338745\n",
      " Epoch 727/1000; Batch 14/14: [===========] - loss: 942.32120607311863\n",
      " Epoch 728/1000; Batch 14/14: [===========] - loss: 933.13301529303736\n",
      " Epoch 729/1000; Batch 14/14: [===========] - loss: 938.99641975182936\n",
      " Epoch 730/1000; Batch 14/14: [===========] - loss: 931.30691687352214\n",
      " Epoch 731/1000; Batch 14/14: [===========] - loss: 936.65231301281185\n",
      " Epoch 732/1000; Batch 14/14: [===========] - loss: 940.13283491100149\n",
      " Epoch 733/1000; Batch 14/14: [===========] - loss: 940.32231220980017\n",
      " Epoch 734/1000; Batch 14/14: [===========] - loss: 939.73439124595763\n",
      " Epoch 735/1000; Batch 14/14: [===========] - loss: 927.92585748947988\n",
      " Epoch 736/1000; Batch 14/14: [===========] - loss: 936.67864757427173\n",
      " Epoch 737/1000; Batch 14/14: [===========] - loss: 941.28357215560167\n",
      " Epoch 738/1000; Batch 14/14: [===========] - loss: 941.35873715685566\n",
      " Epoch 739/1000; Batch 14/14: [===========] - loss: 940.71926993719672\n",
      " Epoch 740/1000; Batch 14/14: [===========] - loss: 937.31187777530397\n",
      " Epoch 741/1000; Batch 14/14: [===========] - loss: 939.92112518230191\n",
      " Epoch 742/1000; Batch 14/14: [===========] - loss: 936.9328350886941\n",
      " Epoch 743/1000; Batch 14/14: [===========] - loss: 940.37900697105655\n",
      " Epoch 744/1000; Batch 14/14: [===========] - loss: 937.51538439751615\n",
      " Epoch 745/1000; Batch 14/14: [===========] - loss: 931.08905388144553\n",
      " Epoch 746/1000; Batch 14/14: [===========] - loss: 937.66157343661444\n",
      " Epoch 747/1000; Batch 14/14: [===========] - loss: 924.68696712791141\n",
      " Epoch 748/1000; Batch 14/14: [===========] - loss: 931.12073833633971\n",
      " Epoch 749/1000; Batch 14/14: [===========] - loss: 942.74247712471254\n",
      " Epoch 750/1000; Batch 14/14: [===========] - loss: 940.76012778770125\n",
      " Epoch 751/1000; Batch 14/14: [===========] - loss: 938.95861117631648\n",
      " Epoch 752/1000; Batch 14/14: [===========] - loss: 941.68818637941934\n",
      " Epoch 753/1000; Batch 14/14: [===========] - loss: 926.14056774077151\n",
      " Epoch 754/1000; Batch 14/14: [===========] - loss: 924.83196261713323\n",
      " Epoch 755/1000; Batch 14/14: [===========] - loss: 944.04601689558084\n",
      " Epoch 756/1000; Batch 14/14: [===========] - loss: 942.9909116626348\n",
      " Epoch 757/1000; Batch 14/14: [===========] - loss: 939.09183915019687\n",
      " Epoch 758/1000; Batch 14/14: [===========] - loss: 929.79976523179617\n",
      " Epoch 759/1000; Batch 14/14: [===========] - loss: 940.81104823603632\n",
      " Epoch 760/1000; Batch 14/14: [===========] - loss: 940.61028323486297\n",
      " Epoch 761/1000; Batch 14/14: [===========] - loss: 940.29702499537585\n",
      " Epoch 762/1000; Batch 14/14: [===========] - loss: 939.97857640675947\n",
      " Epoch 763/1000; Batch 14/14: [===========] - loss: 939.69084976856977\n",
      " Epoch 764/1000; Batch 14/14: [===========] - loss: 939.43900501751966\n",
      " Epoch 765/1000; Batch 14/14: [===========] - loss: 939.22134292747275\n",
      " Epoch 766/1000; Batch 14/14: [===========] - loss: 939.03041863852996\n",
      " Epoch 767/1000; Batch 14/14: [===========] - loss: 938.87207674011665\n",
      " Epoch 768/1000; Batch 14/14: [===========] - loss: 938.72288415155771\n",
      " Epoch 769/1000; Batch 14/14: [===========] - loss: 938.59502033910537\n",
      " Epoch 770/1000; Batch 14/14: [===========] - loss: 938.48139390111264\n",
      " Epoch 771/1000; Batch 14/14: [===========] - loss: 938.27486696178975\n",
      " Epoch 772/1000; Batch 14/14: [===========] - loss: 938.19763997964176\n",
      " Epoch 773/1000; Batch 14/14: [===========] - loss: 938.10906171529935\n",
      " Epoch 774/1000; Batch 14/14: [===========] - loss: 942.16015852398584\n",
      " Epoch 775/1000; Batch 14/14: [===========] - loss: 923.49650237633312\n",
      " Epoch 776/1000; Batch 14/14: [===========] - loss: 938.48782889949159\n",
      " Epoch 777/1000; Batch 14/14: [===========] - loss: 942.58516732960521\n",
      " Epoch 778/1000; Batch 14/14: [===========] - loss: 943.51004937468164\n",
      " Epoch 779/1000; Batch 14/14: [===========] - loss: 924.11285906973275\n",
      " Epoch 780/1000; Batch 14/14: [===========] - loss: 938.86859855737565\n",
      " Epoch 781/1000; Batch 14/14: [===========] - loss: 943.12742332572214\n",
      " Epoch 782/1000; Batch 14/14: [===========] - loss: 938.12450511328426\n",
      " Epoch 783/1000; Batch 14/14: [===========] - loss: 922.22840233322746\n",
      " Epoch 784/1000; Batch 14/14: [===========] - loss: 922.35070827597227\n",
      " Epoch 785/1000; Batch 14/14: [===========] - loss: 921.4060773888033\n",
      " Epoch 786/1000; Batch 14/14: [===========] - loss: 921.01613835229322\n",
      " Epoch 787/1000; Batch 14/14: [===========] - loss: 935.52278199925768\n",
      " Epoch 788/1000; Batch 14/14: [===========] - loss: 941.58051046948233\n",
      " Epoch 789/1000; Batch 14/14: [===========] - loss: 943.31377750374416\n",
      " Epoch 790/1000; Batch 14/14: [===========] - loss: 939.29510084774865\n",
      " Epoch 791/1000; Batch 14/14: [===========] - loss: 944.35051071874977\n",
      " Epoch 792/1000; Batch 14/14: [===========] - loss: 923.67765102167165\n",
      " Epoch 793/1000; Batch 14/14: [===========] - loss: 923.80993292055752\n",
      " Epoch 794/1000; Batch 14/14: [===========] - loss: 934.84520738876077\n",
      " Epoch 795/1000; Batch 14/14: [===========] - loss: 942.92315120976312\n",
      " Epoch 796/1000; Batch 14/14: [===========] - loss: 923.73150134525418\n",
      " Epoch 797/1000; Batch 14/14: [===========] - loss: 937.81784221957237\n",
      " Epoch 798/1000; Batch 14/14: [===========] - loss: 935.53164781068836\n",
      " Epoch 799/1000; Batch 14/14: [===========] - loss: 942.86782401220165\n",
      " Epoch 800/1000; Batch 14/14: [===========] - loss: 925.14329051270117\n",
      " Epoch 801/1000; Batch 14/14: [===========] - loss: 920.22602347931894\n",
      " Epoch 802/1000; Batch 14/14: [===========] - loss: 938.18401015509992\n",
      " Epoch 803/1000; Batch 14/14: [===========] - loss: 930.56169364599626\n",
      " Epoch 804/1000; Batch 14/14: [===========] - loss: 920.90106244244323\n",
      " Epoch 805/1000; Batch 14/14: [===========] - loss: 937.37351733710433\n",
      " Epoch 806/1000; Batch 14/14: [===========] - loss: 929.2831029565291\n",
      " Epoch 807/1000; Batch 14/14: [===========] - loss: 937.26252643220912\n",
      " Epoch 808/1000; Batch 14/14: [===========] - loss: 925.9388429946453\n",
      " Epoch 809/1000; Batch 14/14: [===========] - loss: 935.62465794689066\n",
      " Epoch 810/1000; Batch 14/14: [===========] - loss: 943.3200693556756\n",
      " Epoch 811/1000; Batch 14/14: [===========] - loss: 926.53515586876034\n",
      " Epoch 812/1000; Batch 14/14: [===========] - loss: 936.53114601589824\n",
      " Epoch 813/1000; Batch 14/14: [===========] - loss: 943.55478068253514\n",
      " Epoch 814/1000; Batch 14/14: [===========] - loss: 926.56972221774732\n",
      " Epoch 815/1000; Batch 14/14: [===========] - loss: 951.41364340011336\n",
      " Epoch 816/1000; Batch 14/14: [===========] - loss: 927.32387287718383\n",
      " Epoch 817/1000; Batch 14/14: [===========] - loss: 925.61945210961416\n",
      " Epoch 818/1000; Batch 14/14: [===========] - loss: 923.67362691102863\n",
      " Epoch 819/1000; Batch 14/14: [===========] - loss: 922.74361082551013\n",
      " Epoch 820/1000; Batch 14/14: [===========] - loss: 921.22438376602156\n",
      " Epoch 821/1000; Batch 14/14: [===========] - loss: 926.51757560330475\n",
      " Epoch 822/1000; Batch 14/14: [===========] - loss: 933.32413235953866\n",
      " Epoch 823/1000; Batch 14/14: [===========] - loss: 939.51435428827244\n",
      " Epoch 824/1000; Batch 14/14: [===========] - loss: 936.36332907538615\n",
      " Epoch 825/1000; Batch 14/14: [===========] - loss: 931.61210714069688\n",
      " Epoch 826/1000; Batch 14/14: [===========] - loss: 937.39696139080666\n",
      " Epoch 827/1000; Batch 14/14: [===========] - loss: 918.26209105228761\n",
      " Epoch 828/1000; Batch 14/14: [===========] - loss: 933.21262940308533\n",
      " Epoch 829/1000; Batch 14/14: [===========] - loss: 934.80185149658135\n",
      " Epoch 830/1000; Batch 14/14: [===========] - loss: 934.23809028110828\n",
      " Epoch 831/1000; Batch 14/14: [===========] - loss: 934.60511839166246\n",
      " Epoch 832/1000; Batch 14/14: [===========] - loss: 934.41094921108084\n",
      " Epoch 833/1000; Batch 14/14: [===========] - loss: 933.89939600818732\n",
      " Epoch 834/1000; Batch 14/14: [===========] - loss: 934.18654913173927\n",
      " Epoch 835/1000; Batch 14/14: [===========] - loss: 933.19780506577984\n",
      " Epoch 836/1000; Batch 14/14: [===========] - loss: 933.88721469350151\n",
      " Epoch 837/1000; Batch 14/14: [===========] - loss: 932.50825386915473\n",
      " Epoch 838/1000; Batch 14/14: [===========] - loss: 915.26057435023979\n",
      " Epoch 839/1000; Batch 14/14: [===========] - loss: 932.12050122253638\n",
      " Epoch 840/1000; Batch 14/14: [===========] - loss: 939.62387964888523\n",
      " Epoch 841/1000; Batch 14/14: [===========] - loss: 933.97087929422238\n",
      " Epoch 842/1000; Batch 14/14: [===========] - loss: 914.86709297772653\n",
      " Epoch 843/1000; Batch 14/14: [===========] - loss: 936.79807429527797\n",
      " Epoch 844/1000; Batch 14/14: [===========] - loss: 940.14494445699098\n",
      " Epoch 845/1000; Batch 14/14: [===========] - loss: 929.49497811085149\n",
      " Epoch 846/1000; Batch 14/14: [===========] - loss: 914.08064682742038\n",
      " Epoch 847/1000; Batch 14/14: [===========] - loss: 935.99737540108924\n",
      " Epoch 848/1000; Batch 14/14: [===========] - loss: 939.44707133573511\n",
      " Epoch 849/1000; Batch 14/14: [===========] - loss: 940.45773493033223\n",
      " Epoch 850/1000; Batch 14/14: [===========] - loss: 933.86554384565946\n",
      " Epoch 851/1000; Batch 14/14: [===========] - loss: 937.29419153799389\n",
      " Epoch 852/1000; Batch 14/14: [===========] - loss: 929.38492962567935\n",
      " Epoch 853/1000; Batch 14/14: [===========] - loss: 912.50550377574193\n",
      " Epoch 854/1000; Batch 14/14: [===========] - loss: 915.34879258169787\n",
      " Epoch 855/1000; Batch 14/14: [===========] - loss: 936.42886540454791\n",
      " Epoch 856/1000; Batch 14/14: [===========] - loss: 938.01014123741071\n",
      " Epoch 857/1000; Batch 14/14: [===========] - loss: 940.40055512775582\n",
      " Epoch 858/1000; Batch 14/14: [===========] - loss: 941.46687289038888\n",
      " Epoch 859/1000; Batch 14/14: [===========] - loss: 958.44459614323561\n",
      " Epoch 860/1000; Batch 14/14: [===========] - loss: 934.77095667746864\n",
      " Epoch 861/1000; Batch 14/14: [===========] - loss: 958.29539018093365\n",
      " Epoch 862/1000; Batch 14/14: [===========] - loss: 934.26850123254877\n",
      " Epoch 863/1000; Batch 14/14: [===========] - loss: 917.63626859838059\n",
      " Epoch 864/1000; Batch 14/14: [===========] - loss: 913.63841863430874\n",
      " Epoch 865/1000; Batch 14/14: [===========] - loss: 912.55490183045622\n",
      " Epoch 866/1000; Batch 14/14: [===========] - loss: 909.88561812866949\n",
      " Epoch 867/1000; Batch 14/14: [===========] - loss: 935.17886424308342\n",
      " Epoch 868/1000; Batch 14/14: [===========] - loss: 942.70881436277676\n",
      " Epoch 869/1000; Batch 14/14: [===========] - loss: 941.04211418128642\n",
      " Epoch 870/1000; Batch 14/14: [===========] - loss: 919.91501853395312\n",
      " Epoch 871/1000; Batch 14/14: [===========] - loss: 912.28623735015914\n",
      " Epoch 872/1000; Batch 14/14: [===========] - loss: 935.67330158790563\n",
      " Epoch 873/1000; Batch 14/14: [===========] - loss: 922.00856406106828\n",
      " Epoch 874/1000; Batch 14/14: [===========] - loss: 937.7949004271763\n",
      " Epoch 875/1000; Batch 14/14: [===========] - loss: 923.43123371840801\n",
      " Epoch 876/1000; Batch 14/14: [===========] - loss: 938.41427087978218\n",
      " Epoch 877/1000; Batch 14/14: [===========] - loss: 944.32884549998867\n",
      " Epoch 878/1000; Batch 14/14: [===========] - loss: 925.73062711920153\n",
      " Epoch 879/1000; Batch 14/14: [===========] - loss: 912.82450129148971\n",
      " Epoch 880/1000; Batch 14/14: [===========] - loss: 936.15678954998322\n",
      " Epoch 881/1000; Batch 14/14: [===========] - loss: 939.09983186621512\n",
      " Epoch 882/1000; Batch 14/14: [===========] - loss: 921.77794759586157\n",
      " Epoch 883/1000; Batch 14/14: [===========] - loss: 912.76095043397039\n",
      " Epoch 884/1000; Batch 14/14: [===========] - loss: 936.55702559978866\n",
      " Epoch 885/1000; Batch 14/14: [===========] - loss: 935.20036684041123\n",
      " Epoch 886/1000; Batch 14/14: [===========] - loss: 935.90284021574867\n",
      " Epoch 887/1000; Batch 14/14: [===========] - loss: 936.75500749072883\n",
      " Epoch 888/1000; Batch 14/14: [===========] - loss: 924.77956304957427\n",
      " Epoch 889/1000; Batch 14/14: [===========] - loss: 913.85576182906965\n",
      " Epoch 890/1000; Batch 14/14: [===========] - loss: 912.60309209317144\n",
      " Epoch 891/1000; Batch 14/14: [===========] - loss: 918.56523258183316\n",
      " Epoch 892/1000; Batch 14/14: [===========] - loss: 921.21567579271469\n",
      " Epoch 893/1000; Batch 14/14: [===========] - loss: 923.54303615196863\n",
      " Epoch 894/1000; Batch 14/14: [===========] - loss: 921.34935195048472\n",
      " Epoch 895/1000; Batch 14/14: [===========] - loss: 923.78636679428061\n",
      " Epoch 896/1000; Batch 14/14: [===========] - loss: 922.12074535866047\n",
      " Epoch 897/1000; Batch 14/14: [===========] - loss: 924.23318291339862\n",
      " Epoch 898/1000; Batch 14/14: [===========] - loss: 922.47675563606756\n",
      " Epoch 899/1000; Batch 14/14: [===========] - loss: 924.21582457213251\n",
      " Epoch 900/1000; Batch 14/14: [===========] - loss: 923.51668510551112\n",
      " Epoch 901/1000; Batch 14/14: [===========] - loss: 924.69040677315573\n",
      " Epoch 902/1000; Batch 14/14: [===========] - loss: 925.26864470747347\n",
      " Epoch 903/1000; Batch 14/14: [===========] - loss: 919.33516808041778\n",
      " Epoch 904/1000; Batch 14/14: [===========] - loss: 919.35825537722123\n",
      " Epoch 905/1000; Batch 14/14: [===========] - loss: 920.86892792010616\n",
      " Epoch 906/1000; Batch 14/14: [===========] - loss: 917.46143217297884\n",
      " Epoch 907/1000; Batch 14/14: [===========] - loss: 920.00435923294153\n",
      " Epoch 908/1000; Batch 14/14: [===========] - loss: 917.21817426444264\n",
      " Epoch 909/1000; Batch 14/14: [===========] - loss: 920.25591054161236\n",
      " Epoch 910/1000; Batch 14/14: [===========] - loss: 917.19579973894161\n",
      " Epoch 911/1000; Batch 14/14: [===========] - loss: 919.87779594176033\n",
      " Epoch 912/1000; Batch 14/14: [===========] - loss: 916.79848890700362\n",
      " Epoch 913/1000; Batch 14/14: [===========] - loss: 916.93834889383829\n",
      " Epoch 914/1000; Batch 14/14: [===========] - loss: 921.47039719534725\n",
      " Epoch 915/1000; Batch 14/14: [===========] - loss: 915.75498990428422\n",
      " Epoch 916/1000; Batch 14/14: [===========] - loss: 916.08731383131838\n",
      " Epoch 917/1000; Batch 14/14: [===========] - loss: 915.47650089033949\n",
      " Epoch 918/1000; Batch 14/14: [===========] - loss: 919.02449583956147\n",
      " Epoch 919/1000; Batch 14/14: [===========] - loss: 917.18099149589057\n",
      " Epoch 920/1000; Batch 14/14: [===========] - loss: 919.75676808349038\n",
      " Epoch 921/1000; Batch 14/14: [===========] - loss: 916.02115754490667\n",
      " Epoch 922/1000; Batch 14/14: [===========] - loss: 919.94813711325539\n",
      " Epoch 923/1000; Batch 14/14: [===========] - loss: 917.81721386815253\n",
      " Epoch 924/1000; Batch 14/14: [===========] - loss: 911.45492781049394\n",
      " Epoch 925/1000; Batch 14/14: [===========] - loss: 915.73751328732682\n",
      " Epoch 926/1000; Batch 14/14: [===========] - loss: 938.54696733274267\n",
      " Epoch 927/1000; Batch 14/14: [===========] - loss: 915.26938035150379\n",
      " Epoch 928/1000; Batch 14/14: [===========] - loss: 914.93611912713224\n",
      " Epoch 929/1000; Batch 14/14: [===========] - loss: 914.33346440031977\n",
      " Epoch 930/1000; Batch 14/14: [===========] - loss: 914.20136966249582\n",
      " Epoch 931/1000; Batch 14/14: [===========] - loss: 917.27742157695733\n",
      " Epoch 932/1000; Batch 14/14: [===========] - loss: 915.65074586984035\n",
      " Epoch 933/1000; Batch 14/14: [===========] - loss: 918.41448591912434\n",
      " Epoch 934/1000; Batch 14/14: [===========] - loss: 916.04745810109516\n",
      " Epoch 935/1000; Batch 14/14: [===========] - loss: 912.99186351432377\n",
      " Epoch 936/1000; Batch 14/14: [===========] - loss: 914.42250377789764\n",
      " Epoch 937/1000; Batch 14/14: [===========] - loss: 932.82695142939715\n",
      " Epoch 938/1000; Batch 14/14: [===========] - loss: 917.35539674493759\n",
      " Epoch 939/1000; Batch 14/14: [===========] - loss: 912.96216596224567\n",
      " Epoch 940/1000; Batch 14/14: [===========] - loss: 911.84519606322938\n",
      " Epoch 941/1000; Batch 14/14: [===========] - loss: 911.13395554813068\n",
      " Epoch 942/1000; Batch 14/14: [===========] - loss: 911.22433855936194\n",
      " Epoch 943/1000; Batch 14/14: [===========] - loss: 920.24625991964342\n",
      " Epoch 944/1000; Batch 14/14: [===========] - loss: 914.75023534213944\n",
      " Epoch 945/1000; Batch 14/14: [===========] - loss: 915.76619482661661\n",
      " Epoch 946/1000; Batch 14/14: [===========] - loss: 914.98114753455063\n",
      " Epoch 947/1000; Batch 14/14: [===========] - loss: 910.44102225730463\n",
      " Epoch 948/1000; Batch 14/14: [===========] - loss: 919.38552454367278\n",
      " Epoch 949/1000; Batch 14/14: [===========] - loss: 912.89376782043032\n",
      " Epoch 950/1000; Batch 14/14: [===========] - loss: 915.38693748023248\n",
      " Epoch 951/1000; Batch 14/14: [===========] - loss: 914.14565256936738\n",
      " Epoch 952/1000; Batch 14/14: [===========] - loss: 913.32310127293453\n",
      " Epoch 953/1000; Batch 14/14: [===========] - loss: 912.25490568545083\n",
      " Epoch 954/1000; Batch 14/14: [===========] - loss: 914.18984732046612\n",
      " Epoch 955/1000; Batch 14/14: [===========] - loss: 912.78469440357562\n",
      " Epoch 956/1000; Batch 14/14: [===========] - loss: 911.85912724503573\n",
      " Epoch 957/1000; Batch 14/14: [===========] - loss: 910.46684587808396\n",
      " Epoch 958/1000; Batch 14/14: [===========] - loss: 912.03220994672196\n",
      " Epoch 959/1000; Batch 14/14: [===========] - loss: 910.7655751610217\n",
      " Epoch 960/1000; Batch 14/14: [===========] - loss: 909.95238021746897\n",
      " Epoch 961/1000; Batch 14/14: [===========] - loss: 908.95528975603835\n",
      " Epoch 962/1000; Batch 14/14: [===========] - loss: 907.24490683947527\n",
      " Epoch 963/1000; Batch 14/14: [===========] - loss: 909.23104596258413\n",
      " Epoch 964/1000; Batch 14/14: [===========] - loss: 908.53019579045274\n",
      " Epoch 965/1000; Batch 14/14: [===========] - loss: 907.53359007030837\n",
      " Epoch 966/1000; Batch 14/14: [===========] - loss: 909.70760496059374\n",
      " Epoch 967/1000; Batch 14/14: [===========] - loss: 908.39046262680618\n",
      " Epoch 968/1000; Batch 14/14: [===========] - loss: 906.15990492319121\n",
      " Epoch 969/1000; Batch 14/14: [===========] - loss: 907.08860850129018\n",
      " Epoch 970/1000; Batch 14/14: [===========] - loss: 904.87332601969434\n",
      " Epoch 971/1000; Batch 14/14: [===========] - loss: 915.65259223782283\n",
      " Epoch 972/1000; Batch 14/14: [===========] - loss: 903.67650826909741\n",
      " Epoch 973/1000; Batch 14/14: [===========] - loss: 905.90308245746455\n",
      " Epoch 974/1000; Batch 14/14: [===========] - loss: 904.04868002284331\n",
      " Epoch 975/1000; Batch 14/14: [===========] - loss: 905.26036593930547\n",
      " Epoch 976/1000; Batch 14/14: [===========] - loss: 903.25667845064418\n",
      " Epoch 977/1000; Batch 14/14: [===========] - loss: 900.63237179219254\n",
      " Epoch 978/1000; Batch 14/14: [===========] - loss: 904.87600520435798\n",
      " Epoch 979/1000; Batch 14/14: [===========] - loss: 904.76252269027495\n",
      " Epoch 980/1000; Batch 14/14: [===========] - loss: 904.65898050603044\n",
      " Epoch 981/1000; Batch 14/14: [===========] - loss: 951.10885757154765\n",
      " Epoch 982/1000; Batch 14/14: [===========] - loss: 907.62572107267172\n",
      " Epoch 983/1000; Batch 14/14: [===========] - loss: 916.69485590925593\n",
      " Epoch 984/1000; Batch 14/14: [===========] - loss: 904.87055987222642\n",
      " Epoch 985/1000; Batch 14/14: [===========] - loss: 899.58113109721192\n",
      " Epoch 986/1000; Batch 14/14: [===========] - loss: 902.76826745156485\n",
      " Epoch 987/1000; Batch 14/14: [===========] - loss: 915.55779709398184\n",
      " Epoch 988/1000; Batch 14/14: [===========] - loss: 903.66241565649336\n",
      " Epoch 989/1000; Batch 14/14: [===========] - loss: 903.87778712965805\n",
      " Epoch 990/1000; Batch 14/14: [===========] - loss: 898.53963525330524\n",
      " Epoch 991/1000; Batch 14/14: [===========] - loss: 903.70397163104845\n",
      " Epoch 992/1000; Batch 14/14: [===========] - loss: 898.24471087541882\n",
      " Epoch 993/1000; Batch 14/14: [===========] - loss: 898.97168185390487\n",
      " Epoch 994/1000; Batch 14/14: [===========] - loss: 900.46889094684067\n",
      " Epoch 995/1000; Batch 14/14: [===========] - loss: 912.71422636824628\n",
      " Epoch 996/1000; Batch 14/14: [===========] - loss: 912.71237235468652\n",
      " Epoch 997/1000; Batch 14/14: [===========] - loss: 902.20460547423522\n",
      " Epoch 998/1000; Batch 14/14: [===========] - loss: 899.12732889364759\n",
      " Epoch 999/1000; Batch 14/14: [===========] - loss: 892.87104214557127\n",
      " Epoch 1000/1000; Batch 14/14: [===========] - loss: 900.91914268264791\n"
     ]
    }
   ],
   "source": [
    "nn = NeauralNetwork(layers=[\n",
    "        Layer(units=10, input_layer=True),\n",
    "        # Layer(units=40, activation=\"sigmoid\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=1),\n",
    "    ],\n",
    "    loss_function = \"mse\",\n",
    "    learning_rate=0.001, \n",
    "    verbose=True,\n",
    "    optimizer=\"adagrad\",\n",
    "    batch_size = 32,\n",
    "    epochs=1000\n",
    ")\n",
    "y_diab = y_diab.reshape(-1, 1)\n",
    "nn.fit(X_diab, y_diab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-3.00863455e-04],\n",
       "       [-1.28876178e-04],\n",
       "       [-3.37749660e-04],\n",
       "       [-7.73479155e-05],\n",
       "       [ 8.18006031e-06],\n",
       "       [-2.12885040e-04],\n",
       "       [-4.03425618e-04],\n",
       "       [-9.79001148e-05],\n",
       "       [-2.37367204e-04],\n",
       "       [-3.99828573e-05],\n",
       "       [-3.15673745e-04],\n",
       "       [-3.61422914e-04],\n",
       "       [-1.52242264e-05],\n",
       "       [-1.42808485e-04],\n",
       "       [-3.35033253e-05],\n",
       "       [-1.78867959e-04],\n",
       "       [ 6.54289830e-05],\n",
       "       [-9.84896127e-05],\n",
       "       [-1.04739105e-04],\n",
       "       [-2.75358676e-05],\n",
       "       [-1.70954311e-04],\n",
       "       [-3.92214966e-04],\n",
       "       [-2.10782776e-04],\n",
       "       [-3.55347262e-04],\n",
       "       [-1.34492732e-04],\n",
       "       [-3.12458788e-04],\n",
       "       [-3.13882001e-04],\n",
       "       [-1.86040192e-04],\n",
       "       [-1.44840732e-04],\n",
       "       [-2.11750346e-04],\n",
       "       [-1.22677384e-04],\n",
       "       [-1.33908463e-04],\n",
       "       [-3.44780867e-04],\n",
       "       [-1.89532739e-04],\n",
       "       [-1.39426663e-04],\n",
       "       [-3.11759788e-04],\n",
       "       [ 7.68526227e-05],\n",
       "       [-4.86714102e-05],\n",
       "       [-1.21538020e-04],\n",
       "       [-2.64196474e-04],\n",
       "       [ 1.05585335e-05],\n",
       "       [-2.74970222e-04],\n",
       "       [-3.74839213e-04],\n",
       "       [-3.44651545e-04],\n",
       "       [-3.16120260e-04],\n",
       "       [-3.40742623e-04],\n",
       "       [-8.28315239e-05],\n",
       "       [-1.92726484e-04],\n",
       "       [-2.45570634e-04],\n",
       "       [-2.95260699e-04],\n",
       "       [-3.43340266e-05],\n",
       "       [-2.69925511e-04],\n",
       "       [-1.74841734e-05],\n",
       "       [ 4.14580324e-05],\n",
       "       [-1.32671560e-04],\n",
       "       [-1.33700160e-04],\n",
       "       [-5.74683001e-05],\n",
       "       [-1.95542710e-04],\n",
       "       [-9.36061787e-05],\n",
       "       [-3.34593884e-05],\n",
       "       [-1.76795894e-04],\n",
       "       [-3.49767196e-04],\n",
       "       [-3.93745637e-04],\n",
       "       [-1.23794801e-04],\n",
       "       [-2.63679905e-04],\n",
       "       [-1.10492981e-04],\n",
       "       [-1.90196449e-04],\n",
       "       [-1.23503918e-04],\n",
       "       [-1.93117751e-04],\n",
       "       [-1.09104320e-04],\n",
       "       [-1.52449465e-04],\n",
       "       [ 6.18511501e-05],\n",
       "       [-1.71823539e-04],\n",
       "       [-1.56493999e-04],\n",
       "       [-2.40428456e-05],\n",
       "       [-1.22841026e-04],\n",
       "       [-3.21832802e-04],\n",
       "       [-2.75752200e-04],\n",
       "       [-1.14039995e-04],\n",
       "       [-1.92914352e-04],\n",
       "       [ 3.58766314e-05],\n",
       "       [-1.59173720e-04],\n",
       "       [-1.59667085e-04],\n",
       "       [-1.01903469e-04],\n",
       "       [-3.00605673e-04],\n",
       "       [-1.41368837e-04],\n",
       "       [-4.59207889e-04],\n",
       "       [ 3.32863698e-05],\n",
       "       [-2.83326872e-04],\n",
       "       [-1.37039336e-04],\n",
       "       [-1.52568262e-04],\n",
       "       [-1.84463850e-04],\n",
       "       [ 1.30270980e-04],\n",
       "       [-1.15169370e-04],\n",
       "       [-1.41243627e-04],\n",
       "       [-2.05441681e-04],\n",
       "       [-7.35885500e-05],\n",
       "       [-2.63382274e-04],\n",
       "       [-4.11071493e-04],\n",
       "       [ 5.22887213e-05],\n",
       "       [ 2.18953858e-05],\n",
       "       [-1.54976023e-04],\n",
       "       [-1.46843669e-04],\n",
       "       [-2.83695203e-04],\n",
       "       [-9.91363194e-05],\n",
       "       [-1.51102691e-04],\n",
       "       [-2.52221562e-04],\n",
       "       [-1.97465185e-04],\n",
       "       [-2.09408878e-04],\n",
       "       [ 9.61981790e-05],\n",
       "       [-2.71300979e-04],\n",
       "       [-5.19211176e-05],\n",
       "       [-1.68815219e-04],\n",
       "       [-6.70547876e-05],\n",
       "       [-1.19588068e-04],\n",
       "       [-1.70498591e-04],\n",
       "       [-1.21389754e-04],\n",
       "       [ 1.04967584e-04],\n",
       "       [ 6.31965783e-06],\n",
       "       [-7.00357853e-05],\n",
       "       [-1.12550499e-04],\n",
       "       [ 7.04186619e-05],\n",
       "       [-1.78680190e-04],\n",
       "       [-1.95562754e-04],\n",
       "       [-4.58389318e-06],\n",
       "       [-1.23792985e-04],\n",
       "       [-2.87199086e-04],\n",
       "       [-4.66764365e-04],\n",
       "       [-1.31059511e-04],\n",
       "       [-1.85287462e-04],\n",
       "       [-1.28308411e-04],\n",
       "       [-2.30509952e-04],\n",
       "       [-6.45818572e-05],\n",
       "       [-4.30580376e-04],\n",
       "       [-1.11522708e-04],\n",
       "       [-2.74867836e-04],\n",
       "       [-2.86321866e-04],\n",
       "       [ 2.97606340e-06],\n",
       "       [-1.18293836e-04],\n",
       "       [-9.41577818e-05],\n",
       "       [-1.39051119e-04],\n",
       "       [-2.87525462e-04],\n",
       "       [-9.99526620e-05],\n",
       "       [-8.18274857e-05],\n",
       "       [-1.95717681e-04],\n",
       "       [-5.61288143e-05],\n",
       "       [-2.29579990e-04],\n",
       "       [-1.61693634e-04],\n",
       "       [-2.44051877e-04],\n",
       "       [-2.98914415e-04],\n",
       "       [-1.01083061e-04],\n",
       "       [ 2.17805910e-06],\n",
       "       [-6.71896177e-05],\n",
       "       [-7.33242802e-05],\n",
       "       [-1.89997644e-04],\n",
       "       [-5.54086252e-05],\n",
       "       [-1.81044136e-05],\n",
       "       [-3.93585611e-05],\n",
       "       [-7.32557667e-05],\n",
       "       [ 4.77608989e-05],\n",
       "       [-5.73145874e-05],\n",
       "       [-5.47519975e-05],\n",
       "       [-1.71274587e-04],\n",
       "       [-8.00551106e-05],\n",
       "       [ 1.05977881e-05],\n",
       "       [-1.43313327e-04],\n",
       "       [-6.14776037e-04],\n",
       "       [-1.84312679e-04],\n",
       "       [-2.87120171e-04],\n",
       "       [ 3.72032100e-05],\n",
       "       [-4.15652352e-04],\n",
       "       [-1.52702137e-04],\n",
       "       [-2.29387995e-04],\n",
       "       [-4.62045711e-04],\n",
       "       [-3.14758861e-04],\n",
       "       [-7.66819520e-05],\n",
       "       [-9.63022584e-05],\n",
       "       [-1.06956013e-04],\n",
       "       [-1.22411342e-04],\n",
       "       [ 3.78822057e-05],\n",
       "       [-2.19693944e-04],\n",
       "       [-9.34399109e-05],\n",
       "       [-1.76350251e-04],\n",
       "       [-3.43949633e-05],\n",
       "       [-1.39613826e-04],\n",
       "       [-2.01353576e-04],\n",
       "       [-4.00208373e-04],\n",
       "       [-2.76237915e-04],\n",
       "       [ 1.06740779e-04],\n",
       "       [-1.36408654e-06],\n",
       "       [ 3.58841594e-05],\n",
       "       [-2.38863311e-04],\n",
       "       [-3.32215086e-04],\n",
       "       [-2.57206716e-04],\n",
       "       [-1.24900589e-04],\n",
       "       [-7.42430112e-05],\n",
       "       [-2.05836905e-05],\n",
       "       [-1.08289271e-05],\n",
       "       [-1.98462554e-04],\n",
       "       [ 1.01792612e-04],\n",
       "       [-5.42617451e-05],\n",
       "       [-1.42204294e-04],\n",
       "       [-1.65004915e-04],\n",
       "       [-1.55285661e-04],\n",
       "       [ 8.66086936e-05],\n",
       "       [-8.50283469e-06],\n",
       "       [-1.46635609e-04],\n",
       "       [ 1.13440983e-05],\n",
       "       [-1.83228175e-05],\n",
       "       [-2.83861188e-04],\n",
       "       [-3.29558764e-05],\n",
       "       [-7.18538006e-05],\n",
       "       [-1.34212534e-05],\n",
       "       [-6.73097472e-05],\n",
       "       [-1.46541760e-04],\n",
       "       [-1.76014874e-04],\n",
       "       [-1.21683523e-04],\n",
       "       [ 7.82861520e-05],\n",
       "       [-7.59212886e-05],\n",
       "       [-1.36601423e-04],\n",
       "       [-4.70376547e-04],\n",
       "       [-5.95989844e-05],\n",
       "       [-1.78922204e-04],\n",
       "       [-1.99669146e-04],\n",
       "       [-1.66032682e-04],\n",
       "       [-1.97499680e-04],\n",
       "       [-4.01965966e-04],\n",
       "       [-2.18323677e-04],\n",
       "       [-1.27506245e-04],\n",
       "       [-1.55912169e-04],\n",
       "       [-1.83323773e-04],\n",
       "       [ 8.05337609e-05],\n",
       "       [-9.95608454e-05],\n",
       "       [-1.29706226e-04],\n",
       "       [-9.07543870e-05],\n",
       "       [-3.72923994e-05],\n",
       "       [ 1.19442033e-04],\n",
       "       [-1.19938515e-04],\n",
       "       [-3.03858577e-04],\n",
       "       [-1.37607428e-05],\n",
       "       [-1.51111520e-04],\n",
       "       [-2.46176268e-05],\n",
       "       [-5.63563825e-04],\n",
       "       [-3.39769338e-04],\n",
       "       [-2.84658412e-04],\n",
       "       [-2.32878347e-04],\n",
       "       [-2.38556660e-05],\n",
       "       [-8.28413445e-05],\n",
       "       [ 1.46042518e-04],\n",
       "       [ 4.14981445e-05],\n",
       "       [-6.26486020e-05],\n",
       "       [-2.36312677e-04],\n",
       "       [-1.83660455e-04],\n",
       "       [-7.93807394e-05],\n",
       "       [-1.35467619e-04],\n",
       "       [-7.14127613e-05],\n",
       "       [-2.87908290e-04],\n",
       "       [-2.72699012e-04],\n",
       "       [ 5.99095701e-05],\n",
       "       [ 9.08936032e-06],\n",
       "       [-2.54310026e-04],\n",
       "       [-8.03319667e-05],\n",
       "       [-2.43858114e-04],\n",
       "       [-3.75706074e-04],\n",
       "       [-3.16370772e-04],\n",
       "       [-8.56153374e-05],\n",
       "       [-2.71536659e-04],\n",
       "       [-1.63560924e-04],\n",
       "       [-5.12564303e-05],\n",
       "       [-8.70366275e-05],\n",
       "       [-1.60162362e-04],\n",
       "       [-1.83226905e-04],\n",
       "       [-1.96963858e-04],\n",
       "       [-4.09934536e-05],\n",
       "       [ 8.07081450e-05],\n",
       "       [-1.08229610e-04],\n",
       "       [ 1.92825011e-04],\n",
       "       [-1.42354034e-04],\n",
       "       [-1.85346883e-04],\n",
       "       [-2.59920765e-04],\n",
       "       [-1.94056715e-04],\n",
       "       [-2.17853793e-04],\n",
       "       [ 2.09141173e-05],\n",
       "       [-8.37487317e-05],\n",
       "       [-4.43630094e-04],\n",
       "       [-3.87640449e-05],\n",
       "       [-1.73942185e-04],\n",
       "       [ 1.01163454e-04],\n",
       "       [-2.77493505e-04],\n",
       "       [-3.57665741e-04],\n",
       "       [-1.31462974e-04],\n",
       "       [ 6.15077456e-05],\n",
       "       [-1.27432095e-04],\n",
       "       [-2.07194301e-04],\n",
       "       [-2.93551096e-04],\n",
       "       [-1.67311239e-04],\n",
       "       [-9.99281749e-05],\n",
       "       [-1.32663911e-04],\n",
       "       [-3.62388473e-04],\n",
       "       [-1.92731102e-04],\n",
       "       [-2.23094143e-04],\n",
       "       [-2.11057310e-05],\n",
       "       [-6.96638304e-07],\n",
       "       [ 3.55708937e-05],\n",
       "       [-7.29421388e-05],\n",
       "       [-4.31569136e-04],\n",
       "       [-2.51639150e-04],\n",
       "       [ 6.52144793e-05],\n",
       "       [-1.84771232e-04],\n",
       "       [-1.15539759e-05],\n",
       "       [-1.07123112e-04],\n",
       "       [-2.01513511e-04],\n",
       "       [-1.56541896e-04],\n",
       "       [-2.00033922e-04],\n",
       "       [-3.47697162e-05],\n",
       "       [-1.49007043e-04],\n",
       "       [-1.76608296e-04],\n",
       "       [ 1.36591572e-04],\n",
       "       [-1.04283047e-04],\n",
       "       [-8.22616825e-05],\n",
       "       [ 1.01034545e-04],\n",
       "       [ 1.21829344e-04],\n",
       "       [-1.80421207e-04],\n",
       "       [-6.92955392e-05],\n",
       "       [ 9.30944625e-05],\n",
       "       [-2.65659691e-04],\n",
       "       [-1.80920398e-04],\n",
       "       [-1.97496914e-04],\n",
       "       [-1.69244755e-04],\n",
       "       [-1.93080171e-04],\n",
       "       [-1.86902466e-04],\n",
       "       [-2.57692433e-04],\n",
       "       [-4.03685939e-05],\n",
       "       [-5.16689014e-05],\n",
       "       [-4.70593896e-04],\n",
       "       [-6.06845464e-05],\n",
       "       [-2.54450786e-04],\n",
       "       [-1.68991931e-04],\n",
       "       [-1.25293760e-04],\n",
       "       [-4.87885812e-05],\n",
       "       [-2.71569151e-05],\n",
       "       [-2.75481405e-04],\n",
       "       [-7.86392018e-05],\n",
       "       [-7.91394673e-05],\n",
       "       [-3.38603492e-05],\n",
       "       [-1.60486589e-04],\n",
       "       [-7.44729437e-05],\n",
       "       [-3.09766868e-04],\n",
       "       [-6.14731055e-05],\n",
       "       [-5.33078925e-04],\n",
       "       [-1.52605098e-04],\n",
       "       [-4.57825033e-04],\n",
       "       [-3.18540145e-04],\n",
       "       [ 4.13715564e-05],\n",
       "       [-2.05976755e-05],\n",
       "       [-2.90632083e-04],\n",
       "       [-4.03359250e-04],\n",
       "       [-2.12670296e-04],\n",
       "       [-1.31900978e-04],\n",
       "       [ 3.50843261e-05],\n",
       "       [-5.32021988e-05],\n",
       "       [-1.09856004e-04],\n",
       "       [-1.71385715e-04],\n",
       "       [-2.61441855e-04],\n",
       "       [-1.29321083e-04],\n",
       "       [-1.64969376e-04],\n",
       "       [-3.74955036e-04],\n",
       "       [-3.70608781e-04],\n",
       "       [-1.52276127e-04],\n",
       "       [-1.61177192e-04],\n",
       "       [-9.20679844e-05],\n",
       "       [-1.95509516e-04],\n",
       "       [-1.68800967e-04],\n",
       "       [-1.72471802e-04],\n",
       "       [-1.73155224e-04],\n",
       "       [-8.14422278e-05],\n",
       "       [-1.46632754e-05],\n",
       "       [-4.35904038e-04],\n",
       "       [-3.24728263e-04],\n",
       "       [-1.15385312e-04],\n",
       "       [-5.21054607e-05],\n",
       "       [-3.62746171e-04],\n",
       "       [-2.41847318e-04],\n",
       "       [-2.01634251e-04],\n",
       "       [-1.72699241e-04],\n",
       "       [-2.00701737e-04],\n",
       "       [-8.52118690e-05],\n",
       "       [-2.83313802e-04],\n",
       "       [-2.71790592e-04],\n",
       "       [-4.87073851e-04],\n",
       "       [-1.50888367e-04],\n",
       "       [-1.23034131e-04],\n",
       "       [-7.28183696e-05],\n",
       "       [-1.73894385e-04],\n",
       "       [-4.47907261e-05],\n",
       "       [-1.31276701e-05],\n",
       "       [-4.69074686e-04],\n",
       "       [-2.01560412e-04],\n",
       "       [-1.53067508e-05],\n",
       "       [-8.76162129e-05],\n",
       "       [ 1.07582444e-05],\n",
       "       [-2.03266014e-04],\n",
       "       [ 5.31204804e-05],\n",
       "       [-1.42625831e-04],\n",
       "       [-2.47364801e-05],\n",
       "       [-3.97247934e-04],\n",
       "       [-3.11871303e-04],\n",
       "       [-2.59564166e-04],\n",
       "       [ 1.75892516e-04],\n",
       "       [-1.13345776e-04],\n",
       "       [-1.81722989e-04],\n",
       "       [-1.22643438e-04],\n",
       "       [-1.57240987e-04],\n",
       "       [-1.42603843e-04],\n",
       "       [-1.79802211e-04],\n",
       "       [-8.48922726e-05],\n",
       "       [-1.28088205e-04],\n",
       "       [-2.05480568e-04],\n",
       "       [-4.65833252e-05],\n",
       "       [-1.50138127e-04],\n",
       "       [ 9.01161046e-05],\n",
       "       [-1.08311744e-04],\n",
       "       [-1.04223712e-04],\n",
       "       [-3.74468623e-04],\n",
       "       [-1.91148018e-04],\n",
       "       [-2.16699288e-04],\n",
       "       [ 2.78261391e-05],\n",
       "       [-2.05448799e-04],\n",
       "       [-1.70259622e-04],\n",
       "       [-5.16871163e-05],\n",
       "       [ 4.41724841e-05],\n",
       "       [-1.51036988e-04],\n",
       "       [-2.02497795e-04],\n",
       "       [-1.31409987e-04],\n",
       "       [-7.57520423e-05],\n",
       "       [-6.85681187e-05],\n",
       "       [-2.04430528e-04],\n",
       "       [-1.20080105e-04],\n",
       "       [-2.26055673e-04],\n",
       "       [-1.18398883e-04],\n",
       "       [ 3.72194154e-05],\n",
       "       [-2.14513125e-04]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(X_diab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "alg = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(10),\n",
    "    tf.keras.layers.Dense(20, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='relu'),\n",
    "])\n",
    "\n",
    "alg.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "alg.fit(X_diab, y_diab, epochs=1000, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
