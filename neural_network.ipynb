{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "X_diab, y_diab = load_diabetes(return_X_y=True) # returns diabetes data shapes: (442, 10) and (442,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "\n",
    "X_reg, y_reg = make_regression(n_samples=60, n_features=10, noise=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(\n",
    "            self, \n",
    "            units, \n",
    "            *, \n",
    "            input_layer: bool = False,\n",
    "            activation: str = \"linear\",\n",
    "            use_bias: bool = True,\n",
    "            # optimizer: str = \"gd\" # Testing\n",
    "            ):\n",
    "        \"\"\"\n",
    "        Initialize a neural network layer.\n",
    "\n",
    "        Args:\n",
    "            units (int): Count of neurons in the layer.\n",
    "            input_layer (bool, optional): Whether the layer is an input layer. Defaults to False.\n",
    "            activation (str, optional): Activation function for the layer. Can be \"linear\", \"relu\", or \"sigmoid\". Defaults to \"linear\".\n",
    "            use_bias (bool, optional): Whether to use bias in the layer. Defaults to True.\n",
    "        \"\"\"\n",
    "            \n",
    "        \n",
    "        self.units = units\n",
    "        self.input_layer = input_layer\n",
    "        self.activation = activation\n",
    "        self.use_bias = use_bias\n",
    "        self.optimizer = None # optimizer # Different optimizers for layers\n",
    "\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "\n",
    "        self.w = None # Weights matrix\n",
    "        self._weight_gradient = None # Weight derivative matrix\n",
    "        self._bias_gradient = None # Bias derivative vector\n",
    "\n",
    "        # self._adagrad_weight = None # AdaGrad optimizer's r for weight\n",
    "        # self._adagrad_bias = None # AdaGrad optimizer's r for bias\n",
    "\n",
    "    def activationFunction(self, z):\n",
    "        \"\"\"\n",
    "        Apply the activation function to the given input.\n",
    "\n",
    "        Args:\n",
    "            z (numpy.ndarray): Input to the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output after applying the activation function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return z\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(z, np.zeros(z.shape))\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def _weightInit(self, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the weights matrix based on the input size.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): Size of the input.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return # input_layer doesn't need weights\n",
    "\n",
    "        self.w = np.random.normal(loc = 0, scale = 1 / input_size, size=(input_size, self.units)) # loc -> mean, scale -> variance\n",
    "        self.bias = np.zeros((1, self.units))\n",
    "\n",
    "    def _setOptimizer(self, optimizer):\n",
    "\n",
    "        # TODO:\n",
    "        # check valid\n",
    "        #####\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._adagrad_weight = np.zeros(self.w.shape)\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._adagrad_bias = np.zeros(self.bias.shape)\n",
    "\n",
    "    def _activationDerivative(self):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the activation function.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the activation function.\n",
    "\n",
    "        Notes:\n",
    "            Only supports the \"linear\", \"relu\", and \"sigmoid\" activation functions.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.activation == \"linear\":\n",
    "            return 1\n",
    "\n",
    "        if self.activation == \"relu\":\n",
    "            return (self._output > 0) * 1\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self._output * (1 - self._output)\n",
    "\n",
    "    def _setGrad(self, grad):\n",
    "        \"\"\"\n",
    "        Calculate the gradients of weights and bias for backpropagation.\n",
    "\n",
    "        Args:\n",
    "            grad (numpy.ndarray): Gradient from the previous layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Gradient to be passed to the previous layer.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.input_layer:\n",
    "            return\n",
    "        \n",
    "        grad = grad * self._activationDerivative()\n",
    "        self._weight_gradient = self._input.T @ grad\n",
    "\n",
    "        if self.use_bias:\n",
    "            self._bias_gradient = grad.sum(axis=0, keepdims=True)\n",
    "\n",
    "        return grad @ self.w.T\n",
    "    \n",
    "    def _updateGrad(self, learning_rate):\n",
    "        \"\"\"\n",
    "        Update the weights and bias based on the computed gradients.\n",
    "\n",
    "        Args:\n",
    "            learning_rate (float): Learning rate for gradient descent.\n",
    "\n",
    "        Notes:\n",
    "            Only executed for layers other than the input layer.\n",
    "        \"\"\"\n",
    "\n",
    "        eps = 10e-8\n",
    "\n",
    "        if self.optimizer == \"gd\":\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "            if self.use_bias:\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "        if self.optimizer == \"adagrad\":\n",
    "            self._adagrad_weight += self._weight_gradient ** 2\n",
    "            learning_rate_weight = learning_rate / ( np.sqrt(self._adagrad_weight) + eps)\n",
    "            self.w -= learning_rate * self._weight_gradient\n",
    "\n",
    "            if self.use_bias:\n",
    "                self._adagrad_bias += self._bias_gradient ** 2\n",
    "                learning_rate_bias = learning_rate / ( np.sqrt(self._adagrad_bias) + eps)\n",
    "\n",
    "                self.bias -= learning_rate * self._bias_gradient\n",
    "\n",
    "\n",
    "    def call(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the layer.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input to the layer.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Output of the layer after applying the activation function.\n",
    "        \"\"\"\n",
    "        if self.input_layer:\n",
    "            return X\n",
    "        \n",
    "        self._input = X\n",
    "        self._output = self.activationFunction(X @ self.w + self.bias)\n",
    "\n",
    "        return self._output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeauralNetwork:\n",
    "    def __init__(\n",
    "            self, \n",
    "            layers: list, \n",
    "            loss_function: str = \"mse\", \n",
    "            learning_rate = 0.01, \n",
    "            max_iter=1000,\n",
    "            verbose: bool = False,\n",
    "            optimizer: str = \"gd\"):\n",
    "        \"\"\"\n",
    "        Initialize a neural network.\n",
    "\n",
    "        Args:\n",
    "            layers (list): List of Layer objects defining the network architecture. \n",
    "            loss_function (str, optional): Loss function to use. Defaults to \"mse\".\n",
    "            optimizer (str, optional): Optimization algorithm to use for updating weights during training.\n",
    "                Options include:\n",
    "                - \"gd\" (Gradient Descent): Standard gradient descent.\n",
    "                // - \"sgd\" (Stochastic Gradient Descent): Update weights using a single sample at a time.\n",
    "                // - \"adam\" (Adam): Adaptive Moment Estimation algorithm.\n",
    "                Defaults to \"gd\".\n",
    "\n",
    "            learning_rate (float, optional): Learning rate for gradient descent. Defaults to 0.01.\n",
    "            max_iter (int, optional): Maximum number of iterations for training. Defaults to 1000.\n",
    "            verbose (bool, optional): Whether to display training progress. Defaults to False.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        self.layers = layers\n",
    "        self.loss_function = loss_function\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.optimizer = optimizer # Optimizer for all layers\n",
    "\n",
    "        # Weights initializing:\n",
    "        for i in range(1, len(self.layers)):\n",
    "            self.layers[i]._weightInit(self.layers[i - 1].units)\n",
    "            self.layers[i]._setOptimizer(self.optimizer)\n",
    "\n",
    "\n",
    "\n",
    "    def lossFunction(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the loss between the true values and predicted values.\n",
    "\n",
    "        Args:\n",
    "            y_true (numpy.ndarray): True values.\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "\n",
    "        Returns:\n",
    "            float: Loss value.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 0.5 * np.mean(np.linalg.norm(y_pred - y_true, axis=1)**2)\n",
    "\n",
    "        # Can be add\n",
    "\n",
    "    def _lossFunctionDerivative(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Compute the derivative of the loss function.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Derivative of the loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.loss_function == \"mse\":\n",
    "            return 1 / len(y_pred) * (y_pred - y_true)\n",
    "\n",
    "        # Can be add\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the neural network on the given input-output pairs.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "            y (numpy.ndarray): Output data.\n",
    "\n",
    "        Notes:\n",
    "            Reshapes X and y to match the expected input shapes of the network.\n",
    "        \"\"\"\n",
    "\n",
    "        for _ in range(self.max_iter):\n",
    "            pred = self.forward(X)\n",
    "\n",
    "            if self.verbose:\n",
    "                process_percent = int(_ / self.max_iter * 20)\n",
    "                print(f\"\\r {_}/{self.max_iter}: [{process_percent * '=' + '>' + (20 - process_percent) * '-'}] - loss: {self.lossFunction(y, pred)}\", end=\"\")\n",
    "            self.backward(pred, y)\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"\\r {self.max_iter}/{self.max_iter}: [{21 * '='}] - loss: {self.lossFunction(y, pred)}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Perform predictions using the trained neural network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: Predicted output data.\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Perform a forward pass through the network.\n",
    "\n",
    "        Args:\n",
    "            X (numpy.ndarray): Input data.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray\n",
    "        \"\"\"\n",
    "\n",
    "        X_ = np.copy(X)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            X_ = layer.call(X_)\n",
    "        return X_\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Perform backpropagation to update the weights of the network.\n",
    "\n",
    "        Args:\n",
    "            y_pred (numpy.ndarray): Predicted values.\n",
    "            y_true (numpy.ndarray): True values.\n",
    "        \"\"\"\n",
    "        \n",
    "        gradient = self._lossFunctionDerivative(y_pred, y_true)\n",
    "\n",
    "        for i in range(len(self.layers) - 1, 0, -1):\n",
    "            gradient = self.layers[i]._setGrad(gradient)\n",
    "            self.layers[i]._updateGrad(self.learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100000/100000: [=====================] - loss: 890.6319605652773\n"
     ]
    }
   ],
   "source": [
    "nn = NeauralNetwork(layers=[\n",
    "        Layer(units=10, input_layer=True),\n",
    "        # Layer(units=40, activation=\"sigmoid\"),\n",
    "        Layer(units=40, activation=\"relu\"),\n",
    "        Layer(units=1),\n",
    "    ],\n",
    "    loss_function = \"mse\",\n",
    "    learning_rate=0.01, \n",
    "    max_iter=100000,\n",
    "    verbose=True,\n",
    "     optimizer=\"adagrad\",\n",
    ")\n",
    "y_diab = y_diab.reshape(-1, 1)\n",
    "nn.fit(X_diab, y_diab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[239.88652858],\n",
       "       [ 86.49415557],\n",
       "       [226.35709476],\n",
       "       [243.78590718],\n",
       "       [121.5512716 ],\n",
       "       [137.16439609],\n",
       "       [ 67.08825244],\n",
       "       [121.27249147],\n",
       "       [128.42461688],\n",
       "       [237.900439  ],\n",
       "       [120.63421075],\n",
       "       [101.89437118],\n",
       "       [112.61298471],\n",
       "       [187.82324061],\n",
       "       [100.89504375],\n",
       "       [177.9936691 ],\n",
       "       [213.68203485],\n",
       "       [185.25077452],\n",
       "       [157.09341639],\n",
       "       [126.50026175],\n",
       "       [118.87289294],\n",
       "       [ 56.66724432],\n",
       "       [113.36010006],\n",
       "       [284.73687341],\n",
       "       [216.16118541],\n",
       "       [132.53007599],\n",
       "       [ 97.62256736],\n",
       "       [142.05876247],\n",
       "       [149.84470542],\n",
       "       [280.3667196 ],\n",
       "       [131.50678057],\n",
       "       [ 79.25602107],\n",
       "       [374.62340126],\n",
       "       [114.8654318 ],\n",
       "       [ 83.83488483],\n",
       "       [117.43403483],\n",
       "       [241.27767588],\n",
       "       [196.03477367],\n",
       "       [241.53593272],\n",
       "       [116.78972914],\n",
       "       [147.43276027],\n",
       "       [106.4123569 ],\n",
       "       [102.65510068],\n",
       "       [ 93.02819614],\n",
       "       [291.97935304],\n",
       "       [115.286078  ],\n",
       "       [209.48737444],\n",
       "       [176.73180983],\n",
       "       [ 80.00068007],\n",
       "       [176.55409063],\n",
       "       [144.14924976],\n",
       "       [185.33579631],\n",
       "       [154.30708253],\n",
       "       [100.51102444],\n",
       "       [128.52222915],\n",
       "       [ 92.19446953],\n",
       "       [220.07168002],\n",
       "       [ 50.57275533],\n",
       "       [161.69713284],\n",
       "       [161.78137168],\n",
       "       [104.55742495],\n",
       "       [140.73910942],\n",
       "       [ 57.28576237],\n",
       "       [130.64536057],\n",
       "       [132.96715571],\n",
       "       [191.41994213],\n",
       "       [144.3710791 ],\n",
       "       [120.89954995],\n",
       "       [110.01199154],\n",
       "       [121.8635834 ],\n",
       "       [ 77.7182053 ],\n",
       "       [272.11272258],\n",
       "       [224.14454594],\n",
       "       [117.86244679],\n",
       "       [132.64079839],\n",
       "       [109.76425629],\n",
       "       [179.52088247],\n",
       "       [166.04989162],\n",
       "       [256.18240573],\n",
       "       [155.76113687],\n",
       "       [187.33604054],\n",
       "       [106.26195973],\n",
       "       [ 63.40407949],\n",
       "       [172.55419946],\n",
       "       [ 42.1155477 ],\n",
       "       [145.43031969],\n",
       "       [ 54.69814861],\n",
       "       [166.23399322],\n",
       "       [ 72.48115803],\n",
       "       [106.55189956],\n",
       "       [ 85.19664323],\n",
       "       [188.45220664],\n",
       "       [142.92481562],\n",
       "       [ 93.03993771],\n",
       "       [122.17464151],\n",
       "       [196.99468889],\n",
       "       [232.03479411],\n",
       "       [235.27264003],\n",
       "       [142.55702322],\n",
       "       [123.66998813],\n",
       "       [170.46129945],\n",
       "       [ 88.32004557],\n",
       "       [186.96349519],\n",
       "       [204.69409771],\n",
       "       [130.19357174],\n",
       "       [103.30449597],\n",
       "       [155.79173685],\n",
       "       [151.11776407],\n",
       "       [245.46487127],\n",
       "       [162.61423781],\n",
       "       [ 74.54437948],\n",
       "       [ 93.62687151],\n",
       "       [198.20861306],\n",
       "       [246.04017192],\n",
       "       [300.61785529],\n",
       "       [234.38783081],\n",
       "       [271.35241141],\n",
       "       [278.09700221],\n",
       "       [172.7321223 ],\n",
       "       [178.59520446],\n",
       "       [177.8030476 ],\n",
       "       [217.46464902],\n",
       "       [242.86572483],\n",
       "       [ 92.53549377],\n",
       "       [141.33272597],\n",
       "       [168.67685264],\n",
       "       [ 91.76928237],\n",
       "       [126.07133043],\n",
       "       [ 90.58179949],\n",
       "       [265.79926815],\n",
       "       [286.64304625],\n",
       "       [141.62382009],\n",
       "       [ 96.9760279 ],\n",
       "       [ 70.88652466],\n",
       "       [144.85036346],\n",
       "       [274.03019942],\n",
       "       [ 69.22488846],\n",
       "       [289.27668582],\n",
       "       [354.21760911],\n",
       "       [277.68393055],\n",
       "       [142.3235233 ],\n",
       "       [362.57110908],\n",
       "       [225.49370443],\n",
       "       [105.31617149],\n",
       "       [211.3586779 ],\n",
       "       [291.87764202],\n",
       "       [200.42986401],\n",
       "       [201.90657548],\n",
       "       [106.44729511],\n",
       "       [154.88218564],\n",
       "       [222.87691314],\n",
       "       [134.70648641],\n",
       "       [268.35351387],\n",
       "       [ 94.82005037],\n",
       "       [175.04139442],\n",
       "       [250.87061753],\n",
       "       [123.39921925],\n",
       "       [114.24835084],\n",
       "       [ 94.08251667],\n",
       "       [259.39883889],\n",
       "       [ 95.78200178],\n",
       "       [247.82998067],\n",
       "       [150.29924554],\n",
       "       [229.20086709],\n",
       "       [174.36850372],\n",
       "       [ 80.41571726],\n",
       "       [ 76.0731282 ],\n",
       "       [297.43961928],\n",
       "       [306.33147707],\n",
       "       [198.84899712],\n",
       "       [ 53.44505071],\n",
       "       [ 82.0950524 ],\n",
       "       [272.39302886],\n",
       "       [109.62258643],\n",
       "       [184.20251059],\n",
       "       [133.23490609],\n",
       "       [155.51087621],\n",
       "       [264.50541089],\n",
       "       [ 70.1882009 ],\n",
       "       [180.5292207 ],\n",
       "       [158.34642658],\n",
       "       [ 68.34156455],\n",
       "       [182.29486806],\n",
       "       [161.49251081],\n",
       "       [229.68152809],\n",
       "       [156.43272069],\n",
       "       [149.11051913],\n",
       "       [103.77516501],\n",
       "       [178.1465659 ],\n",
       "       [ 81.78533524],\n",
       "       [224.22010744],\n",
       "       [156.9458671 ],\n",
       "       [106.08370288],\n",
       "       [145.60274073],\n",
       "       [128.51688951],\n",
       "       [174.40625116],\n",
       "       [ 99.82368603],\n",
       "       [175.6831177 ],\n",
       "       [163.04272075],\n",
       "       [142.83834914],\n",
       "       [116.23054794],\n",
       "       [ 70.03443624],\n",
       "       [186.62442716],\n",
       "       [230.32637832],\n",
       "       [242.87606897],\n",
       "       [221.02349566],\n",
       "       [174.09205262],\n",
       "       [219.32436532],\n",
       "       [238.36494305],\n",
       "       [125.75474704],\n",
       "       [136.47094206],\n",
       "       [155.12502295],\n",
       "       [131.06147151],\n",
       "       [100.1955786 ],\n",
       "       [104.76885748],\n",
       "       [325.66823376],\n",
       "       [296.02277209],\n",
       "       [261.24289067],\n",
       "       [185.95413643],\n",
       "       [163.46969498],\n",
       "       [ 73.46032594],\n",
       "       [177.20130829],\n",
       "       [167.45630939],\n",
       "       [183.37590413],\n",
       "       [ 58.94626828],\n",
       "       [233.08384904],\n",
       "       [ 70.18323488],\n",
       "       [125.27297315],\n",
       "       [149.57208253],\n",
       "       [ 88.76219888],\n",
       "       [222.24322732],\n",
       "       [115.29313233],\n",
       "       [172.32852752],\n",
       "       [114.82594368],\n",
       "       [252.20318935],\n",
       "       [180.10637277],\n",
       "       [190.60669215],\n",
       "       [ 70.3091711 ],\n",
       "       [209.20579737],\n",
       "       [197.85045987],\n",
       "       [315.29605781],\n",
       "       [115.23885656],\n",
       "       [ 66.97874706],\n",
       "       [ 85.58520064],\n",
       "       [148.13594588],\n",
       "       [153.91590467],\n",
       "       [103.09219277],\n",
       "       [ 76.22060791],\n",
       "       [269.52301946],\n",
       "       [264.16822297],\n",
       "       [301.0481178 ],\n",
       "       [278.60726595],\n",
       "       [132.57670964],\n",
       "       [223.23068404],\n",
       "       [324.99605139],\n",
       "       [113.86026723],\n",
       "       [324.21492856],\n",
       "       [ 97.84794043],\n",
       "       [151.66969964],\n",
       "       [101.0791974 ],\n",
       "       [ 33.46286886],\n",
       "       [127.54709602],\n",
       "       [317.10384371],\n",
       "       [ 58.80349648],\n",
       "       [122.14957318],\n",
       "       [121.11911428],\n",
       "       [ 37.99849985],\n",
       "       [124.96637314],\n",
       "       [295.79638298],\n",
       "       [ 94.86114327],\n",
       "       [259.36767639],\n",
       "       [157.09455539],\n",
       "       [176.68829469],\n",
       "       [254.2869595 ],\n",
       "       [142.47461679],\n",
       "       [151.58001368],\n",
       "       [249.89200518],\n",
       "       [ 92.08520433],\n",
       "       [110.23318623],\n",
       "       [143.18177172],\n",
       "       [184.76805675],\n",
       "       [100.01251256],\n",
       "       [169.27010269],\n",
       "       [101.80655643],\n",
       "       [126.93246863],\n",
       "       [288.72067161],\n",
       "       [ 49.63867409],\n",
       "       [188.38389376],\n",
       "       [ 85.94799628],\n",
       "       [142.88249832],\n",
       "       [295.58411558],\n",
       "       [276.30229994],\n",
       "       [ 88.6289477 ],\n",
       "       [224.6984611 ],\n",
       "       [ 72.93484386],\n",
       "       [123.02729097],\n",
       "       [ 91.45532369],\n",
       "       [ 80.35779872],\n",
       "       [116.31455096],\n",
       "       [120.31314028],\n",
       "       [252.1507482 ],\n",
       "       [ 94.74213573],\n",
       "       [204.97609912],\n",
       "       [262.46800062],\n",
       "       [182.25543666],\n",
       "       [134.53881283],\n",
       "       [118.11988594],\n",
       "       [177.51319094],\n",
       "       [100.02633004],\n",
       "       [153.08405142],\n",
       "       [187.97888105],\n",
       "       [171.85754863],\n",
       "       [117.0782713 ],\n",
       "       [247.46359603],\n",
       "       [162.98314202],\n",
       "       [ 96.95016411],\n",
       "       [221.88471284],\n",
       "       [166.5311356 ],\n",
       "       [158.52631863],\n",
       "       [204.31455122],\n",
       "       [191.5200216 ],\n",
       "       [279.66520225],\n",
       "       [291.89467378],\n",
       "       [265.21967712],\n",
       "       [234.89364037],\n",
       "       [222.87371612],\n",
       "       [170.47595128],\n",
       "       [223.88487469],\n",
       "       [157.71688448],\n",
       "       [102.33469251],\n",
       "       [204.02119169],\n",
       "       [136.95241862],\n",
       "       [348.23358743],\n",
       "       [189.62467825],\n",
       "       [ 76.77367858],\n",
       "       [100.04616921],\n",
       "       [294.3912811 ],\n",
       "       [158.11377977],\n",
       "       [203.11367521],\n",
       "       [136.77079435],\n",
       "       [160.17371602],\n",
       "       [246.3118531 ],\n",
       "       [153.02480773],\n",
       "       [145.2492266 ],\n",
       "       [223.19911223],\n",
       "       [151.29802689],\n",
       "       [193.30489048],\n",
       "       [105.15147065],\n",
       "       [139.31727481],\n",
       "       [109.94960749],\n",
       "       [290.96002954],\n",
       "       [ 83.83885951],\n",
       "       [ 93.72789217],\n",
       "       [132.92839791],\n",
       "       [273.8596073 ],\n",
       "       [ 94.43794458],\n",
       "       [ 95.50817703],\n",
       "       [232.7842926 ],\n",
       "       [ 73.7675915 ],\n",
       "       [185.94985983],\n",
       "       [184.37522445],\n",
       "       [111.9671256 ],\n",
       "       [326.31367282],\n",
       "       [103.49999827],\n",
       "       [176.15740935],\n",
       "       [216.72443928],\n",
       "       [268.62621349],\n",
       "       [276.47742766],\n",
       "       [200.17606449],\n",
       "       [160.10332429],\n",
       "       [ 64.8414038 ],\n",
       "       [236.70115905],\n",
       "       [ 86.11176299],\n",
       "       [167.54199757],\n",
       "       [179.51066536],\n",
       "       [195.30715436],\n",
       "       [167.42929305],\n",
       "       [206.58411577],\n",
       "       [198.08476572],\n",
       "       [ 93.91344562],\n",
       "       [155.97431537],\n",
       "       [ 91.99095839],\n",
       "       [207.08673342],\n",
       "       [110.13941014],\n",
       "       [142.81588231],\n",
       "       [167.92436499],\n",
       "       [110.90914165],\n",
       "       [192.53928241],\n",
       "       [154.63705354],\n",
       "       [ 54.743233  ],\n",
       "       [304.91868838],\n",
       "       [ 75.17541388],\n",
       "       [ 85.37854492],\n",
       "       [123.69802142],\n",
       "       [255.08436237],\n",
       "       [230.06338768],\n",
       "       [ 53.35700805],\n",
       "       [236.37279556],\n",
       "       [222.72667057],\n",
       "       [211.48824418],\n",
       "       [186.37022163],\n",
       "       [ 94.27384657],\n",
       "       [197.39068629],\n",
       "       [252.51996963],\n",
       "       [219.93926149],\n",
       "       [293.78072197],\n",
       "       [ 90.20706376],\n",
       "       [137.13011617],\n",
       "       [193.08031981],\n",
       "       [214.49557605],\n",
       "       [228.23006147],\n",
       "       [116.47728777],\n",
       "       [266.64701581],\n",
       "       [122.24767532],\n",
       "       [143.59201557],\n",
       "       [181.66677703],\n",
       "       [277.20035977],\n",
       "       [ 92.60025466],\n",
       "       [110.4076593 ],\n",
       "       [ 75.74201789],\n",
       "       [153.94219069],\n",
       "       [242.70527182],\n",
       "       [256.37329616],\n",
       "       [140.22065322],\n",
       "       [156.52732193],\n",
       "       [131.41483921],\n",
       "       [173.26861756],\n",
       "       [119.61571785],\n",
       "       [327.4147643 ],\n",
       "       [114.98016381],\n",
       "       [131.55963848],\n",
       "       [169.83501905],\n",
       "       [248.65745033],\n",
       "       [ 66.21718561],\n",
       "       [106.26612486],\n",
       "       [104.17172795],\n",
       "       [ 63.79617903],\n",
       "       [207.64041751],\n",
       "       [ 91.26199474],\n",
       "       [112.2067939 ],\n",
       "       [222.4397299 ],\n",
       "       [ 77.29798567]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.predict(X_diab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "\n",
    "# alg = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Input(10),\n",
    "#     tf.keras.layers.Dense(20, activation='relu'),\n",
    "#     tf.keras.layers.Dense(1, activation='relu'),\n",
    "# ])\n",
    "\n",
    "# alg.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# alg.fit(X_diab, y_diab, epochs=1000, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alg.predict(X_diab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
